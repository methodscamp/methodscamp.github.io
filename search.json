[
  {
    "objectID": "index.html#class-schedule",
    "href": "index.html#class-schedule",
    "title": "Methods Camp",
    "section": "Class schedule",
    "text": "Class schedule\n\n\n\nDate\nTime\nLocation\n\n\n\n\nThurs, Aug. 10\n9:00 AM - 4:00 PM\nRLP 1.302D\n\n\nFri, Aug. 11\n9:00 AM - 4:00 PM\nRLP 1.302E\n\n\nSat, Aug. 12\nNo class\n-\n\n\nSun, Aug. 13\nNo class\n-\n\n\nMon, Aug. 14\n9:00 AM - 4:00 PM\nRLP 1.302D\n\n\nTues, Aug. 15\n9:00 AM - 4:00 PM\nRLP 1.302D\n\n\nWeds, Aug. 16\n9:00 AM - 4:00 PM\nRLP 1.302E\n\n\n\nOn class days, we will have a lunch break from 12:00-1:00 PM. We’ll also take short breaks periodically during the morning and afternoon sessions as needed."
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "Methods Camp",
    "section": "Description",
    "text": "Description\nWelcome to Introduction to Methods for Political Science, aka “Methods Camp”! Methods Camp is designed to give everyone a chance to brush up on some skills in preparation for the introductory Statistics and Formal Theory courses. The other goal of Methods Camp is to allow you to get to know your cohort. We hope that matrix algebra and the chain rule will still prove to be good bonding exercises!\nAs you can see from the above schedule, we’ll be meeting on Thursday, August 10th and Friday, August 11th as well as from Monday, August 14th through Wednesday, August 16th. Classes at UT begin the start of the following week on Monday, August 22nd. Below is a tentative schedule outlining what will be covered in the class, although we may rearrange things if we find we’re going too slowly or too quickly through the material."
  },
  {
    "objectID": "index.html#course-outline",
    "href": "index.html#course-outline",
    "title": "Methods Camp",
    "section": "Course outline",
    "text": "Course outline\n1 Thursday morning: Intro to R\n\nIntroductions\nR and RStudio: basics\nObjects (vectors, matrices, data frames, etc.)\nBasic functions (mean(), length(), etc.)\nPackages: installation and loading (including the tidyverse)\n\n2 Thursday afternoon: tidyverse basics I\n\nTidy data\nData wrangling with dplyr\nData visualization basics with ggplot2\n\n3 Friday morning: Matrices\n\nMatrices\nSystems of linear equations\nMatrix operations (multiplication, transpose, inverse, determinant)\nSolving systems of linear equations in matrix form (and why that’s cool)\nIntroduction to OLS\n\n4 Friday afternoon: tidyverse basics II\n\nLoading data in different formats (.csv, R, Excel, Stata, SPSS)\nRecoding values (if_else(), case_when())\nMissing values\nPivoting data\nMerging data\nPlotting extensions (trend graphs, facets, customization)\n\n5 Monday morning: Functions\n\nDefinitions\nFunctions in R\nCommon types of functions\nLogarithms and exponents\nComposite functions\n\n6 Monday afternoon: Calculus\n\nLimits\nDerivatives\nIntegrals\nCalculus in R\n\n7 Tuesday: Probability, statistics, and simulations\n\nBasic concepts\nRandom variables and their properties\nCommon distributions\nMonte Carlo simulations\nSampling\nLoops in R\nBootstrapping\n\n8 Wednesday morning: Text analysis\n\nString manipulation with stringr\nSimple text analysis and visualization with tidytext\n\n9 Wednesday afternoon: Wrap-up\n\nProject management fundamentals\nSelf-study resources and materials\nOther software (Overleaf, Zotero, etc.)\nMethods resources at UT"
  },
  {
    "objectID": "index.html#contact-info",
    "href": "index.html#contact-info",
    "title": "Methods Camp",
    "section": "Contact info",
    "text": "Contact info\nIf you have any questions during or outside of methods camp, you can contact us via email:\n\nAndrés Cruz: andres.cruz at utexas dot edu\nMatt Martin: mjmartin at utexas dot edu\n\nIf you are interested in learning more about our research, you can also check out our respective websites and Twitter accounts (or should we say X…):\n\nAndrés Cruz: [Website] [Twitter]\nMatt Martin: [Website] [Twitter]"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Methods Camp",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe thank previous Methods Camp instructors for their accumulated experience and materials, which we have based ours upon. UT GOV professors Stephen Jessee, Connor Jerzak, and Dan Nielson have given us amazing feedback for this iteration of Methods Camp. All errors remain our own (and will hopefully be fixed with your help!).\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. First edition. Beijing ; Boston: O’Reilly.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/."
  },
  {
    "objectID": "00_setup.html#installing-r-and-rstudio",
    "href": "00_setup.html#installing-r-and-rstudio",
    "title": "Setup",
    "section": "Installing R and RStudio",
    "text": "Installing R and RStudio\nR is a programming language optimized for statistics and data analysis. Most people use R from RStudio, a graphical user interface (GUI) that includes a file pane, a graphics pane, and other goodies. Both R and RStudio are open source, i.e., free as in beer and free as in freedom!\nYour first steps should be to install R and RStudio, in that order (if you have installed these programs before, make sure that your versions are up-to-date—if they are not, follow the instructions below):\n\nDownload and install R from the official website, CRAN. Click on “Download R for &lt;Windows/Mac&gt;” and follow the instructions. If you have a Mac, make sure to select the version appropriate for your system (Apple Silicon for newer M1/M2 Macs and Intel for older Macs).\nDownload and install RStudio from the official website. Scroll down and select the installer for your operating system.\n\nAfter these two steps, you can open RStudio in your system, as you would with any program. You should see something like this:\n\n\n\nFigure 1: How RStudio looks after a clean installation.\n\n\nThat’s it for the installation! We also strongly recommend that you change a couple of RStudio’s default settings.1 You can change settings by clicking on Tools &gt; Global Options in the menubar. Here are our recommendations:\n\nGeneral &gt; Uncheck \"Restore .RData into workspace at startup\"\nGeneral &gt; Save workspace to .RData on Exit &gt; Select \"Never\"\nCode &gt; Check \"Use native pipe operator\"\nTools &gt; Global Options &gt; Appearance to change to a dark theme, if you want! Pros: better for night sessions, hacker vibes…"
  },
  {
    "objectID": "00_setup.html#setting-up-for-methods-camp",
    "href": "00_setup.html#setting-up-for-methods-camp",
    "title": "Setup",
    "section": "Setting up for Methods Camp",
    "text": "Setting up for Methods Camp\nAll materials for Methods Camp are both on this website and available as RStudio projects for you to execute locally. An RStudio project is simply a folder where one keeps scripts, datasets, and other files needed for a data analysis project.\nThere are two RStudio projects for you to download, available as .zip compressed files. On MacOS, the file will be uncompressed automatically. On Windows, you should do Right click &gt; Extract all.\n\nDownload Part 1 of the class materials.\nDownload Part 2 of the class materials\n\n\n\n\n\n\n\nWarning\n\n\n\nMake sure to properly unzip the materials. Double-clicking the .zip file on most Windows systems will not unzip the folder—you must do Right click &gt; Extract all.\n\n\nYou should now have a folder called methodscamp_part1/ on your computer. Navigate to the methodscamp_part1.Rproj file within it and open it. RStudio should open the project right away. You should see methodscamp_part1 on the top-right of RStudio—this indicates that you are working in our RStudio project.\n\n\n\nFigure 2: How the bottom-right corner of RStudio looks after opening our project.\n\n\nThat’s all for setup! We can now start coding. After opening our RStudio project, we’ll begin by opening the 01_r_intro.qmd file from the “Files” panel, in the bottom-right portion of RStudio. This is a Quarto document,2 which contains both code and explanations (you can also read the materials in the next chapter of this website).\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. First edition. Beijing ; Boston: O’Reilly.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/."
  },
  {
    "objectID": "00_setup.html#footnotes",
    "href": "00_setup.html#footnotes",
    "title": "Setup",
    "section": "",
    "text": "The idea behind these settings (or at least the first two) is to force R to start from scratch with each new session. No lingering objects from previous coding sessions avoids misunderstandings and helps with reproducibility!↩︎\nPerhaps you have used R Markdown before. Quarto is the next iteration of R Markdown, and is both more flexible and more powerful!↩︎"
  },
  {
    "objectID": "01_r_intro.html#objects",
    "href": "01_r_intro.html#objects",
    "title": "1  Intro to R",
    "section": "1.1 Objects",
    "text": "1.1 Objects\nA huge part of R is working with objects. Let’s see how they work:\n\nmy_object &lt;- 10 # opt/alt + minus sign will make the arrow \n\n\nmy_object # to print the value of an object, just call its name\n\n[1] 10\n\n\nWe can now use this object in our operations:\n\n2 ^ my_object\n\n[1] 1024\n\n\nOr even create another object out of it:\n\nmy_object2 &lt;- my_object * 2\n\n\nmy_object2\n\n[1] 20\n\n\nYou can delete objects with the rm() function (for “remove”):\n\nrm(my_object2)"
  },
  {
    "objectID": "01_r_intro.html#vectors-and-functions",
    "href": "01_r_intro.html#vectors-and-functions",
    "title": "1  Intro to R",
    "section": "1.2 Vectors and functions",
    "text": "1.2 Vectors and functions\nObjects can be of different types. One of the most useful ones is the vector, which holds a series of values. To create one manually, we can use the c() function (for “combine”):\n\nmy_vector &lt;- c(6, -11, my_object, 0, 20)\n\n\nmy_vector\n\n[1]   6 -11  10   0  20\n\n\nOne can also define vectors by sequences:\n\n3:10\n\n[1]  3  4  5  6  7  8  9 10\n\n\nWe can use square brackets to retrieve parts of vectors:\n\nmy_vector[4] # fourth element\n\n[1] 0\n\n\n\nmy_vector[1:2] # first two elements\n\n[1]   6 -11\n\n\nLet’s check out some basic functions we can use with numbers and numeric vectors:\n\nsqrt(my_object) # squared root\n\n[1] 3.162278\n\n\n\nlog(my_object) # logarithm (natural by default)\n\n[1] 2.302585\n\n\n\nabs(-5) # absolute value\n\n[1] 5\n\n\n\nmean(my_vector)\n\n[1] 5\n\n\n\nmedian(my_vector)\n\n[1] 6\n\n\n\nsd(my_vector) # standard deviation\n\n[1] 11.53256\n\n\n\nsum(my_vector)\n\n[1] 25\n\n\n\nmin(my_vector) # minimum value\n\n[1] -11\n\n\n\nmax(my_vector) # maximum value\n\n[1] 20\n\n\n\nlength(my_vector) # length (number of elements)\n\n[1] 5\n\n\nNotice that if we wanted to save any of these results for later, we would need to assign them:\n\nmy_mean &lt;- mean(my_vector)\n\n\nmy_mean\n\n[1] 5\n\n\nThese functions are quite simple: they take one object and do one operation. A lot of functions are a bit more complex—they take multiple objects or take options. For example, see the sort() function, which by default sorts a vector increasingly:\n\nsort(my_vector)\n\n[1] -11   0   6  10  20\n\n\nIf we instead want to sort our vector decreasingly, we can use the decreasing = TRUE argument (T also works as an abbreviation for TRUE).\n\nsort(my_vector, decreasing = TRUE)\n\n[1]  20  10   6   0 -11\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you use the argument values in order, you can avoid writing the argument names (see below). This is sometimes useful, but can also lead to confusing code—use it with caution.\n\nsort(my_vector, T)\n\n[1]  20  10   6   0 -11\n\n\n\n\nA useful function to create vectors in sequence is seq(). Notice its arguments:\n\nseq(from = 30, to = 100, by = 5)\n\n [1]  30  35  40  45  50  55  60  65  70  75  80  85  90  95 100\n\n\nTo check the arguments of a function, you can examine its help file: look the function up on the “Help” panel on RStudio or use a command like the following: ?sort.\n\n\n\n\n\n\nExercise\n\n\n\nExamine the help file of the log() function. How can we compute the the base-10 logarithm of my_object? Your code:\n\n\nOther than numeric vectors, character vectors are also useful:\n\nmy_character_vector &lt;- c(\"Apple\", \"Orange\", \"Watermelon\", \"Banana\")\n\n\nmy_character_vector[3]\n\n[1] \"Watermelon\"\n\n\n\nnchar(my_character_vector) # count number of characters\n\n[1]  5  6 10  6"
  },
  {
    "objectID": "01_r_intro.html#data-frames-and-lists",
    "href": "01_r_intro.html#data-frames-and-lists",
    "title": "1  Intro to R",
    "section": "1.3 Data frames and lists",
    "text": "1.3 Data frames and lists\nAnother useful object type is the data frame. Data frames can store multiple vectors in a tabular format. We can manually create one with the data.frame() function:\n\nmy_data_frame &lt;- data.frame(fruit = my_character_vector,\n                            calories_per_100g = c(52, 47, 30, 89),\n                            water_per_100g = c(85.6, 86.8, 91.4, 74.9))\n\n\nmy_data_frame\n\n       fruit calories_per_100g water_per_100g\n1      Apple                52           85.6\n2     Orange                47           86.8\n3 Watermelon                30           91.4\n4     Banana                89           74.9\n\n\nNow we have a little 4x3 data frame of fruits with their calorie counts and water composition. We gathered the nutritional information from the USDA (2019).\nWe can use the data_frame$column construct to access the vectors within the data frame:\n\nmean(my_data_frame$calories_per_100g)\n\n[1] 54.5\n\n\n\n\n\n\n\n\nExercise\n\n\n\nObtain the maximum value of water content per 100g in the data. Your code:\n\n\nSome useful commands to learn attributes of our data frame:\n\ndim(my_data_frame)\n\n[1] 4 3\n\n\n\nnrow(my_data_frame)\n\n[1] 4\n\n\n\nnames(my_data_frame) # column names\n\n[1] \"fruit\"             \"calories_per_100g\" \"water_per_100g\"   \n\n\nWe will learn much more about data frames in our next module on data analysis.\nAfter talking about vectors and data frames, the last object type that we will cover is the list. Lists are super flexible objects that can contain just about anything:\n\nmy_list &lt;- list(my_object, my_vector, my_data_frame)\n\n\nmy_list\n\n[[1]]\n[1] 10\n\n[[2]]\n[1]   6 -11  10   0  20\n\n[[3]]\n       fruit calories_per_100g water_per_100g\n1      Apple                52           85.6\n2     Orange                47           86.8\n3 Watermelon                30           91.4\n4     Banana                89           74.9\n\n\nTo retrieve the elements of a list, we need to use double square brackets:\n\nmy_list[[1]]\n\n[1] 10\n\n\nLists are sometimes useful due to their flexibility, but are much less common in routine data analysis compared to vectors or data frames."
  },
  {
    "objectID": "01_r_intro.html#packages",
    "href": "01_r_intro.html#packages",
    "title": "1  Intro to R",
    "section": "1.4 Packages",
    "text": "1.4 Packages\nThe R community has developed thousands of packages, which are specialized collections of functions, datasets, and other resources. To install one, you should use the install.packages() command. Below we will install the tidyverse package, a suite for data analysis that we will use in the next modules. You just need to install packages once, and then they will be available system-wide.\n\ninstall.packages(\"tidyverse\") # this can take a couple of minutes\n\nIf you want to use an installed package in your script, you must load it with the library() function. Some packages, as shown below, will print descriptive messages once loaded.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember that install.packages(\"package\") needs to be executed just once, while library(package) needs to be in each script in which you plan to use the package. In general, never include install.packages(\"package\") as part of your scripts or Quarto documents!\n\n\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. First edition. Beijing ; Boston: O’Reilly.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/."
  },
  {
    "objectID": "02_tidy_data1.html#loading-data",
    "href": "02_tidy_data1.html#loading-data",
    "title": "2  Tidy data analysis I",
    "section": "2.1 Loading data",
    "text": "2.1 Loading data\nThroughout this module we will work with a dataset of senators during the Trump presidency, which was adapted from FiveThirtyEight (2021).\nWe have stored the dataset in .csv format under the data/ subfolder. Loading it into R is simple (notice that we need to assign it to an object):\n\ntrump_scores &lt;- read_csv(\"data/trump_scores_538.csv\")\n\nRows: 122 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): bioguide, last_name, state, party\ndbl (4): num_votes, agree, agree_pred, margin_trump\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ntrump_scores\n\n# A tibble: 122 × 8\n   bioguide last_name  state party num_votes agree agree_pred margin_trump\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 A000360  Alexander  TN    R           118 0.890      0.856       26.0  \n 2 B000575  Blunt      MO    R           128 0.906      0.787       18.6  \n 3 B000944  Brown      OH    D           128 0.258      0.642        8.13 \n 4 B001135  Burr       NC    R           121 0.893      0.560        3.66 \n 5 B001230  Baldwin    WI    D           128 0.227      0.510        0.764\n 6 B001236  Boozman    AR    R           129 0.915      0.851       26.9  \n 7 B001243  Blackburn  TN    R           131 0.885      0.889       26.0  \n 8 B001261  Barrasso   WY    R           129 0.891      0.895       46.3  \n 9 B001267  Bennet     CO    D           121 0.273      0.417       -4.91 \n10 B001277  Blumenthal CT    D           128 0.203      0.294      -13.6  \n# ℹ 112 more rows\n\n\nLet’s review the dataset’s columns:\n\nbioguide: A unique ID for each politician, from the Congress Bioguide.\nlast_name\nstate\nparty\nnum_votes: Number of votes for which data was available.\nagree: Proportion (0-1) of votes in which the senator voted in agreement with Trump.\nagree_pred: Predicted proportion of vote agreement, calculated using Trump’s margin (see next variable).\nmargin_trump: Margin of victory (percentage points) of Trump in the senator’s state.\n\nWe can inspect our data by using the interface above. An alternative is to run the command View(trump_scores) or click on the object in RStudio’s environment panel (in the top-right section).\nDo you have any questions about the data?\nBy the way, the tidyverse works amazingly with tidy data. If you can get your data to this format (and we will see ways to do this), your life will be much easier:\n\n\n\n\n\n\n\n\n\n\n\nSource: Illustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst."
  },
  {
    "objectID": "02_tidy_data1.html#wrangling-data-with-dplyr",
    "href": "02_tidy_data1.html#wrangling-data-with-dplyr",
    "title": "2  Tidy data analysis I",
    "section": "2.2 Wrangling data with dplyr",
    "text": "2.2 Wrangling data with dplyr\nWe often need to modify data to conduct our analyses, e.g., creating columns, filtering rows, etc. In the tidyverse, these operations are conducted with multiple verbs, which we will review now.\n\n2.2.1 Selecting columns\nWe can select specific columns in our dataset with the select() function. All dplyr wrangling verbs take a data frame as their first argument—in this case, the columns we want to select are the other arguments.\n\nselect(trump_scores, last_name, party)\n\n# A tibble: 122 × 2\n   last_name  party\n   &lt;chr&gt;      &lt;chr&gt;\n 1 Alexander  R    \n 2 Blunt      R    \n 3 Brown      D    \n 4 Burr       R    \n 5 Baldwin    D    \n 6 Boozman    R    \n 7 Blackburn  R    \n 8 Barrasso   R    \n 9 Bennet     D    \n10 Blumenthal D    \n# ℹ 112 more rows\n\n\nThis is a good moment to talk about “pipes.” Notice how the code below produces the same output as the one above, but with a slightly different syntax. Pipes (|&gt;) “kick” the object on the left of the pipe to the first argument of the function on the right. One can read pipes as “then,” so the code below can be read as “take trump_scores, then select the columns last_name and party.” Pipes are very useful to chain multiple operations, as we will see in a moment.\n\ntrump_scores |&gt; \n  select(last_name, party)\n\n# A tibble: 122 × 2\n   last_name  party\n   &lt;chr&gt;      &lt;chr&gt;\n 1 Alexander  R    \n 2 Blunt      R    \n 3 Brown      D    \n 4 Burr       R    \n 5 Baldwin    D    \n 6 Boozman    R    \n 7 Blackburn  R    \n 8 Barrasso   R    \n 9 Bennet     D    \n10 Blumenthal D    \n# ℹ 112 more rows\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can insert a pipe with the Cmd/Ctrl + Shift + M shortcut. If you have not changed the default RStudio settings, an “old” pipe (%&gt;%) might appear. While most of the functionality is the same, the |&gt; “new” pipes are more readable. You can change this RStudio option in Tools &gt; Global Options &gt; Code &gt; Use native pipe operator. Make sure to check the other suggested settings in our Setup module!\n\n\nGoing back to selecting columns, you can select ranges:\n\ntrump_scores |&gt; \n  select(bioguide:party)\n\n# A tibble: 122 × 4\n   bioguide last_name  state party\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;\n 1 A000360  Alexander  TN    R    \n 2 B000575  Blunt      MO    R    \n 3 B000944  Brown      OH    D    \n 4 B001135  Burr       NC    R    \n 5 B001230  Baldwin    WI    D    \n 6 B001236  Boozman    AR    R    \n 7 B001243  Blackburn  TN    R    \n 8 B001261  Barrasso   WY    R    \n 9 B001267  Bennet     CO    D    \n10 B001277  Blumenthal CT    D    \n# ℹ 112 more rows\n\n\nYou can also deselect columns using a minus sign:\n\ntrump_scores |&gt; \n  select(-last_name)\n\n# A tibble: 122 × 7\n   bioguide state party num_votes agree agree_pred margin_trump\n   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 A000360  TN    R           118 0.890      0.856       26.0  \n 2 B000575  MO    R           128 0.906      0.787       18.6  \n 3 B000944  OH    D           128 0.258      0.642        8.13 \n 4 B001135  NC    R           121 0.893      0.560        3.66 \n 5 B001230  WI    D           128 0.227      0.510        0.764\n 6 B001236  AR    R           129 0.915      0.851       26.9  \n 7 B001243  TN    R           131 0.885      0.889       26.0  \n 8 B001261  WY    R           129 0.891      0.895       46.3  \n 9 B001267  CO    D           121 0.273      0.417       -4.91 \n10 B001277  CT    D           128 0.203      0.294      -13.6  \n# ℹ 112 more rows\n\n\nAnd use a few helper functions, like matches():\n\ntrump_scores |&gt; \n  select(last_name, matches(\"agree\"))\n\n# A tibble: 122 × 3\n   last_name  agree agree_pred\n   &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 Alexander  0.890      0.856\n 2 Blunt      0.906      0.787\n 3 Brown      0.258      0.642\n 4 Burr       0.893      0.560\n 5 Baldwin    0.227      0.510\n 6 Boozman    0.915      0.851\n 7 Blackburn  0.885      0.889\n 8 Barrasso   0.891      0.895\n 9 Bennet     0.273      0.417\n10 Blumenthal 0.203      0.294\n# ℹ 112 more rows\n\n\nOr everything(), which we usually use to reorder columns:\n\ntrump_scores |&gt; \n  select(last_name, everything())\n\n# A tibble: 122 × 8\n   last_name  bioguide state party num_votes agree agree_pred margin_trump\n   &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 Alexander  A000360  TN    R           118 0.890      0.856       26.0  \n 2 Blunt      B000575  MO    R           128 0.906      0.787       18.6  \n 3 Brown      B000944  OH    D           128 0.258      0.642        8.13 \n 4 Burr       B001135  NC    R           121 0.893      0.560        3.66 \n 5 Baldwin    B001230  WI    D           128 0.227      0.510        0.764\n 6 Boozman    B001236  AR    R           129 0.915      0.851       26.9  \n 7 Blackburn  B001243  TN    R           131 0.885      0.889       26.0  \n 8 Barrasso   B001261  WY    R           129 0.891      0.895       46.3  \n 9 Bennet     B001267  CO    D           121 0.273      0.417       -4.91 \n10 Blumenthal B001277  CT    D           128 0.203      0.294      -13.6  \n# ℹ 112 more rows\n\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that all these commands have not edited our existent objects—they have just printed the requested outputs to the screen. In order to modify objects, you need to use the assignment operator (&lt;-). For example:\n\ntrump_scores_reduced &lt;- trump_scores |&gt; \n  select(last_name, matches(\"agree\"))\n\n\ntrump_scores_reduced\n\n# A tibble: 122 × 3\n   last_name  agree agree_pred\n   &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 Alexander  0.890      0.856\n 2 Blunt      0.906      0.787\n 3 Brown      0.258      0.642\n 4 Burr       0.893      0.560\n 5 Baldwin    0.227      0.510\n 6 Boozman    0.915      0.851\n 7 Blackburn  0.885      0.889\n 8 Barrasso   0.891      0.895\n 9 Bennet     0.273      0.417\n10 Blumenthal 0.203      0.294\n# ℹ 112 more rows\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSelect the variables last_name, party, num_votes, and agree from the data frame. Your code:\n\n\n\n\n2.2.2 Renaming columns\nWe can use the rename() function to rename columns, with the syntax new_name = old_name. For example:\n\ntrump_scores |&gt; \n  rename(prop_agree = agree, prop_agree_pred = agree_pred)\n\n# A tibble: 122 × 8\n   bioguide last_name  state party num_votes prop_agree prop_agree_pred\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;\n 1 A000360  Alexander  TN    R           118      0.890           0.856\n 2 B000575  Blunt      MO    R           128      0.906           0.787\n 3 B000944  Brown      OH    D           128      0.258           0.642\n 4 B001135  Burr       NC    R           121      0.893           0.560\n 5 B001230  Baldwin    WI    D           128      0.227           0.510\n 6 B001236  Boozman    AR    R           129      0.915           0.851\n 7 B001243  Blackburn  TN    R           131      0.885           0.889\n 8 B001261  Barrasso   WY    R           129      0.891           0.895\n 9 B001267  Bennet     CO    D           121      0.273           0.417\n10 B001277  Blumenthal CT    D           128      0.203           0.294\n# ℹ 112 more rows\n# ℹ 1 more variable: margin_trump &lt;dbl&gt;\n\n\nThis is a good occasion to show how pipes allow us to chain operations. How do we read the following code out loud? (Remember that pipes are read as “then”).\n\ntrump_scores |&gt; \n  select(last_name, matches(\"agree\")) |&gt; \n  rename(prop_agree = agree, prop_agree_pred = agree_pred)\n\n# A tibble: 122 × 3\n   last_name  prop_agree prop_agree_pred\n   &lt;chr&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n 1 Alexander       0.890           0.856\n 2 Blunt           0.906           0.787\n 3 Brown           0.258           0.642\n 4 Burr            0.893           0.560\n 5 Baldwin         0.227           0.510\n 6 Boozman         0.915           0.851\n 7 Blackburn       0.885           0.889\n 8 Barrasso        0.891           0.895\n 9 Bennet          0.273           0.417\n10 Blumenthal      0.203           0.294\n# ℹ 112 more rows\n\n\n\n\n2.2.3 Creating columns\nIt is common to want to create columns, based on existing ones. We can use mutate() to do so. For example, we could want our main variables of interest in terms of percentages instead of proportions:\n\ntrump_scores |&gt; \n  select(last_name, agree, agree_pred) |&gt; # select just for clarity\n  mutate(pct_agree = 100 * agree,\n         pct_agree_pred = 100 * agree_pred)\n\n# A tibble: 122 × 5\n   last_name  agree agree_pred pct_agree pct_agree_pred\n   &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;\n 1 Alexander  0.890      0.856      89.0           85.6\n 2 Blunt      0.906      0.787      90.6           78.7\n 3 Brown      0.258      0.642      25.8           64.2\n 4 Burr       0.893      0.560      89.3           56.0\n 5 Baldwin    0.227      0.510      22.7           51.0\n 6 Boozman    0.915      0.851      91.5           85.1\n 7 Blackburn  0.885      0.889      88.5           88.9\n 8 Barrasso   0.891      0.895      89.1           89.5\n 9 Bennet     0.273      0.417      27.3           41.7\n10 Blumenthal 0.203      0.294      20.3           29.4\n# ℹ 112 more rows\n\n\nWe can also use multiple columns for creating a new one. For example, let’s retrieve the total number of votes in which the senator agreed with Trump:\n\ntrump_scores |&gt; \n  select(last_name, num_votes, agree) |&gt; # select just for clarity\n  mutate(num_votes_agree = num_votes * agree)\n\n# A tibble: 122 × 4\n   last_name  num_votes agree num_votes_agree\n   &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;\n 1 Alexander        118 0.890           105  \n 2 Blunt            128 0.906           116  \n 3 Brown            128 0.258            33  \n 4 Burr             121 0.893           108  \n 5 Baldwin          128 0.227            29  \n 6 Boozman          129 0.915           118  \n 7 Blackburn        131 0.885           116  \n 8 Barrasso         129 0.891           115  \n 9 Bennet           121 0.273            33.0\n10 Blumenthal       128 0.203            26  \n# ℹ 112 more rows\n\n\n\n\n2.2.4 Filtering rows\nAnother common operation is to filter rows based on logical conditions. We can do so with the filter() function. For example, we can filter to only get Democrats:\n\ntrump_scores |&gt; \n  filter(party == \"D\")\n\n# A tibble: 55 × 8\n   bioguide last_name  state party num_votes agree agree_pred margin_trump\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 B000944  Brown      OH    D           128 0.258      0.642        8.13 \n 2 B001230  Baldwin    WI    D           128 0.227      0.510        0.764\n 3 B001267  Bennet     CO    D           121 0.273      0.417       -4.91 \n 4 B001277  Blumenthal CT    D           128 0.203      0.294      -13.6  \n 5 B001288  Booker     NJ    D           119 0.160      0.290      -14.1  \n 6 C000127  Cantwell   WA    D           128 0.242      0.276      -15.5  \n 7 C000141  Cardin     MD    D           128 0.25       0.209      -26.4  \n 8 C000174  Carper     DE    D           129 0.295      0.318      -11.4  \n 9 C001070  Casey      PA    D           129 0.287      0.508        0.724\n10 C001088  Coons      DE    D           128 0.289      0.319      -11.4  \n# ℹ 45 more rows\n\n\nNotice that == here is a logical operator, read as “is equal to.” So our full chain of operations says the following: take trump_scores, then filter it to get rows where party is equal to “D”.\nThere are other logical operators:\n\n\n\nLogical operator\nMeaning\n\n\n\n\n==\n“is equal to”\n\n\n!=\n“is not equal to”\n\n\n&gt;\n“is greater than”\n\n\n&lt;\n“is less than”\n\n\n&gt;=\n“is greater than or equal to”\n\n\n&lt;=\n“is less than or equal to”\n\n\n%in%\n“is contained in”\n\n\n&\n“and” (intersection)\n\n\n|\n“or” (union)\n\n\n\nLet’s see a couple of other examples.\n\ntrump_scores |&gt; \n  filter(agree &gt; 0.5)\n\n# A tibble: 69 × 8\n   bioguide last_name state party num_votes agree agree_pred margin_trump\n   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 A000360  Alexander TN    R           118 0.890      0.856        26.0 \n 2 B000575  Blunt     MO    R           128 0.906      0.787        18.6 \n 3 B001135  Burr      NC    R           121 0.893      0.560         3.66\n 4 B001236  Boozman   AR    R           129 0.915      0.851        26.9 \n 5 B001243  Blackburn TN    R           131 0.885      0.889        26.0 \n 6 B001261  Barrasso  WY    R           129 0.891      0.895        46.3 \n 7 B001310  Braun     IN    R            44 0.909      0.713        19.2 \n 8 C000567  Cochran   MS    R            68 0.971      0.830        17.8 \n 9 C000880  Crapo     ID    R           125 0.904      0.870        31.8 \n10 C001035  Collins   ME    R           129 0.651      0.441        -2.96\n# ℹ 59 more rows\n\n\n\ntrump_scores |&gt; \n  filter(state %in% c(\"CA\", \"TX\"))\n\n# A tibble: 4 × 8\n  bioguide last_name state party num_votes agree agree_pred margin_trump\n  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n1 C001056  Cornyn    TX    R           129 0.922      0.659         9.00\n2 C001098  Cruz      TX    R           126 0.921      0.663         9.00\n3 F000062  Feinstein CA    D           128 0.242      0.201       -30.1 \n4 H001075  Harris    CA    D           116 0.164      0.209       -30.1 \n\n\n\ntrump_scores |&gt; \n  filter(state == \"WV\" & party == \"D\")\n\n# A tibble: 1 × 8\n  bioguide last_name state party num_votes agree agree_pred margin_trump\n  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n1 M001183  Manchin   WV    D           129 0.504      0.893         42.2\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nAdd a new column to the data frame, called diff_agree, which subtracts agree and agree_pred. How would you create abs_diff_agree, defined as the absolute value of diff_agree? Your code:\nFilter the data frame to only get senators for which we have information on fewer than (or equal to) five votes. Your code:\nFilter the data frame to only get Democrats who agreed with Trump in at least 30% of votes. Your code:\n\n\n\n\n\n2.2.5 Ordering rows\nThe arrange() function allows us to order rows according to values. For example, let’s order based on the agree variable:\n\ntrump_scores |&gt; \n  arrange(agree)\n\n# A tibble: 122 × 8\n   bioguide last_name    state party num_votes agree agree_pred margin_trump\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 H000273  Hickenlooper CO    D             2 0         0.0302        -4.91\n 2 H000601  Hagerty      TN    R             2 0         0.115         26.0 \n 3 L000570  Luján        NM    D           186 0.124     0.243         -8.21\n 4 G000555  Gillibrand   NY    D           121 0.124     0.242        -22.5 \n 5 M001176  Merkley      OR    D           129 0.155     0.323        -11.0 \n 6 W000817  Warren       MA    D           116 0.155     0.216        -27.2 \n 7 B001288  Booker       NJ    D           119 0.160     0.290        -14.1 \n 8 S000033  Sanders      VT    D           112 0.161     0.221        -26.4 \n 9 H001075  Harris       CA    D           116 0.164     0.209        -30.1 \n10 M000133  Markey       MA    D           127 0.165     0.213        -27.2 \n# ℹ 112 more rows\n\n\nMaybe we only want senators with more than a few data points. Remember that we can chain operations:\n\ntrump_scores |&gt; \n  filter(num_votes &gt;= 10) |&gt; \n  arrange(agree)\n\n# A tibble: 115 × 8\n   bioguide last_name  state party num_votes agree agree_pred margin_trump\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 L000570  Luján      NM    D           186 0.124      0.243        -8.21\n 2 G000555  Gillibrand NY    D           121 0.124      0.242       -22.5 \n 3 M001176  Merkley    OR    D           129 0.155      0.323       -11.0 \n 4 W000817  Warren     MA    D           116 0.155      0.216       -27.2 \n 5 B001288  Booker     NJ    D           119 0.160      0.290       -14.1 \n 6 S000033  Sanders    VT    D           112 0.161      0.221       -26.4 \n 7 H001075  Harris     CA    D           116 0.164      0.209       -30.1 \n 8 M000133  Markey     MA    D           127 0.165      0.213       -27.2 \n 9 W000779  Wyden      OR    D           129 0.186      0.323       -11.0 \n10 B001277  Blumenthal CT    D           128 0.203      0.294       -13.6 \n# ℹ 105 more rows\n\n\nBy default, arrange() uses increasing order (like sort()). To use decreasing order, add a minus sign:\n\ntrump_scores |&gt; \n  filter(num_votes &gt;= 10) |&gt; \n  arrange(-agree)\n\n# A tibble: 115 × 8\n   bioguide last_name state party num_votes agree agree_pred margin_trump\n   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 M001198  Marshall  KS    R           183 0.973      0.933        20.6 \n 2 C000567  Cochran   MS    R            68 0.971      0.830        17.8 \n 3 H000338  Hatch     UT    R            84 0.964      0.825        18.1 \n 4 M001197  McSally   AZ    R           136 0.949      0.562         3.55\n 5 P000612  Perdue    GA    R           119 0.941      0.606         5.16\n 6 C001096  Cramer    ND    R           135 0.941      0.908        35.7 \n 7 R000307  Roberts   KS    R           127 0.937      0.818        20.6 \n 8 C001056  Cornyn    TX    R           129 0.922      0.659         9.00\n 9 H001061  Hoeven    ND    R           129 0.922      0.883        35.7 \n10 C001047  Capito    WV    R           127 0.921      0.896        42.2 \n# ℹ 105 more rows\n\n\nYou can also order rows by more than one variable. What this does is to order by the first variable, and resolve any ties by ordering by the second variable (and so forth if you have more than two ordering variables). For example, let’s first order our data frame by party, and then within party order by agreement with Trump:\n\ntrump_scores |&gt; \n  filter(num_votes &gt;= 10) |&gt; \n  arrange(party, agree)\n\n# A tibble: 115 × 8\n   bioguide last_name  state party num_votes agree agree_pred margin_trump\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 L000570  Luján      NM    D           186 0.124      0.243        -8.21\n 2 G000555  Gillibrand NY    D           121 0.124      0.242       -22.5 \n 3 M001176  Merkley    OR    D           129 0.155      0.323       -11.0 \n 4 W000817  Warren     MA    D           116 0.155      0.216       -27.2 \n 5 B001288  Booker     NJ    D           119 0.160      0.290       -14.1 \n 6 S000033  Sanders    VT    D           112 0.161      0.221       -26.4 \n 7 H001075  Harris     CA    D           116 0.164      0.209       -30.1 \n 8 M000133  Markey     MA    D           127 0.165      0.213       -27.2 \n 9 W000779  Wyden      OR    D           129 0.186      0.323       -11.0 \n10 B001277  Blumenthal CT    D           128 0.203      0.294       -13.6 \n# ℹ 105 more rows\n\n\n\n\n\n\n\n\nExercise\n\n\n\nArrange the data by diff_pred, the difference between agreement and predicted agreement with Trump. (You should have code on how to create this variable from the last exercise). Your code:\n\n\n\n\n2.2.6 Summarizing data\ndplyr makes summarizing data a breeze using the summarize() function:\n\ntrump_scores |&gt; \n  summarize(mean_agree = mean(agree),\n            mean_agree_pred = mean(agree_pred))\n\n# A tibble: 1 × 2\n  mean_agree mean_agree_pred\n       &lt;dbl&gt;           &lt;dbl&gt;\n1      0.592           0.572\n\n\nTo make summaries, we can use any function that takes a vector and returns one value. Another example:\n\ntrump_scores |&gt; \n  filter(num_votes &gt;= 5) |&gt; # to filter out senators with few data points\n  summarize(max_agree = max(agree),\n            min_agree = min(agree))\n\n# A tibble: 1 × 2\n  max_agree min_agree\n      &lt;dbl&gt;     &lt;dbl&gt;\n1         1     0.124\n\n\nGrouped summaries allow us to disaggregate summaries according to other variables (usually categorical):\n\ntrump_scores |&gt; \n  filter(num_votes &gt;= 5) |&gt; # to filter out senators with few data points\n  summarize(mean_agree = mean(agree),\n            max_agree = max(agree),\n            min_agree = min(agree),\n            .by = party) # to group by party\n\n# A tibble: 2 × 4\n  party mean_agree max_agree min_agree\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 R          0.876     1         0.651\n2 D          0.272     0.548     0.124\n\n\n\n\n\n\n\n\nExercise\n\n\n\nObtain the maximum absolute difference in agreement with Trump (the abs_diff_agree variable from before) for each party.\n\n\n\n\n2.2.7 Overview\n\n\n\nFunction\nPurpose\n\n\n\n\nselect()\nSelect columns\n\n\nrename()\nRename columns\n\n\nmutate()\nCreating columns\n\n\nfilter()\nFiltering rows\n\n\narrange()\nOrdering rows\n\n\nsummarize()\nSummarizing data\n\n\nsummarize(…, .by = )\nSummarizing data (by groups)"
  },
  {
    "objectID": "02_tidy_data1.html#visualizing-data-with-ggplot2",
    "href": "02_tidy_data1.html#visualizing-data-with-ggplot2",
    "title": "2  Tidy data analysis I",
    "section": "2.3 Visualizing data with ggplot2",
    "text": "2.3 Visualizing data with ggplot2\nggplot2 is the package in charge of data visualization in the tidyverse. It is extremely flexible and allows us to draw bar plots, box plots, histograms, scatter plots, and many other types of plots (see examples at R Charts).\nThroughout this module we will use a subset of our data frame, which only includes senators with more than a few data points:\n\ntrump_scores_ss &lt;- trump_scores |&gt; \n  filter(num_votes &gt;= 10)\n\nThe ggplot2 syntax provides a unifying interface (the “grammar of graphics” or “gg”) for drawing all different types of plots. One draws plots by adding different “layers,” and the core code always includes the following:\n\nA ggplot() command with a data = argument specifying a data frame and a mapping = aes() argument specifying “aesthetic mappings,” i.e., how we want to use the columns in the data frame in the plot (for example, in the x-axis, as color, etc.).\n“geoms,” such as geom_bar() or geom_point(), specifying what to draw on the plot.\n\nSo all ggplot2 commands will have at least three elements: data, aesthetic mappings, and geoms.\n\n2.3.1 Univariate plots: categorical\nLet’s see an example of a bar plot with a categorical variable:\n\nggplot(data = trump_scores_ss, mapping = aes(x = party)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAs with any other function, we can drop the argument names if we specify the argument values in order. This is common in ggplot2 code:\n\nggplot(trump_scores_ss, aes(x = party)) +\n  geom_bar()\n\n\n\n\n\n\nNotice how geom_bar() automatically computes the number of observations in each category for us. Sometimes we want to use numbers in our data frame as part of a bar plot. Here we can use the geom_col() geom specifying both x and y aesthetic mappings, in which is sometimes called a “column plot:”\n\nggplot(trump_scores_ss |&gt; filter(state == \"ME\"),\n       aes(x = last_name, y = agree)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nDraw a column plot with the agreement with Trump of Bernie Sanders and Ted Cruz. What happens if you use last_name as the y aesthetic mapping and agree in the x aesthetic mapping? Your code:\n\n\nA common use of geom_col() is to create “ranking plots.” For example, who are the senators with highest agreement with Trump? We can start with something like this:\n\nggplot(trump_scores_ss,\n       aes(x = agree, y = last_name)) +\n  geom_col()\n\n\n\n\nWe might want to (1) select the top 10 observations and (2) order the bars according to the agree values. We can do these operations with slice_max() and fct_reorder(), as shown below:\n\nggplot(trump_scores_ss |&gt; slice_max(agree, n = 10),\n       aes(x = agree, y = fct_reorder(last_name, agree))) +\n  geom_col()\n\n\n\n\nWe can also plot the senators with the lowest agreement with Trump using slice_min() and fct_reorder() with a minus sign in the ordering variable:\n\nggplot(trump_scores_ss |&gt; slice_min(agree, n = 10),\n       aes(x = agree, y = fct_reorder(last_name, -agree))) +\n  geom_col()\n\n\n\n\n\n\n2.3.2 Univariate plots: numerical\nWe can draw a histogram with geom_histogram():\n\nggplot(trump_scores_ss, aes(x = agree)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNotice the warning message above. It’s telling us that, by default, geom_histogram() will draw 30 bins. Sometimes we want to modify this behavior. The following code has some common options for geom_histogram() and their explanations:\n\nggplot(trump_scores_ss, aes(x = agree)) +\n  geom_histogram(binwidth = 0.05,   # draw bins every 0.05 jumps in x\n                 boundary = 0,      # don't shift bins to integers\n                 closed   = \"left\") # close bins on the left\n\n\n\n\nSometimes we want to manually alter a scale. This is accomplished with the scale_*() family of ggplot2 functions. Here we use the scale_x_continuous() function to make the x-axis go from 0 to 1:\n\nggplot(trump_scores_ss, aes(x = agree)) +\n  geom_histogram(binwidth = 0.05, boundary = 0, closed   = \"left\") +   \n  scale_x_continuous(limits = c(0, 1))\n\n\n\n\nAdding the fill aesthetic mapping to a histogram will divide it according to a categorical variable. This is actually a bivariate plot!\n\nggplot(trump_scores_ss, aes(x = agree, fill = party)) +\n  geom_histogram(binwidth = 0.05, boundary = 0, closed   = \"left\") +   \n  scale_x_continuous(limits = c(0, 1)) +\n  # change default colors:\n  scale_fill_manual(values = c(\"D\" = \"blue\", \"R\" = \"red\"))\n\n\n\n\n\n\n2.3.3 Bivariate plots\nAnother common bivariate plot for categorical and numerical variables is the grouped box plot:\n\nggplot(trump_scores_ss, aes(x = agree, y = party)) +\n  geom_boxplot() +\n  scale_x_continuous(limits = c(0, 1)) # same change as before\n\n\n\n\nFor bivariate plots of numerical variables, scatter plots are made with geom_point():\n\nggplot(trump_scores_ss, aes(x = margin_trump, y = agree)) +\n  geom_point()\n\n\n\n\nWe can add the color aesthetic mapping to add a third variable:\n\nggplot(trump_scores_ss, aes(x = margin_trump, y = agree, color = party)) +\n  geom_point() +\n  scale_color_manual(values = c(\"D\" = \"blue\", \"R\" = \"red\"))\n\n\n\n\nLet’s finish our plot with the labs() function, which allows us to add labels to our aesthetic mappings, as well as titles and notes:\n\nggplot(trump_scores, aes(x = margin_trump, y = agree, color = party)) +\n  geom_point() +\n  scale_color_manual(values = c(\"D\" = \"blue\", \"R\" = \"red\")) +\n  labs(x = \"Trump margin in the senator's state (p.p.)\",\n       y = \"Votes in agreement with Trump (prop.)\",\n       color = \"Party\",\n       title = \"Relationship between Trump margins and senators' votes\",\n       caption = \"Data source: FiveThirtyEight (2021)\")\n\n\n\n\nWe will review a few more customization options, including text labels and facets, in a subsequent module.\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. First edition. Beijing ; Boston: O’Reilly.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/."
  },
  {
    "objectID": "03_matrices.html#introduction",
    "href": "03_matrices.html#introduction",
    "title": "3  Matrices",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\n\n3.1.1 Scalars\nOne number (for example, 12) is referred to as a scalar.\n\\[\na = 12\n\\]\n\n\n3.1.2 Vectors\nWe can put several scalars together to make a vector. Here is an example:\n\\[\n\\overrightarrow b =\n\\begin{bmatrix}\n  12 \\\\\n  14 \\\\\n  15\n\\end{bmatrix}\n\\]\nSince this is a column of numbers, we cleverly refer to it as a column vector.\nHere is another example of a vector, this time represented as a row vector:\n\\[\n\\overrightarrow c = \\begin{bmatrix}\n  12 & 14 & 15\n\\end{bmatrix}\n\\]\nColumn vectors are possibly more common and useful, but we sometimes write things down using row vectors to\nVectors are fairly easy to construct in R. As we saw before, we can use the c() function to combine elements:\n\nc(5, 25, -2, 1)\n\n[1]  5 25 -2  1\n\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember that the code above does not create any objects. To do so, you’d need to use the assignment operator (&lt;-):\n\nvector_example &lt;- c(5, 25, -2, 1)\nvector_example\n\n[1]  5 25 -2  1\n\n\n\n\nOr we can also create vectors from sequences with the : operator or the seq() function:\n\n10:20\n\n [1] 10 11 12 13 14 15 16 17 18 19 20\n\n\n\nseq(from = 3, to = 27, by = 3)\n\n[1]  3  6  9 12 15 18 21 24 27"
  },
  {
    "objectID": "03_matrices.html#operators",
    "href": "03_matrices.html#operators",
    "title": "3  Matrices",
    "section": "3.2 Operators",
    "text": "3.2 Operators\n\n3.2.1 Summation\nThe summation operator \\(\\sum\\) (i.e., the uppercase Sigma letter) lets us perform an operation on a sequence of numbers, which is often but not always a vector.\n\\[\\overrightarrow d = \\begin{bmatrix}\n12 & 7 & -2 & 3 & -1\n\\end{bmatrix}\\]\nWe can then calculate the sum of the first three elements of the vector, which is expressed as follows: \\[\\sum_{i=1}^3 d_i\\]\nThen we do the following math: \\[12+7+(-2)=17\\]\nIt is also common to use \\(n\\) in the superscript to indicate that we want to sum all elements:\n\\[\n\\sum_{i=1}^n d_i = 12 + 7 + (-2) + 3 + (-1) = 19\n\\] We can perform these operations using the sum() function in R:\n\nvector_d &lt;- c(12, 7, -2, 3, -1)\n\n\nsum(vector_d[1:3])\n\n[1] 17\n\n\n\nsum(vector_d)\n\n[1] 19\n\n\n\n\n3.2.2 Product\nThe product operator \\(\\prod\\) (i.e., the uppercase Pi letter) can also perform operations over a sequence of elements in a vector. Recall our previous vector:\n\\[\\overrightarrow d = \\begin{bmatrix}\n12 & 7 & -2 & 3 & 1\n\\end{bmatrix}\\]\nWe might want to calculate the product of all its elements, which is expressed as follows: \\[\\prod_{i=1}^n d_i = 12 \\cdot 7 \\cdot (-2) \\cdot 3 \\cdot (-1) = 504\\]\nIn R, we can compute products using the prod() function:\n\nprod(vector_d)\n\n[1] 504\n\n\n\n\n\n\n\n\nExercise\n\n\n\nGet the product of the first three elements of vector \\(d\\). Write the notation by hand and use R to obtain the number."
  },
  {
    "objectID": "03_matrices.html#matrices",
    "href": "03_matrices.html#matrices",
    "title": "3  Matrices",
    "section": "3.3 Matrices",
    "text": "3.3 Matrices\n\n3.3.1 Basics\nWe can append vectors together to form a matrix:\n\\[A = \\begin{bmatrix}\n12 & 14 & 15 \\\\\n115 & 22 & 127 \\\\\n193 & 29 & 219\n\\end{bmatrix}\\]\nThe number of rows and columns of a matrix constitute the dimensions of the matrix. The first number is the number of rows (“r”) and the second number is the number of columns (“c”) in the matrix.\n\n\n\n\n\n\nImportant\n\n\n\nFind a way to remember “r x c” permanently. The order of the dimensions never changes.\n\n\nMatrix \\(A\\) above, for example, is a \\(3x3\\) matrix. Sometimes we’d refer to it as \\(A_{3x3}\\).\n\n\n\n\n\n\nTip\n\n\n\nIt is common to use capital letters (sometimes bold-faced) to represent matrices. In contrast, vectors are usually represented with either bold lowercase letters or lowercase letters with an arrow on top (e.g., \\(\\overrightarrow v\\)).\n\n\n\nConstructing matrices in R\nThere are different ways to create matrices in R. One of the simplest is via rbind() or cbind(), which paste vectors together (either by rows or by columns):\n\n# Create some vectors\nvector1 &lt;- 1:4\nvector2 &lt;- 5:8\nvector3 &lt;- 9:12\nvector4 &lt;- 13:16\n\n\n# Using rbind(), each vector will be a row \nrbind_mat &lt;- rbind(vector1, vector2, vector3, vector4)\nrbind_mat\n\n        [,1] [,2] [,3] [,4]\nvector1    1    2    3    4\nvector2    5    6    7    8\nvector3    9   10   11   12\nvector4   13   14   15   16\n\n\n\n# Using cbind(), each vector will be a column\ncbind_mat &lt;- cbind(vector1, vector2, vector3, vector4)\ncbind_mat\n\n     vector1 vector2 vector3 vector4\n[1,]       1       5       9      13\n[2,]       2       6      10      14\n[3,]       3       7      11      15\n[4,]       4       8      12      16\n\n\nAn alternative is to use to properly named matrix() function. The basic syntax is matrix(data, nrow, ncol, byrow):\n\ndata is the input vector which becomes the data elements of the matrix.\nnrow is the number of rows to be created.\nncol is the number of columns to be created.\nbyrow is a logical clue. If TRUE then the input vector elements are arranged by row. By default (FALSE), elements are arranged by column.\n\nLet’s see some examples:\n\n# Elements are arranged sequentially by row.\nM &lt;- matrix(c(1:12), nrow = 4, byrow = T)\nM\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n[4,]   10   11   12\n\n\n\n# Elements are arranged sequentially by column (byrow = F by default).\nN &lt;- matrix(c(1:12), nrow = 4)\nN\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n\n\n\n\n3.3.2 Structure\nHow do we refer to specific elements of the matrix? For example, matrix \\(A\\) is an \\(m\\times n\\) matrix where \\(m=n=3\\). This is sometimes called a square matrix.\nMore generally, matrix \\(B\\) is an \\(m\\times n\\) matrix where the elements look like this: \\[B=\n\\begin{bmatrix}\nb_{11} & b_{12} & b_{13} & \\ldots & b_{1n} \\\\\nb_{21} & b_{22} & b_{23} & \\ldots & b_{2n} \\\\\n\\vdots & \\vdots & \\vdots & \\ldots & \\vdots \\\\\nb_{m1} & b_{m2} & b_{m3} & \\ldots & b_{mn}\n\\end{bmatrix}\\]\nThus \\(b_{23}\\) refers to the second unit down and third across. More generally, we refer to row indices as \\(i\\) and to column indices as \\(j\\).\nIn R, we can access a matrix’s elements using square brackets:\n\n# In matrix N, access the element at 1st row and 3rd column.\nN[1,3]\n\n[1] 9\n\n\n\n# In matrix N, access the element at 4th row and 2nd column.\nN[4,2]\n\n[1] 8\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen trying to identify a specific element, the first subscript is the element’s row and the second subscript is the element’s column (always in that order)."
  },
  {
    "objectID": "03_matrices.html#matrix-operations",
    "href": "03_matrices.html#matrix-operations",
    "title": "3  Matrices",
    "section": "3.4 Matrix operations",
    "text": "3.4 Matrix operations\n\n3.4.1 Addition and subtraction\n\nAddition and subtraction are straightforward operations.\nMatrices must have exactly the same dimensions for both of these operations.\nWe add or subtract each element with the corresponding element from the other matrix.\nThis is expressed as follows:\n\n\\[A \\pm B=C\\]\n\\[c_{ij}=a_{ij} \\pm b_{ij} \\text{ }\\forall i,j\\]\n\\[\\begin{bmatrix}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix}\n\\pm\n\\begin{bmatrix}\nb_{11} & b_{12} & b_{13}\\\\\nb_{21} & b_{22} & b_{23}\\\\\nb_{31} & b_{32} & b_{33}\n\\end{bmatrix}\\] \\[=\\] \\[\\begin{bmatrix}\na_{11}\\pm b_{11} & a_{12}\\pm b_{12} & a_{13}\\pm b_{13}\\\\\na_{21}\\pm b_{21} & a_{22}\\pm b_{22} & a_{23}\\pm b_{23}\\\\\na_{31}\\pm b_{31} & a_{32}\\pm b_{32} & a_{33}\\pm b_{33}\n\\end{bmatrix}\\]\n\nAddition and subtraction in R\nWe start by creating two 2x3 matrices:\n\n# Create two 2x3 matrices.\nmatrix1 &lt;- matrix(c(3, 9, -1, 4, 2, 6), nrow = 2)\nmatrix1\n\n     [,1] [,2] [,3]\n[1,]    3   -1    2\n[2,]    9    4    6\n\n\n\nmatrix2 &lt;- matrix(c(5, 2, 0, 9, 3, 4), nrow = 2)\nmatrix2\n\n     [,1] [,2] [,3]\n[1,]    5    0    3\n[2,]    2    9    4\n\n\nWe can simply use the + and - operators for addition and substraction:\n\nmatrix1 + matrix2\n\n     [,1] [,2] [,3]\n[1,]    8   -1    5\n[2,]   11   13   10\n\n\n\nmatrix1 - matrix2\n\n     [,1] [,2] [,3]\n[1,]   -2   -1   -1\n[2,]    7   -5    2\n\n\n\n\n\n\n\n\nExercise\n\n\n\n(Use code for one of these and do the other one by hand!)\n1) Calculate \\(A + B\\)\n\\[A= \\begin{bmatrix}\n1 & 0 \\\\\n-2 & -1\n\\end{bmatrix}\\]\n\\[B = \\begin{bmatrix}\n5 & 1 \\\\\n2 & -1\n\\end{bmatrix}\\]\n\n2) Calculate \\(A - B\\)\n\\[A= \\begin{bmatrix}\n6 & -2 & 8 & 12 \\\\\n4 & 42 & 8 & -6\n\\end{bmatrix}\\]\n\\[B = \\begin{bmatrix}\n18 & 42 & 3 & 7 \\\\\n0 & -42 & 15 & 4\n\\end{bmatrix}\\]\n\n\n\n\n\n3.4.2 Scalar multiplication\nScalar multiplication is very intuitive. As we know, a scalar is a single number. We multiply each value in the matrix by the scalar to perform this operation.\nFormally, this is expressed as follows: \\[A =\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix}\\] \\[cA =\n\\begin{bmatrix}\nca_{11} & ca_{12} & ca_{13}\\\\\nca_{21} & ca_{22} & ca_{23}\\\\\nca_{31} & ca_{32} & ca_{33}\n\\end{bmatrix}\\]\nIn R, all we need to do is take an established matrix and multiply it by some scalar:\n\n# matrix1 from our previous example\nmatrix1\n\n     [,1] [,2] [,3]\n[1,]    3   -1    2\n[2,]    9    4    6\n\n\n\nmatrix1 * 3\n\n     [,1] [,2] [,3]\n[1,]    9   -3    6\n[2,]   27   12   18\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCalculate \\(2\\times A\\) and \\(-3 \\times B\\). Again, do one by hand and the other one using R.\n\\[A= \\begin{bmatrix}\n    1 & 4 & 8 \\\\\n    0 & -1 & 3\n    \\end{bmatrix}\\] \\[ B = \\begin{bmatrix}\n    -15 & 1 & 5 \\\\\n    2 & -42 & 0 \\\\\n    7 & 1 & 6\n    \\end{bmatrix}\\]\n\n\n\n\n3.4.3 Matrix multiplication\n\nMultiplying matrices is slightly trickier than multiplying scalars.\nTwo matrices must be conformable for them to be multiplied together. This means that the number of columns in the first matrix equals the number of rows in the second.\nWhen multiplying \\(A \\times B\\), if \\(A\\) is \\(m \\times n\\), \\(B\\) must have \\(n\\) rows.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe conformability requirement never changes. Before multiplying anything, check to make sure the matrices are indeed conformable.\n\n\n\nThe resulting matrix will have the same number of rows as the first matrix and the number of columns in the second. For example, if \\(A\\) is \\(i \\times k\\) and \\(B\\) is \\(k \\times j\\), then \\(A \\times B\\) will be \\(i \\times j\\).\n\n\nWhich of the following can we multiply? What will be the dimensions of the resulting matrix? \\[\\begin{aligned}\nB=\n\\begin{bmatrix}\n2 \\\\\n3\\\\\n4\\\\\n1\n\\end{bmatrix}\nM =\n\\begin{bmatrix}\n1 & 0 & 2\\\\\n1 & 2 & 4\\\\\n2 & 3 & 2\n\\end{bmatrix}\nL =\n\\begin{bmatrix}\n6 & 5 & -1\\\\\n1 & 4 & 3\n\\end{bmatrix}\n\\end{aligned}\\]\nWhy can’t we multiply in the opposite order?\n\n\n\n\n\n\nWarning\n\n\n\nWhen multiplying matrices, order matters. Even if multiplication is possible in both directions, in general \\(AB \\neq BA\\).\n\n\n\nMultiplication steps\n\nMultiply each row by each column, summing up each pair of multiplied terms.\n\n\n\n\n\n\n\nTip\n\n\n\nThis is sometimes to referred to as the “dot product,” where we multiply matching members, then sum up.\n\n\n\nThe element in position \\(ij\\) is the sum of the products of elements in the \\(i\\)th row of the first matrix (\\(A\\)) and the corresponding elements in the \\(j\\)th column of the second matrix (\\(B\\)). \\[c_{ij}=\\sum_{k=1}^n a_{ik}b_{kj}\\]\n\n\n\nExample\nSuppose a company manufactures two kinds of furniture: chairs and sofas.\n\nA chair costs $100 for wood, $270 for cloth, and $130 for feathers.\nEach sofa costs $150 for wood, $420 for cloth, and $195 for feathers.\n\n\n\n\n\nChair\nSofa\n\n\n\n\nWood\n100\n150\n\n\nCloth\n270\n420\n\n\nFeathers\n130\n195\n\n\n\nThe same information about unit cost (\\(C\\)) can be presented as a matrix.\n\\[C = \\begin{bmatrix}\n100 & 150\\\\\n270 & 420\\\\\n130 & 195\n\\end{bmatrix}\\]\nNote that each of the three rows of this 3 x 2 matrix represents a material (wood, cloth, or feathers), and each of the two columns represents a product (chair or coach). The elements are the unit cost (in USD).\n\nNow, suppose that the company will produce 45 chairs and 30 sofas this month. This production quantity can be represented in the following table, and also as a 2 x 1 matrix (\\(Q\\)):\n\n\n\nProduct\nQuantity\n\n\n\n\nChair\n45\n\n\nSofa\n30\n\n\n\n\\[Q = \\begin{bmatrix}\n45 \\\\\n30\n\\end{bmatrix}\\]\nWhat will be the company’s total cost? The “total expenditure” is equal to the “unit cost” times the “production quantity” (the number of units).\nThe total expenditure (\\(E\\)) for each material this month is calculated by multiplying these two matrices.\n\\[\\begin{aligned} E = CQ =\n\\begin{bmatrix}\n100 & 150\\\\\n270 & 420\\\\\n130 & 195\n\\end{bmatrix}\n\\begin{bmatrix}\n45 \\\\\n30\n\\end{bmatrix} =\n\\begin{bmatrix}\n(100)(45) + (150)(30) \\\\\n(270)(45) + (420)(30) \\\\\n(130)(45) + (195)(30)\n\\end{bmatrix} =\n\\begin{bmatrix}\n9,000 \\\\\n24,750 \\\\\n11,700\n\\end{bmatrix}\n\\end{aligned}\\]\nMultiplying the 3x2 Cost matrix (\\(C\\)) times the 2x1 Quantity matrix (\\(Q\\)) yields the 3x1 Expenditure matrix (\\(E\\)).\nAs a result of this matrix multiplication, we determine that this month the company will incur expenditures of:\n\n$9,000 for wood\n$24,750 for cloth\n$11,700 for feathers.\n\n\n\nMatrix multiplication in R\nBefore attempting matrix multiplication, we must make sure the matrices are conformable (as we do for our manual calculations).\nThen we can multiply our matrices together using the %*% operator.\n\nC &lt;- matrix(c(100, 270, 130, 150, 420, 195), nrow = 3)\nC\n\n     [,1] [,2]\n[1,]  100  150\n[2,]  270  420\n[3,]  130  195\n\n\n\nQ &lt;- matrix(c(45, 30), nrow = 2)\nQ\n\n     [,1]\n[1,]   45\n[2,]   30\n\n\n\nC %*% Q\n\n      [,1]\n[1,]  9000\n[2,] 24750\n[3,] 11700\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you have a missing value or NA in one of the matrices you are trying to multiply (something we will discuss in further detail in the next module), you will have NAs in your resulting matrix.\n\n\n\n\n\n\n3.4.4 Properties of operations\n\nAddition and subtraction:\n\nAssociative: \\((A \\pm B) \\pm C = A \\pm (B \\pm C)\\)\nCommunicative: \\(A \\pm B = B \\pm A\\)\n\nMultiplication:\n\n\\(AB \\neq BA\\)\n\\(A(BC) = (AB)C\\)\n\\(A(B+C) = AB + AC\\)\n\\((A+B)C = AC + BC\\)"
  },
  {
    "objectID": "03_matrices.html#special-matrices",
    "href": "03_matrices.html#special-matrices",
    "title": "3  Matrices",
    "section": "3.5 Special matrices",
    "text": "3.5 Special matrices\nSquare matrix\n\nIn a square matrix, the number of rows equals the number of columns (\\(m=n\\)):\nThe diagonal of a matrix is a set of numbers consisting of the elements on the line from the upper-left-hand to the lower-right-hand corner of the matrix. Diagonals are particularly useful in square matrices.\nThe trace of a matrix, denoted as \\(tr(A)\\), is the sum of the diagonal elements of the matrix.\n\nDiagonal matrix:\n\nIn a diagonal matrix, all of the elements of the matrix that are not on the diagonal are equal to zero.\n\nScalar matrix:\n\nA scalar matrix is a diagonal matrix where the diagonal elements are all equal to each other. In other words, we’re really only concerned with one scalar (or element) held in the diagonal.\n\nIdentity matrix:\n\nThe identity matrix is a scalar matrix with all of the diagonal elements equal to one.\nRemember that, as with all diagonal matrices, the off-diagonal elements are equal to zero.\nThe capital letter \\(I\\) is reserved for the identity matrix. For convenience, a 3x3 identity matrix can be denoted as \\(I_3\\)."
  },
  {
    "objectID": "03_matrices.html#transpose",
    "href": "03_matrices.html#transpose",
    "title": "3  Matrices",
    "section": "3.6 Transpose",
    "text": "3.6 Transpose\nThe transpose is the original matrix with the rows and the columns interchanged.\nThe notation is either \\(J'\\) (“J prime”) or \\(J^T\\) (“J transpose”).\n\\[J =\n\\begin{bmatrix}\n4 & 5\\\\\n3 & 0\\\\\n7 & -2\n\\end{bmatrix}\\]\n\\[J' = J^T =\n\\begin{bmatrix}\n4 & 3 & 7 \\\\\n5 & 0 & -2\n\\end{bmatrix}\\]\nIn R, we use t() to get the transpose.\n\nJ &lt;- matrix(c(4, 3, 7, 5, 0, -2), ncol = 2)\nJ\n\n     [,1] [,2]\n[1,]    4    5\n[2,]    3    0\n[3,]    7   -2\n\n\n\nt(J)\n\n     [,1] [,2] [,3]\n[1,]    4    3    7\n[2,]    5    0   -2"
  },
  {
    "objectID": "03_matrices.html#inverse",
    "href": "03_matrices.html#inverse",
    "title": "3  Matrices",
    "section": "3.7 Inverse",
    "text": "3.7 Inverse\n\nJust like a number has a reciprocal, a matrix has an inverse.\nWhen we multiply a matrix by its inverse we get the identity matrix (which is like “1” for matrices).\n\n\\[A × A^{-1} = I\\]\n\nThe inverse of A is \\(A^{-1}\\) only when:\n\n\\[AA^{-1} = A^{-1}A = I\\]\n\nSometimes there is no inverse at all.\n\n\n\n\n\n\n\nNote\n\n\n\nFor now, don’t worry about calculating the inverse of a matrix manually. This is the type of task we use R for.\n\n\n\nIn R, we use the solve() function to calculate the inverse of a matrix:\n\n\nA &lt;- matrix(c(3, 2, 5, 2, 3, 2, 5, 2, 4), ncol = 3)\nA\n\n     [,1] [,2] [,3]\n[1,]    3    2    5\n[2,]    2    3    2\n[3,]    5    2    4\n\n\n\nsolve(A)\n\n            [,1]        [,2]       [,3]\n[1,] -0.29629630 -0.07407407  0.4074074\n[2,] -0.07407407  0.48148148 -0.1481481\n[3,]  0.40740741 -0.14814815 -0.1851852"
  },
  {
    "objectID": "03_matrices.html#linear-systems-and-matrices",
    "href": "03_matrices.html#linear-systems-and-matrices",
    "title": "3  Matrices",
    "section": "3.8 Linear systems and matrices",
    "text": "3.8 Linear systems and matrices\n\nA system of equations can be represented by an augmented matrix.\nSystem of equations: \\[{\\color{red}{3}}x + {\\color{green}{6}}y = {\\color{blue}{12}}\\] \\[{\\color{red}{5}}x + {\\color{green}{10}}y = {\\color{blue}{25}}\\]\nIn an augmented matrix, each row represents one equation in the system and each column represents a variable or the constant terms. \\[\\begin{bmatrix}\n{\\color{red}{3}} & {\\color{green}{6}} & {\\color{blue}{12}}\\\\\n{\\color{red}{5}} & {\\color{green}{10}} & {\\color{blue}{25}}\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "03_matrices.html#ols-and-matrices",
    "href": "03_matrices.html#ols-and-matrices",
    "title": "3  Matrices",
    "section": "3.9 OLS and matrices",
    "text": "3.9 OLS and matrices\n\nWe can use the logic above to calculate estimates for our ordinary least squares (OLS) models.\nOLS is a linear regression technique used to find the best-fitting line for a set of data points (observations) by minimizing the residuals (the differences between the observed and predicted values).\nWe minimize the sum of the squared errors.\n\n\n3.9.1 Dependent variable\n\nSuppose, for example, we have a sample consisting of \\(n\\) observations.\nThe dependent variable is denoted as an \\(n \\times1\\) column vector.\n\n\\[Y = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\ny_3 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\\]\n\n\n3.9.2 Independent variables\n\nSuppose there are \\(k\\) independent variables and a constant term, meaning \\(k+1\\) columns and \\(n\\) rows.\nWe can represent these variables as an \\(n \\times (k+1)\\) matrix, expressed as follows:\n\n\\[X= \\begin{bmatrix}\n1 & x_{11} & \\dots & x_{1k} \\\\\n1 & x_{21} & \\dots & x_{2k} \\\\\n\\vdots & \\vdots & \\dots & \\vdots \\\\\n1 & x_{n1} & \\dots & x_{nk}\n\\end{bmatrix}\\]\n\n\\(x_{ij}\\) is the \\(i\\)-th observation of the \\(j\\)-th independent variable.\n\n\n\n3.9.3 Linear regression model\n\nLet’s say we have 173 observations (n = 173) and 2 IVs (k = 3).\nThis can be expressed as the following linear equation: \\[y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\epsilon\\]\nIn matrix form, we have: \\[\\begin{aligned} \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} = \\begin{bmatrix}\n1 & x_{11} & x_{21} \\\\\n1 & x_{21} & x_{22} \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & x_{1173} & x_{2173}\n\\end{bmatrix} \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2\n\\end{bmatrix} + \\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\vdots \\\\\n\\epsilon_{173}\n\\end{bmatrix}\\end{aligned} \\]\nAll 173 equations can be represented by: \\[y=X\\beta+\\epsilon\\]\n\n\n\n3.9.4 Estimates\n\nWithout getting too much into the mechanics, we can calculate our coefficient estimates with matrix algebra using the following equation:\n\n\\[\\hat{\\beta} = (X'X)^{-1}X'Y\\]\n\nRead aloud, we say “X prime X inverse, X prime Y”.\nThe little hat on our beta (\\(\\hat{\\beta}\\)) signifies that these are estimates.\nRemember, the OLS method is to choose \\(\\hat{\\beta}\\) such that the sum of squared residuals (“SSR”) is minimized.\n\n\n3.9.4.1 Example in R\n\nWe will load the mtcars data set (our favorite) for this example, which contains data about many different car models.\n\n\ncars_df &lt;- mtcars\n\n\nNow, we want to estimate the association between hp (horsepower) and wt (weight), our independent variables, and mpg (miles per gallon), our dependent variable.\nFirst, we transform our dependent variable into a matrix, using the as.matrix function and specifying the column of the mtcars data set to create a column vector of our observed values for the DV.\n\n\nY &lt;- as.matrix(cars_df$mpg)\nY\n\n      [,1]\n [1,] 21.0\n [2,] 21.0\n [3,] 22.8\n [4,] 21.4\n [5,] 18.7\n [6,] 18.1\n [7,] 14.3\n [8,] 24.4\n [9,] 22.8\n[10,] 19.2\n[11,] 17.8\n[12,] 16.4\n[13,] 17.3\n[14,] 15.2\n[15,] 10.4\n[16,] 10.4\n[17,] 14.7\n[18,] 32.4\n[19,] 30.4\n[20,] 33.9\n[21,] 21.5\n[22,] 15.5\n[23,] 15.2\n[24,] 13.3\n[25,] 19.2\n[26,] 27.3\n[27,] 26.0\n[28,] 30.4\n[29,] 15.8\n[30,] 19.7\n[31,] 15.0\n[32,] 21.4\n\n\n\nNext, we do the same thing for our independent variables of interest, and our constant.\n\n\n# create two separate matrices for IVs\nX1 &lt;- as.matrix(cars_df$hp)\nX2 &lt;- as.matrix(cars_df$wt)\n\n# create constant column\n\n# bind them altogether into one matrix\nconstant &lt;-  rep(1, nrow(cars_df))\nX &lt;- cbind(constant, X1, X2)\nX\n\n      constant          \n [1,]        1 110 2.620\n [2,]        1 110 2.875\n [3,]        1  93 2.320\n [4,]        1 110 3.215\n [5,]        1 175 3.440\n [6,]        1 105 3.460\n [7,]        1 245 3.570\n [8,]        1  62 3.190\n [9,]        1  95 3.150\n[10,]        1 123 3.440\n[11,]        1 123 3.440\n[12,]        1 180 4.070\n[13,]        1 180 3.730\n[14,]        1 180 3.780\n[15,]        1 205 5.250\n[16,]        1 215 5.424\n[17,]        1 230 5.345\n[18,]        1  66 2.200\n[19,]        1  52 1.615\n[20,]        1  65 1.835\n[21,]        1  97 2.465\n[22,]        1 150 3.520\n[23,]        1 150 3.435\n[24,]        1 245 3.840\n[25,]        1 175 3.845\n[26,]        1  66 1.935\n[27,]        1  91 2.140\n[28,]        1 113 1.513\n[29,]        1 264 3.170\n[30,]        1 175 2.770\n[31,]        1 335 3.570\n[32,]        1 109 2.780\n\n\n\nNext, we calculate \\(X'X\\), \\(X'Y\\), and \\((X'X)^{-1}\\).\n\n\nDon’t forget to use %*% for matrix multiplication!\n\n\n# X prime X\nXpX &lt;- t(X) %*% X\n\n# X prime X inverse\nXpXinv &lt;- solve(XpX)\n\n# X prime Y\nXpY &lt;- t(X) %*% Y\n\n# beta coefficient estimates\nbhat &lt;- XpXinv %*% XpY\nbhat\n\n                [,1]\nconstant 37.22727012\n         -0.03177295\n         -3.87783074\n\n\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. First edition. Beijing ; Boston: O’Reilly.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/."
  },
  {
    "objectID": "04_tidy_data2.html#loading-data-in-different-formats.",
    "href": "04_tidy_data2.html#loading-data-in-different-formats.",
    "title": "4  Tidy data analysis II",
    "section": "4.1 Loading data in different formats.",
    "text": "4.1 Loading data in different formats.\nIn this module we will use cross-national data from the Quality of Government (QoG) project (Dahlberg et al., 2023).\nNotice how in the data/ folder we have multiple versions of the same dataset (a subset of the QOG basic dataset): .csv (comma-separated values), .rds (R), .xlsx (Excel), .dta (Stata), and .sav (SPSS).\n\n4.1.1 CSV and R data files\nWe can use the read_csv() and read_rds() functions from the tidyverse1 to read the .csv and .rds (R) data files:\n\nqog_csv &lt;- read_csv(\"data/sample_qog_bas_ts_jan23.csv\")\n\nRows: 1085 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): cname, ccodealp, region, ht_colonial\ndbl (4): year, wdi_pop, vdem_polyarchy, vdem_corr\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nqog_rds &lt;- read_rds(\"data/sample_qog_bas_ts_jan23.rds\")\n\nFor reading files from other software (Excel, Stata, or SPSS), we need to load additional packages. Luckily, they are automatically installed when one installs the tidyverse.\n\n\n4.1.2 Excel data files\nFor Excel files (.xls or .xlsx files), the readxl package has a handy read_excel() function.\n\nlibrary(readxl)\nqog_excel &lt;- read_excel(\"data/sample_qog_bas_ts_jan23.xlsx\")\n\n\n\n\n\n\n\nTip\n\n\n\nUseful arguments of the read_excel() function include sheet =, which reads particular sheets (specified via their positions or sheet names), and range =, which extracts a particular cell range (e.g., `A5:E25`).\n\n\n\n\n4.1.3 Stata and SPSS data files\nTo load files from Stata (.dta) or SPSS (.spss), one needs the haven package and its properly-named read_stata() and read_spss() functions:\n\nlibrary(haven)\nqog_stata &lt;- read_stata(\"data/sample_qog_bas_ts_jan23.dta\")\nqog_spss &lt;- read_spss(\"data/sample_qog_bas_ts_jan23.sav\")\n\n\n\n\n\n\n\nTip\n\n\n\nDatasets from Stata and SPSS can have additional properties, like variable labels and special types of missing values. To learn more about this, check out the “Labelled data” chapter from Danny Smith’s Survey Research Datasets and R (2020).\n\n\n\n\n4.1.4 Our data for this session\nWe will rename one of our objects to qog:\n\nqog &lt;- qog_csv\nqog\n\n# A tibble: 1,085 × 8\n   cname      ccodealp  year region wdi_pop vdem_polyarchy vdem_corr ht_colonial\n   &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      \n 1 Antigua a… ATG       1990 Carib…   63328             NA        NA British    \n 2 Antigua a… ATG       1991 Carib…   63634             NA        NA British    \n 3 Antigua a… ATG       1992 Carib…   64659             NA        NA British    \n 4 Antigua a… ATG       1993 Carib…   65834             NA        NA British    \n 5 Antigua a… ATG       1994 Carib…   67072             NA        NA British    \n 6 Antigua a… ATG       1995 Carib…   68398             NA        NA British    \n 7 Antigua a… ATG       1996 Carib…   69798             NA        NA British    \n 8 Antigua a… ATG       1997 Carib…   71218             NA        NA British    \n 9 Antigua a… ATG       1998 Carib…   72572             NA        NA British    \n10 Antigua a… ATG       1999 Carib…   73821             NA        NA British    \n# ℹ 1,075 more rows\n\n\nThis dataset is a small sample of QOG, which contains data for countries in the Americas from 1990 to 2020. The observational unit is thus country-year. You can access the full codebook online. The variables are as follows:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ncname\nCountry name\n\n\nccodealp\nCountry code (ISO-3 character convention)\n\n\nyear\nYear\n\n\nregion\nRegion (following legacy WDI convention). Added to QOG by us.\n\n\nwdi_pop\nTotal population, from the World Development Indicators\n\n\nvdem_polyarchy\nV-Dem’s polyarchy index (electoral democracy)\n\n\nvdem_corr\nV-Dem’s corruption index\n\n\nht_colonial\nFormer colonial ruler"
  },
  {
    "objectID": "04_tidy_data2.html#recoding-variables",
    "href": "04_tidy_data2.html#recoding-variables",
    "title": "4  Tidy data analysis II",
    "section": "4.2 Recoding variables",
    "text": "4.2 Recoding variables\nTake a look at the ht_colonial variable. We can do a simple tabulation with count():\n\nqog |&gt; \n  count(ht_colonial)\n\n# A tibble: 6 × 2\n  ht_colonial         n\n  &lt;chr&gt;           &lt;int&gt;\n1 British           372\n2 Dutch              31\n3 French             31\n4 Never colonized    62\n5 Portuguese         31\n6 Spanish           558\n\n\n\n\n\n\n\n\nTip\n\n\n\nAnother common way to compute quick tabulations in R is with the table() function. Be aware that this takes a vector as the input:\n\ntable(qog$ht_colonial)\n\n\n        British           Dutch          French Never colonized      Portuguese \n            372              31              31              62              31 \n        Spanish \n            558 \n\n\n\n\nWe might want to recode this variable. For instance, we could create a dummy/binary variable for whether the country was a British colony. We can do this with if_else(), which works with logical conditions:\n\nqog |&gt; \n  # the arguments are condition, true (what to do if true), false\n  mutate(d_britishcol = if_else(ht_colonial == \"British\", 1, 0)) |&gt; \n  count(d_britishcol)\n\n# A tibble: 2 × 2\n  d_britishcol     n\n         &lt;dbl&gt; &lt;int&gt;\n1            0   713\n2            1   372\n\n\nInstead of a numeric classification (0 and 1), we could use characters:\n\nqog |&gt; \n  mutate(cat_britishcol = if_else(ht_colonial == \"British\", \"British\", \"Other\")) |&gt; \n  count(cat_britishcol)\n\n# A tibble: 2 × 2\n  cat_britishcol     n\n  &lt;chr&gt;          &lt;int&gt;\n1 British          372\n2 Other            713\n\n\nif_else() is great for binary recoding. But sometimes we want to create more than two categories. We can use case_when():\n\nqog |&gt; \n  # syntax is condition ~ value\n  mutate(cat_col = case_when(\n    ht_colonial == \"British\" ~ \"British\",\n    ht_colonial == \"Spanish\" ~ \"Spanish\", \n    .default = \"Other\" # what to do in all other cases\n  )) |&gt; \n  count(cat_col)\n\n# A tibble: 3 × 2\n  cat_col     n\n  &lt;chr&gt;   &lt;int&gt;\n1 British   372\n2 Other     155\n3 Spanish   558\n\n\nThe .default = argument in case_when() can also be used to leave the variable as-is for non-specified cases. For example, let’s combine Portuguese and Spanish colonies:\n\nqog |&gt; \n  # syntax is condition ~ value\n  mutate(cat_col = case_when(\n    ht_colonial %in% c(\"Spanish\", \"Portuguese\") ~ \"Spanish/Portuguese\",\n    .default = ht_colonial # what to do in all other cases\n  )) |&gt; \n  count(cat_col)\n\n# A tibble: 5 × 2\n  cat_col                n\n  &lt;chr&gt;              &lt;int&gt;\n1 British              372\n2 Dutch                 31\n3 French                31\n4 Never colonized       62\n5 Spanish/Portuguese   589\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nCreate a dummy variable, d_large_pop, for whether the country-year has a population of more than 1 million. Then compute its mean. Your code:\nWhich countries are recorded as “Never colonized”? Change their values to other reasonable codings and compute a tabulation with count(). Your code:"
  },
  {
    "objectID": "04_tidy_data2.html#missing-values",
    "href": "04_tidy_data2.html#missing-values",
    "title": "4  Tidy data analysis II",
    "section": "4.3 Missing values",
    "text": "4.3 Missing values\nMissing values are commonplace in real datasets. In R, missing values are a special type of value in vectors, denoted as NA.\n\n\n\n\n\n\nWarning\n\n\n\nThe special value NA is different from the character value “NA”. For example, notice that a numeric vector can have NAs, while it obviously cannot hold the character value “NA”:\n\nc(5, 4.6, NA, 8)\n\n[1] 5.0 4.6  NA 8.0\n\n\n\n\nA quick way to check for missing values in small datasets is with the summary() function:\n\nsummary(qog)\n\n    cname             ccodealp              year         region         \n Length:1085        Length:1085        Min.   :1990   Length:1085       \n Class :character   Class :character   1st Qu.:1997   Class :character  \n Mode  :character   Mode  :character   Median :2005   Mode  :character  \n                                       Mean   :2005                     \n                                       3rd Qu.:2013                     \n                                       Max.   :2020                     \n                                                                        \n    wdi_pop          vdem_polyarchy     vdem_corr      ht_colonial       \n Min.   :    40542   Min.   :0.0710   Min.   :0.0260   Length:1085       \n 1st Qu.:   389131   1st Qu.:0.5570   1st Qu.:0.1890   Class :character  \n Median :  5687744   Median :0.7030   Median :0.5550   Mode  :character  \n Mean   : 25004057   Mean   :0.6569   Mean   :0.4922                     \n 3rd Qu.: 16195902   3rd Qu.:0.8030   3rd Qu.:0.7540                     \n Max.   :331501080   Max.   :0.9160   Max.   :0.9630                     \n                     NA's   :248      NA's   :248                        \n\n\nNotice that we have missingness in the vdem_polyarchy and vdem_corr variables. We might want to filter the dataset to see which observations are in this situation:\n\nqog |&gt; \n  filter(vdem_polyarchy == NA | vdem_corr == NA)\n\n# A tibble: 0 × 8\n# ℹ 8 variables: cname &lt;chr&gt;, ccodealp &lt;chr&gt;, year &lt;dbl&gt;, region &lt;chr&gt;,\n#   wdi_pop &lt;dbl&gt;, vdem_polyarchy &lt;dbl&gt;, vdem_corr &lt;dbl&gt;, ht_colonial &lt;chr&gt;\n\n\nBut the code above doesn’t work! To refer to missing values in logical conditions, we cannot use == NA. Instead, we need to use the is.na() function:\n\nqog |&gt; \n  filter(is.na(vdem_polyarchy) | is.na(vdem_corr))\n\n# A tibble: 248 × 8\n   cname      ccodealp  year region wdi_pop vdem_polyarchy vdem_corr ht_colonial\n   &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      \n 1 Antigua a… ATG       1990 Carib…   63328             NA        NA British    \n 2 Antigua a… ATG       1991 Carib…   63634             NA        NA British    \n 3 Antigua a… ATG       1992 Carib…   64659             NA        NA British    \n 4 Antigua a… ATG       1993 Carib…   65834             NA        NA British    \n 5 Antigua a… ATG       1994 Carib…   67072             NA        NA British    \n 6 Antigua a… ATG       1995 Carib…   68398             NA        NA British    \n 7 Antigua a… ATG       1996 Carib…   69798             NA        NA British    \n 8 Antigua a… ATG       1997 Carib…   71218             NA        NA British    \n 9 Antigua a… ATG       1998 Carib…   72572             NA        NA British    \n10 Antigua a… ATG       1999 Carib…   73821             NA        NA British    \n# ℹ 238 more rows\n\n\nNotice that, in most R functions, missing values are “contagious.” This means that any missing value will contaminate the operation and carry over to the results. For example:\n\nqog |&gt; \n  summarize(mean_vdem_polyarchy = mean(vdem_polyarchy))\n\n# A tibble: 1 × 1\n  mean_vdem_polyarchy\n                &lt;dbl&gt;\n1                  NA\n\n\nSometimes we’d like to perform our operations even in the presence of missing values, simply excluding them. Most basic R functions have an na.rm = argument to do this:\n\nqog |&gt; \n  summarize(mean_vdem_polyarchy = mean(vdem_polyarchy, na.rm = T))\n\n# A tibble: 1 × 1\n  mean_vdem_polyarchy\n                &lt;dbl&gt;\n1               0.657\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCalculate the median value of the corruption variable for each region (i.e., perform a grouped summary). Your code:"
  },
  {
    "objectID": "04_tidy_data2.html#pivoting-data",
    "href": "04_tidy_data2.html#pivoting-data",
    "title": "4  Tidy data analysis II",
    "section": "4.4 Pivoting data",
    "text": "4.4 Pivoting data\nWe will now load another time-series cross-sectional dataset, but in a slightly different format. It’s adapted from the World Bank’s World Development Indicators (WDI) (2023) and records gross domestic product at purchasing power parity (GDP PPP).\n\ngdp &lt;- read_excel(\"data/wdi_gdp_ppp.xlsx\")\n\n\ngdp\n\n# A tibble: 266 × 35\n   country_name        country_code   `1990`   `1991`   `1992`   `1993`   `1994`\n   &lt;chr&gt;               &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 Aruba               ABW           2.03e 9  2.19e 9  2.32e 9  2.48e 9  2.69e 9\n 2 Africa Eastern and… AFE           9.41e11  9.42e11  9.23e11  9.19e11  9.35e11\n 3 Afghanistan         AFG          NA       NA       NA       NA       NA      \n 4 Africa Western and… AFW           5.76e11  5.84e11  5.98e11  5.92e11  5.91e11\n 5 Angola              AGO           6.85e10  6.92e10  6.52e10  4.95e10  5.02e10\n 6 Albania             ALB           1.59e10  1.14e10  1.06e10  1.16e10  1.26e10\n 7 Andorra             AND          NA       NA       NA       NA       NA      \n 8 Arab World          ARB           2.19e12  2.25e12  2.35e12  2.41e12  2.48e12\n 9 United Arab Emirat… ARE           2.01e11  2.03e11  2.10e11  2.12e11  2.27e11\n10 Argentina           ARG           4.61e11  5.04e11  5.43e11  5.88e11  6.22e11\n# ℹ 256 more rows\n# ℹ 28 more variables: `1995` &lt;dbl&gt;, `1996` &lt;dbl&gt;, `1997` &lt;dbl&gt;, `1998` &lt;dbl&gt;,\n#   `1999` &lt;dbl&gt;, `2000` &lt;dbl&gt;, `2001` &lt;dbl&gt;, `2002` &lt;dbl&gt;, `2003` &lt;dbl&gt;,\n#   `2004` &lt;dbl&gt;, `2005` &lt;dbl&gt;, `2006` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2008` &lt;dbl&gt;,\n#   `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;,\n#   `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;, `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;,\n#   `2019` &lt;dbl&gt;, `2020` &lt;dbl&gt;, `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;\n\n\nNote how the information is recorded differently. Here columns are not variables, but years. We call datasets like this one wide, in contrast to the long datasets we have seen before. In general, R and the tidyverse work much nicer with long datasets. Luckily, the tidyr package of the tidyverse makes it easy to convert datasets between these two formats.\n\n\n\nSource: Illustration by Allison Horst, adapted by Peter Higgins.\n\n\nWe will use the pivot_longer() function:\n\ngdp_long &lt;- gdp |&gt; \n  pivot_longer(cols = -c(country_name, country_code), # cols to not pivot\n               names_to = \"year\", # how to name the column with names\n               values_to = \"wdi_gdp_ppp\",  # how to name the column with values\n               names_transform = as.integer) # make sure that years are numeric\ngdp_long\n\n# A tibble: 8,778 × 4\n   country_name country_code  year wdi_gdp_ppp\n   &lt;chr&gt;        &lt;chr&gt;        &lt;int&gt;       &lt;dbl&gt;\n 1 Aruba        ABW           1990 2025472682.\n 2 Aruba        ABW           1991 2186758474.\n 3 Aruba        ABW           1992 2315391348.\n 4 Aruba        ABW           1993 2484593045.\n 5 Aruba        ABW           1994 2688426606.\n 6 Aruba        ABW           1995 2756904694.\n 7 Aruba        ABW           1996 2789595753.\n 8 Aruba        ABW           1997 2986175079.\n 9 Aruba        ABW           1998 3045659222.\n10 Aruba        ABW           1999 3083365758.\n# ℹ 8,768 more rows\n\n\nDone! This is a much friendlier format to work with. For example, we can now do summaries:\n\ngdp_long |&gt; \n  summarize(mean_gdp_ppp = mean(wdi_gdp_ppp, na.rm = T), .by = country_name)\n\n# A tibble: 266 × 2\n   country_name                mean_gdp_ppp\n   &lt;chr&gt;                              &lt;dbl&gt;\n 1 Aruba                            3.38e 9\n 2 Africa Eastern and Southern      1.61e12\n 3 Afghanistan                      5.56e10\n 4 Africa Western and Central       1.15e12\n 5 Angola                           1.38e11\n 6 Albania                          2.56e10\n 7 Andorra                        NaN      \n 8 Arab World                       4.22e12\n 9 United Arab Emirates             4.29e11\n10 Argentina                        8.06e11\n# ℹ 256 more rows\n\n\n\n\n\n\n\n\nExercise\n\n\n\nConvert back gdp_long to a wide format using pivot_wider(). Check out the help file using ?pivot_wider. Your code:"
  },
  {
    "objectID": "04_tidy_data2.html#merging-datasets",
    "href": "04_tidy_data2.html#merging-datasets",
    "title": "4  Tidy data analysis II",
    "section": "4.5 Merging datasets",
    "text": "4.5 Merging datasets\nIt is extremely common to want to integrate data from multiple sources. Combining information from two datasets is called merging or joining.\nTo do this, we need ID variables in common between the two data sets. Using our QOG and WDI datasets, these variables will be country code (which in this case is shared between the two datasets) and year.\n\n\n\n\n\n\nTip\n\n\n\nStandardized unit codes (like country codes) are extremely useful when merging data. It’s harder than expected for a computer to realize that “Bolivia (Plurinational State of)” and “Bolivia” refer to the same unit. By default, these units will not be matched.2\n\n\nOkay, now to the merging. Imagine we want to add information about GDP to our QOG main dataset. To do so, we can use the left_join() function, from the tidyverse’s dplyr package:\n\nqog_plus &lt;- left_join(qog, # left data frame, which serves as a \"base\"\n                      gdp_long, # right data frame, from which to draw new columns\n                      by = c(\"ccodealp\" = \"country_code\", # can define name equivalencies!\n                             \"year\"))\n\n\nqog_plus |&gt; \n  # select variables for clarity\n  select(cname, ccodealp, year, wdi_pop, wdi_gdp_ppp)\n\n# A tibble: 1,085 × 5\n   cname               ccodealp  year wdi_pop wdi_gdp_ppp\n   &lt;chr&gt;               &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n 1 Antigua and Barbuda ATG       1990   63328  966660878.\n 2 Antigua and Barbuda ATG       1991   63634  987701012.\n 3 Antigua and Barbuda ATG       1992   64659  999143284.\n 4 Antigua and Barbuda ATG       1993   65834 1051896837.\n 5 Antigua and Barbuda ATG       1994   67072 1122128908.\n 6 Antigua and Barbuda ATG       1995   68398 1073208718.\n 7 Antigua and Barbuda ATG       1996   69798 1144088355.\n 8 Antigua and Barbuda ATG       1997   71218 1206688391.\n 9 Antigua and Barbuda ATG       1998   72572 1263778328.\n10 Antigua and Barbuda ATG       1999   73821 1310634399.\n# ℹ 1,075 more rows\n\n\n\n\n\n\n\n\nTip\n\n\n\nMost of the time, you’ll want to do a left_join(), which is great for adding new information to a “base” dataset, without dropping information from the latter. In limited situations, other types of joins can be helpful. To learn more about them, you can read Jenny Bryan’s excellent tutorial on dplyr joins.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nThere is a dataset on country’s CO2 emissions, again from the World Bank (2023), in “data/wdi_co2.csv”. Load the dataset into R and add a new variable with its information, wdi_co2, to our qog_plus data frame. Finally, compute the average values of CO2 emissions per capita, by country. Tip: this exercise requires you to do many steps—plan ahead before you start coding! Your code:"
  },
  {
    "objectID": "04_tidy_data2.html#plotting-extensions-trend-graphs-facets-and-customization",
    "href": "04_tidy_data2.html#plotting-extensions-trend-graphs-facets-and-customization",
    "title": "4  Tidy data analysis II",
    "section": "4.6 Plotting extensions: trend graphs, facets, and customization",
    "text": "4.6 Plotting extensions: trend graphs, facets, and customization\n\n\n\n\n\n\nExercise\n\n\n\nDraw a scatterplot with time in the x-axis and democracy scores in the y-axis. Your code:\n\n\nHow can we visualize trends effectively? One alternative is to use a trend graph. Let’s start by computing the yearly averages for democracy in the whole region:\n\ndem_yearly &lt;- qog |&gt; \n  summarize(mean_dem = mean(vdem_polyarchy, na.rm = T), .by = year)\ndem_yearly\n\n# A tibble: 31 × 2\n    year mean_dem\n   &lt;dbl&gt;    &lt;dbl&gt;\n 1  1990    0.581\n 2  1991    0.600\n 3  1992    0.605\n 4  1993    0.620\n 5  1994    0.629\n 6  1995    0.642\n 7  1996    0.651\n 8  1997    0.657\n 9  1998    0.663\n10  1999    0.661\n# ℹ 21 more rows\n\n\nNow we can plot them with a scatterplot:\n\nggplot(dem_yearly, aes(x = year, y = mean_dem)) +\n  geom_point()\n\n\n\n\nWe can add geom_line() to connect the dots:\n\nggplot(dem_yearly, aes(x = year, y = mean_dem)) +\n  geom_point() +\n  geom_line()\n\n\n\n\nWe can, of course, remove to points to only keep the line:\n\nggplot(dem_yearly, aes(x = year, y = mean_dem)) +\n  geom_line()\n\n\n\n\nWhat if we want to plot trends for different countries? We can use the group and color aesthetic mappings (no need to do a summary here! data is already at the country-year level):\n\n# filter to only get Colombia and Venezuela\ndem_yearly_countries &lt;- qog |&gt; \n  filter(ccodealp %in% c(\"COL\", \"VEN\"))\n\nggplot(dem_yearly_countries, aes(x = year, y = vdem_polyarchy, color = cname)) +\n  geom_line()\n\n\n\n\nRemember that we can use the labs() function to add labels:\n\nggplot(dem_yearly_countries, aes(x = year, y = vdem_polyarchy, color = cname)) +\n  geom_line() +\n  labs(x = \"Year\", y = \"V-Dem Electoral Democracy Score\", color = \"Country\", \n       title = \"Evolution of democracy scores in Colombia and Venezuela\",\n       caption = \"Source: V-Dem (Coppedge et al., 2022) in QOG dataset.\")\n\n\n\n\nAnother way to display these trends is by using facets, which divide a plot into small boxes according to a categorical variable (no need to add color here):\n\nggplot(dem_yearly_countries, aes(x = year, y = vdem_polyarchy)) +\n  geom_line() +\n  facet_wrap(~cname)\n\n\n\n\nFacets are particularly useful for many categories (where the number of distinguishable colors reaches its limit):\n\nggplot(qog |&gt; filter(region == \"South America\"), \n       aes(x = year, y = vdem_polyarchy)) +\n  geom_line() +\n  facet_wrap(~cname)\n\n\n\n\nWith facets, one can control whether each facet picks its own scales or if all facets share the same scale. For example, let’s plot the populations of Canada and the US:\n\nggplot(qog |&gt; filter(cname %in% c(\"Canada\", \"United States\")), \n       aes(x = year, y = wdi_pop)) +\n  geom_line() +\n  facet_wrap(~cname)\n\n\n\n\nThe scales are so disparate that unifying them yields a plot that’s hard to interpret. But if we’re interested in within-country trends, we can let each facet have its own scale with the scales = argument (which can be “fixed”, “free_x”, “free_y”, or “free”):\n\nggplot(qog |&gt; filter(cname %in% c(\"Canada\", \"United States\")), \n       aes(x = year, y = wdi_pop)) +\n  geom_line() +\n  facet_wrap(~cname, scales = \"free_y\")\n\n\n\n\nThis ability to visualize within time trends also makes facets appealing in many situations.\n\n\n\n\n\n\nTip\n\n\n\nPlots made with ggplot2 are extremely customizable. For example, we could want to change the y-axis labels in the last plot to something more readable:\n\nggplot(qog |&gt; filter(cname %in% c(\"Canada\", \"United States\")), \n       aes(x = year, y = wdi_pop)) +\n  geom_line() +\n  facet_wrap(~cname, scales = \"free_y\") +\n  scale_y_continuous(labels = scales::label_number(big.mark = \",\")) +\n  # also add labels\n  labs(x = \"Year\", y = \"Population\",\n       title = \"Population trends in Canada and the United States\",\n       caption = \"Source: World Development Indicators (World Bank, 2023) in QOG dataset.\")\n\n\n\n\nWhile it’s impossible for us to review all the customization options you might need, a fantastic reference is the “ggplot2: Elegant Graphics for Data Analysis” book by Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nUsing your merged dataset from the previous section, plot the trajectories of C02 per capita emissions for the US and Haiti. Use adequate scales.\n\n\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. First edition. Beijing ; Boston: O’Reilly.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/."
  },
  {
    "objectID": "04_tidy_data2.html#footnotes",
    "href": "04_tidy_data2.html#footnotes",
    "title": "4  Tidy data analysis II",
    "section": "",
    "text": "Technically, the read_csv() and read_rds() functions come from readr, one of the tidyverse constituent packages.↩︎\nThere are R packages to deal with these complications. fuzzyjoin matches units by their approximate distance, using some clever algorithms. countrycode allows one to standardize country names and country codes across different conventions.↩︎"
  },
  {
    "objectID": "05_functions.html#basics",
    "href": "05_functions.html#basics",
    "title": "5  Functions",
    "section": "5.1 Basics",
    "text": "5.1 Basics\n\n5.1.1 What is a function?\nInformally, a function is anything that takes input(s) and gives one defined output. There are always three main parts:\n\nThe input (\\(x\\) values, or each value in the domain)\nThe relationship of interest\nThe output (\\(y\\) values, or a unique value in the range)\n\n\n\n\nFigure 5.1: Function machine. Source: Bill Bailey on Wikimedia Commons.\n\n\n\n\n\n\n\n\nNote\n\n\n\n“\\(f(x) = \\space ...\\) is the classic notation for writing a function, but we can also use”\\(y = \\space ...\\)“. This is because \\(y\\) is”a function of” \\(x\\), so \\(y=f(x)\\).\n\n\nLet’s take a look at an example and break down the structure:\n\\[f(x) = 3x + 4\\]\n\n\\(x\\) is the input (some value) that the function takes.\nFor any \\(x\\), we multiply by three and add 4, which is the relationship.\nFinally, \\(f(x)\\) or \\(y\\) is the unique result, or the output.\n\nThe most common name to give a function is, predictably, “\\(f\\)”, but we can have other names such as “\\(g\\)” or “\\(h\\)”. The choice is yours.\n\n\n\n\n\n\nImportant\n\n\n\nWhen reading out loud, we say “[name of function] of x equals [relationship]. For example, \\(f(x) = x^2\\) is referred to as”f of x equals x squared.”\n\n\n\n\n5.1.2 Vertical line test\n\n\n\n\n\n\nExercise\n\n\n\nWhen graphed, vertical lines cannot touch functions at more than one point. Why?\nWhich of the following represent functions?\n\n\n\nFigure 5.2: Vertical line test: examples."
  },
  {
    "objectID": "05_functions.html#functions-in-r",
    "href": "05_functions.html#functions-in-r",
    "title": "5  Functions",
    "section": "5.2 Functions in R",
    "text": "5.2 Functions in R\nOften we need to create our own functions in R. To build them: we use the keyword function alongside the following syntax: function_name &lt;- function(argumentnames){ operation }\n\nfunction_name: the name of the function, that will be stored as an object in the R environment. Make the name concise and memorable!\nfunction(argumentnames): the inputs of the function.\n{ operation }: a set of commands that are run in a predefined order every time we call the function.\n\nFor example, we can create a function that multiplies a number by 2:\n\nmult_by_two &lt;- function(x){x * 2}\n\n\nmult_by_two(x = 5) # we can also omit the argument name (x =)\n\n[1] 10\n\n\nIf the function body works for vectors, our custom function will do too:\n\nmult_by_two(1:10)\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n\nWe can also automate more complicated tasks such as calculating the area of a circle from its radius:\n\ncirc_area_r &lt;- function(r){\n    pi * r ^ 2\n}\ncirc_area_r(r = 3)\n\n[1] 28.27433\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate a function that calculates the area of a circle from its diameter. So your_function(d = 6) should yield the same result as the example above. Your code:\n\n\nFunctions can take more than one argument/input. In a silly example, let’s generalize our first function:\n\nmult_by &lt;- function(x, mult){x * mult}\n\n\nmult_by(x = 1:5, mult = 10)\n\n[1] 10 20 30 40 50\n\n\n\nmult_by(1:5, mult = 10)\n\n[1] 10 20 30 40 50\n\n\n\nmult_by(1:5, 10)\n\n[1] 10 20 30 40 50\n\n\nTo graph a function, we’ll use our friend ggplot2 and stat_function():\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nggplot() +\n  stat_function(fun = mult_by_two, \n                xlim = c(-5, 5)) # domain over which we will plot the function\n\n\n\n\nUser-defined functions have endless possibilities! We encourage you to get creative and try to automate new tasks when possible, especially if they are repetitive.\n\n\n\n\n\n\nTip\n\n\n\nFunctions in R can also take non-numeric inputs. For example:\n\nsay_my_name &lt;- function(my_name){paste(\"My name is\", my_name)}\n\n\nsay_my_name(\"Inigo Montoya\")\n\n[1] \"My name is Inigo Montoya\""
  },
  {
    "objectID": "05_functions.html#common-types-of-functions",
    "href": "05_functions.html#common-types-of-functions",
    "title": "5  Functions",
    "section": "5.3 Common types of functions",
    "text": "5.3 Common types of functions\n\n5.3.1 Linear functions\n\\[y=mx+b\\]\nLinear functions are those whose graph is a straight line (in two dimensions).\n\n\\(m\\) is the slope, or the rate of change (common interpretation: for every one unit increase in \\(x\\), \\(y\\) increases \\(m\\) units).\n\\(b\\) is the y intercept, or the constant term (the value of \\(y\\) when \\(x=0\\)).\n\nBelow is a graph of the function \\(y = 3x + 4\\):\n\nggplot() +\n  stat_function(fun = function(x){3 * x + 4}, # we don't need to create an object\n                xlim = c(-5, 5)) \n\n\n\n\n\n\n5.3.2 Quadratic functions\n\\[y=ax^2 + bx + c\\]\nQuadratic functions take “U” forms. If \\(a\\) is positive, it is a regular “U” shape. If \\(a\\) is negative, it is an “inverted U” shape.\nNote that \\(x^2\\) always returns positive values (or zero).\nBelow is a graph of the function \\(y = x^2\\):\n\nggplot() +\n  stat_function(fun = function(x){x ^ 2},\n                xlim = c(-5, 5)) \n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSocial scientists commonly use linear or quadratic functions as theoretical simplifications of social phenomena. Can you give any examples?\n\n\n\n\n\n\n\n\nExercise\n\n\n\nGraph the function \\(y = x^2 + 2x - 10\\), i.e., a quadratic function with \\(a=1\\), \\(b=2\\), and \\(c=-10\\).\nNext, try switching up these values and the xlim = argument. How do they each alter the function (and plot)?\n\n\n\n\n5.3.3 Cubic functions\n\\[y=ax^3 + bx^2 + cx +d\\]\nThese lines (generally) have two curves (inflection points).\nBelow is a graph of the function \\(y = x^3\\):\n\nggplot() +\n  stat_function(fun = function(x){x ^ 3},\n                xlim = c(-5, 5)) \n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWe’ll briefly introduce Desmos, an online graphing calculator. Use Desmos to graph the following function \\(y = 1x^3 + 1x^2 + 1x + 1\\). What happens when you change the \\(a\\), \\(b\\), \\(c\\), and \\(d\\) parameters?\n\n\n\n\n5.3.4 Polynomial functions\n\\[y=ax^n + bx^{n-1} + ... + c\\]\nThese functions have (a maximum of) \\(n-1\\) changes in direction (turning points). They also have (a maximum of) \\(n\\) x-intercepts.\nHigh-order polynomials can be made arbitrarily precise!\nBelow is a graph of the function \\(y = \\frac{1}{4}x^4 - 5 x^2 + x\\).\n\nggplot() +\n  stat_function(fun = function(x){1/4 * x ^ 4 - 5 * x ^ 2 + x},\n                xlim = c(-5, 5)) \n\n\n\n\n\n\n5.3.5 Exponential functions\n\\[y = ab^{x}\\]\nHere our input (\\(x\\)), is the exponent.\nBelow is a graph of the function \\(y = 2^x\\):\n\nggplot() +\n  stat_function(fun = function(x){2 ^ x},\n                xlim = c(-5, 5)) \n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nExponential growth appears quite frequently social science theories. Which variables can be theorized to have exponential growth over time?"
  },
  {
    "objectID": "05_functions.html#logarithms-and-exponents",
    "href": "05_functions.html#logarithms-and-exponents",
    "title": "5  Functions",
    "section": "5.4 Logarithms and exponents",
    "text": "5.4 Logarithms and exponents\n\n5.4.1 Logarithms\nLogarithms are the opposite/inverse of exponents. They ask how many times you must raise the base to get \\(x\\).\nSo \\(log_a(b)=x\\) is asking “a raised to what power x gives b?” For example, \\(\\log_3(81) = 4\\) because \\(3^4=81\\).\n\n\n\n\n\n\nWarning\n\n\n\nLogarithms are undefined if the base is \\(\\le 0\\) (at least in the real numbers).\n\n\n\n\n5.4.2 Relationships\nIf, \\[ log_ax=b\\] then, \\[a^{log_{a}x}=a^b\\] and \\[x=a^b\\]\n\n\n5.4.3 Basic rules\n\\[\\dfrac{\\log_x n}{\\log_x m} = \\log_m n\\]\n\\[\\log_x(ab) = \\log_xa + \\log_xb \\]\n\\[\\log_x\\left(\\frac{a}{b}\\right) = \\log_xa - \\log_xb\\]\n\\[\\log_xa^b = b \\log_x a\\]\n\\[\\log_x 1 = 0\\]\n\\[log_{x}x=1\\]\n\\[m^{\\log_m(a)} = a\\]\n\n\n5.4.4 Natural logarithms\n\nWe most often use natural logarithms for our purposes.\nThis means \\(log_e(x)\\), which is usually written as \\(ln(x)\\).\n\n\n\n\n\n\n\nImportant\n\n\n\n\\(e \\approx 2.7183\\).\n\n\n\n\\(ln(x)\\) and its exponent opposite, \\(e^x\\), have nice properties when we perform calculus.\n\n\n\n5.4.5 Illustration of \\(e\\)\nImagine you invest $1 in a bank and receive 100% interest for one year, and the bank pays you back once a year: \\[(1+1)^1= 2\\].\nWhen it pays you twice a year with compound interest:\n\\[(1+1/2)^2=2.25\\]\nIf it pays you three times a year:\n\\[(1+1/3)^3=2.37...\\]\nWhat will happen when the bank pays you once a month? Once a day?\n\\[(1+\\frac{1}{n})^{n}\\]\nHowever, there is limit to what you can get.\n\\[\\lim_{n\\to\\infty} (1 + \\dfrac{1}{n})^n = 2.7183... = e\\]\nFor any interest rate \\(k\\) and number of times the bank pays you \\(t\\): \\[\\lim_{n\\to\\infty} (1 + \\dfrac{k}{n})^{nt} = e^{kt}\\]\n\n\\(e\\) is important for defining exponential growth. Since \\(ln(e^x) = x\\), the natural logarithm helps us turn exponential functions into linear ones.\n\n\n\n\n\n\n\nExercise\n\n\n\nSolve the problems below, simplifying as much as you can. \\[log_{10}(1000)\\] \\[log_2(\\dfrac{8}{32})\\] \\[10^{log_{10}(300)}\\] \\[ln(1)\\] \\[ln(e^2)\\] \\[ln(5e)\\]\n\n\n\n\n5.4.6 Logarithms in R\nBy default, R’s log() function computes natural logarithms:\n\nlog(100)\n\n[1] 4.60517\n\n\nWe can change this behavior with the base = argument:\n\nlog(100, base = 10)\n\n[1] 2\n\n\nWe can also plot logarithms. Remember that \\(ln(x)\\) \\(\\forall x&lt;0\\) is undefined (at least in the real numbers), and ggplot2 displays a nice warning letting us know!\n\nggplot() +\n  stat_function(fun = function(x){log(x)},\n                xlim = c(-5, 5)) \n\nWarning in log(x): NaNs produced\n\n\nWarning: Removed 50 rows containing missing values (`geom_function()`).\n\n\n\n\n\n\nggplot() +\n  stat_function(fun = function(x){log(x)},\n                xlim = c(1, 100))"
  },
  {
    "objectID": "05_functions.html#composite-functions-functions-of-functions",
    "href": "05_functions.html#composite-functions-functions-of-functions",
    "title": "5  Functions",
    "section": "5.5 Composite functions (functions of functions)",
    "text": "5.5 Composite functions (functions of functions)\nFunctions can take other functions as inputs, e.g., \\(f(g(x))\\). This means that the outside function takes the output of the inside function as its input.\nSay we have the exterior function \\(f(x)=x^2\\) and the interior function \\(g(x)=x-3\\). Then if we want \\(f(g(x))\\), we would subtract 3 from any input, and then square the result.\n\nWe write this as \\((x-3)^2\\), not \\(x^2-3\\)!\n\nR can handle this just fine:\n\nf &lt;- function(x){x ^ 2}\ng &lt;- function(x){x - 3}\n\n\nf(g(5))\n\n[1] 4\n\n\nHere we can also use pipes to make this code more readable (imagine if we were chaining multiple functions…). Remember that pipes can be inserted with the Cmd/Ctrl + Shift + M shortcut.\n\n# compute g(5), THEN f() of that\ng(5) |&gt; f()\n\n[1] 4\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCompute g(f(5)) using the definitions above. First do it manually, and then check your answer with R.\n\n\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. First edition. Beijing ; Boston: O’Reilly.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/."
  },
  {
    "objectID": "06_calculus.html#derivatives",
    "href": "06_calculus.html#derivatives",
    "title": "6  Calculus",
    "section": "6.1 Derivatives",
    "text": "6.1 Derivatives\nDerivatives are about (instantaneous) rate of change.\n\n“In the fall of 1972 President Nixon announced that the rate of increase of inflation was decreasing. This was the first time a sitting president used the third derivative to advance his case for reelection” (Rossi 1996)\n\nLet’s dissect what Nixon might have said:\n\nInflation’s [first derivative, of prices] rate of increase [second derivative] is going down [third derivative].\n\nA more graphical way to think about a derivatives is as a slope. Let’s consider a linear function of the form \\(y = 2 x\\):\n\nlibrary(tidyverse) # could also just do library(ggplot2)\nggplot() +\n  stat_function(fun = function(x){2 * x}, \n                xlim = c(-10, 10))\n\n\n\n\nWe can imagine any political variables in the x- and y-axes. What is the rate of change? In other words, what is the derivative? Remember that we can calculate the slope with:\n\\[\nm = \\frac{f(x_2) - f(x_1)}{x_2-x_1}\n\\]\nNow consider another slightly more complicated function, a quadratic one, \\(y = x^2\\):\n\nggplot() +\n  stat_function(fun = function(x){x ^ 2}, \n                xlim = c(-10, 10))\n\n\n\n\nWhat happens when we apply our slope function?\n\n\n\n\n\n\nExercise\n\n\n\n\nUse the slope formula to calculate the rate of change between 5 and 6.\nUse the slope formula to calculate the rate of change between 5 and 5.5.\nUse the slope formula to calculate the rate of change between 5 and 5.1.\n\n\n\nTakeaway: here the derivative depends on the value of \\(x\\). It is actually \\(2x\\).\nDifferential calculus is about finding these derivatives in a more straightforward manner! We can generalize our slope formula as follows:\n\\[\nm = \\frac{f(x_1+ \\Delta x) - f(x_1)}{\\Delta x}\n\\]\nThe point is that when \\(\\Delta x\\) is arbitrarily small, we’ll get our rate of change. Formalizing this:\n\\[\n\\lim_{\\Delta x\\to0} \\frac{f(x_1+ \\Delta x) - f(x_1)}{\\Delta x} = \\frac{d}{dx} f(x) = \\frac{dy}{dx} = f'(x)\n\\]\nA few points on notation:\n\n\\(\\frac{d}{dx} f(x)\\) is read “The derivative of \\(f\\) of \\(x\\) with respect to \\(x\\).”\n\nThe variable with respect to which we’re differentiating is the one that appears in the bottom (in the case above, this is \\(x\\)).\n\n\n\n\n\n\n\nWarning\n\n\n\nWhile the above looks like a fraction, it’s really not. Do not try to cancel out the \\(d\\)s!\n\n\n\\(f'(x)\\) (read: “\\(f\\) prime \\(x\\)”) is the derivative of \\(f(x)\\). This is a more compact form to refer to derivatives when you have defined \\(f(x)\\) elsewhere.\n\n\n6.1.1 Rules of differentiation\nHow to compute derivatives? Sometimes you can try a bunch of numbers and get at the answer. Sometimes you can use the limit-based formula above, if you know a few properties of limits. But in most cases you will either use software (more on this later) or the rules of differentiation, which we will cover now.\nConstant rule: \\((c)' = 0\\).\nThere is no change in a constant:\n\nggplot() +\n  stat_function(fun = function(x){2}, xlim = c(-10, 10))\n\n\n\n\nCoefficient rule: \\((c \\cdot f(x))' = c \\cdot f'(x)\\).\n\nggplot() +\n  stat_function(fun = function(x){2 * x}, xlim = c(-10, 10), aes(color = \"y = 2x\")) +\n  stat_function(fun = function(x){4 * x}, xlim = c(-10, 10), aes(color = \"y = 4x\")) +\n  scale_color_manual(\"Function\", values = c(\"red\", \"blue\"))\n\n\n\n\nSum/difference rule: \\((f(x) \\pm g(x))' = f'(x) \\pm g'(x)\\).\nThe two rules above give us that the derivative is a linear operator.\nPower rule: \\((x^n)'=nx^{(n-1)}\\)\nRemember when we wanted to calculate the derivative of \\(y=x^2\\) above? We can use the power rule, with \\(n=2\\): \\(\\;nx^{(n-1)} = 2x^{(2-1)}=2x\\). Let’s try out \\(\\frac{d}{dx}4x^3\\) and \\(\\frac{d}{dx}(x^2 + 2x)\\) on the board.\n\n\n\n\n\n\nExercise\n\n\n\nUse the differentiation rules we have covered so far to calculate the derivatives of \\(y\\) with respect to \\(x\\) of the following functions:\n\n\\(y = 2x^2 + 10\\)\n\\(y = 5x^4 - \\frac{2}{3}x^3\\)\n\\(y = 9 \\sqrt x\\)\n\\(y = \\frac{4}{x^2}\\)\n\\(y = ax^3 + b\\), where \\(a\\) and \\(b\\) are constants.\n\\(y = \\frac{2w}{5}\\)\n\n\n\nExponent and logarithm rules:\n\\[\n\\begin{aligned}\n(c^x)' &= c^x \\cdot ln(c), & \\forall x&gt;0 \\\\\n(e^x)' &= e^x \\\\\n\\\\\n(log_a(x))' &= \\frac{1}{x \\cdot ln(a)}, & \\forall x&gt;0  \\\\\n(ln(x))' &= \\frac{1}{x}, & \\forall x&gt;0\n\\end{aligned}\n\\]\nWe saw previously how Euler’s number (\\(e\\)) arises from compound interest. The properties above make it very useful in a lot of calculus applications!\n\n\n\n\n\n\nExercise\n\n\n\nCompute the following:\n\n\\(\\frac{d}{dx}(10e^x)\\)\n\\(\\frac{d}{dx}(ln(x) - \\frac{e^2}{3})\\)\n\n\n\nNow we’ll get to a couple of more advanced (and powerful) rules.\nProduct rule: \\((f(x)g(x))'=f'(x)g(x) + g'(x)f(x)\\)\nLet’s calculate \\(\\frac{d}{dx}(3 \\cdot ln(x) \\cdot x^2)\\) on the board.\nQuotient rule: \\(\\displaystyle(\\frac{f(x)}{g(x)})' = \\frac{f'(x)g(x) + g'(x)f(x)}{[g(x)]^2}\\)\nChain rule: \\((f(g(x))' = f'(g(x)) \\cdot g'(x)\\)\nLet’s compute \\(\\frac{d}{dx}(e^{x^{2}})\\) on the board.\n\n\n\n\n\n\nExercise\n\n\n\nUse the differentiation rules we have covered so far to calculate the derivatives of \\(y\\) with respect to \\(x\\) of the following functions:\n\n\\(x^3 \\cdot x\\)\n\\(e^x \\cdot x^2\\)\n\\((3x^4-8)^2\\)\n\n\n\n\n\n6.1.2 Higher-order derivatives\nWe saw how politicians can refer to higher-order derivatives. To compute them, you simply “pass the outputs,” starting from the lowest order and going up.\nThe second derivative tells us whether the slope of a function is increasing, decreasing, or staying the same at any point \\(x\\) on the function’s domain. For example, when driving a car:\n\n\\(f(x)\\) = distance traveled at time \\(x\\)\n\\(f'(x)\\) = speed at time \\(x\\)\n\\(f''(x)\\) = acceleration at time \\(x\\)\n\nLet’s compute the following second derivative:\n\\[f''(x^4) = \\frac{d^2(x^4)}{dx^2}\\]\n\nFirst, we take the first derivative: \\(f'(x^4)=4x^3\\)\nThen we use that output to take the second derivative: \\(f''(x^4)=f'(4x^3)=12x^2\\)\nWe can keep going… for example, the third derivative: \\[f'''(x^4) = f'(12x^2) = 24x\\]\n\n\n\n\n\n\n\nExercise\n\n\n\nCompute the following:\n\n\\(\\frac{d^3}{dx^3}(x^5)\\)\n\\(f''(4x^{3/2})\\)\n\\(f''(4 \\cdot ln(x))\\)\n\n\n\n\n\n6.1.3 Partial derivatives\nFor a function \\(f(x,z)\\), we might want to know how the function changes with respect to \\(x\\). We call this a partial derivative:\n\\[\n\\frac{\\partial}{\\partial_x}f(x,z) = \\frac{\\partial_y}{\\partial_x} = \\partial_x f\n\\]\nTo obtain a partial derivative, we treat all other variables as constants and take the derivative with respect to the variable of interest (here \\(x\\)). For example:\n\\[\n\\begin{aligned}\ny = f(x,z) &= xz \\\\\n\\frac{\\partial_y}{\\partial_x} &= z\n\\end{aligned}\n\\]\nWhat is \\(\\displaystyle\\frac{\\partial_y}{\\partial_z}?\\)\nLet’s solve \\(\\displaystyle\\frac{\\partial (x^2y+xy^2-x)}{\\partial x}\\) and \\(\\displaystyle\\frac{\\partial (x^2y+xy^2-x)}{\\partial y}\\) on the board.\n\n\n\n\n\n\nExample\n\n\n\nLet’s say that \\(y\\) is how much I like a movie, \\(d\\) is how many dogs a movie has, and \\(e\\) is how many explosions a movie has. I claim that how much I like a movie can be expressed by a function of the type \\(y = f(d, e)\\). Evaluate the following situations:\n\nI like dogs and I don’t care about action. So I believe that the true relationship is \\(y = f(d, e) = 3 \\cdot d\\). What is \\(\\displaystyle\\frac{\\partial_y}{\\partial_d}\\), and how can we interpret it?\nI like dogs and I like action. So I believe that the true relationship is \\(y = f(d, e) = 3 \\cdot d + 1 \\cdot e\\). What is \\(\\displaystyle\\frac{\\partial_y}{\\partial_d}\\), and how can we interpret it?\nI like dogs and I like action. But I definitely don’t like them together—I don’t want the dogs to be in danger! So I believe that the true relationship is \\(y = f(d, e) = 3 \\cdot d + 1 \\cdot e -10 \\cdot d \\cdot e\\). What is \\(\\displaystyle\\frac{\\partial_y}{\\partial_d}\\), and how can we interpret it?\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTake the partial derivative with respect to \\(x\\) and with respect to \\(z\\) of the following functions. What would the notation for each look like?\n\n\\(y = 3xz - x\\)\n\\(x^3+z^3+x^4z^4\\)\n\\(e^{xz}\\)\n\n\n\n\n\n6.1.4 Differentiability of functions\nNot all functions are differentiable at every point of their domains!\nAn important concept here is whether functions are continuous at a point:\n\nInformally: A function is continuous at a point if its graph has no holes or breaks at that point\nFormally: A function is continuous at a point \\(a\\) if: \\(\\lim_{x \\to a} f(x)=f(a)\\)\n\nWhen is a function differentiable at a point?\n\nIf a function is differentiable at a point, it is also continuous at that point.\nIf a function is continuous at a point, it is not necessarily differentiable at that point.\n\nImpossible to calculate derivative at sharp turns, cusps, or vertical tangents.\n\n\n\nggplot() +\n  stat_function(fun = function(x){abs(x) + 2}, xlim = c(-4, 4), \n                aes(color = \"y = |x| + 2\")) +\n  stat_function(fun = function(x){sqrt(abs(x)) + 1}, xlim = c(-4, 4), \n                aes(color = \"y = √(|x|) + 1\")) +\n  stat_function(fun = function(x){sign(x) * abs(x)^(1 / 3)}, xlim = c(-4, 4), \n                aes(color = \"y = ₃√x\")) +\n  scale_colour_manual(\"Function\", values = c(\"red\", \"blue\", \"black\")) +\n  labs(title = \"Examples of functions that are not differentiable at x=0\")\n\n\n\n\n\nInformally, functions need to be continuous and reasonably smooth to be differentiable.\n\n\n\n6.1.5 How do computers calculate derivatives?\nIn quite a few statistics and machine learning problems, computers need to compute derivatives of arbitrarily complex functions, perhaps millions of times. How do they do it? (see Baydin et al. 2018 for discussion of these three approaches)\n\nSymbolic differentiation: automatically combine the rules of differentiation (power rule, product rule, etc.). It is what math solvers use, e.g., WolframAlpha or (presumably) Symbolab.\nNumerical differentiation: infer the derivative by computing the function at different sample values (like we did with \\(y=x^2\\) before. This is what, for example, R’s optim() function does behind the scenes.\nAutomatic differentiation: track how every function is constructed from (differentiable) elementary computer operations (e.g., binary arithmetic), and get the result using the chain rule. Implemented in the TensorFlow, PyTorch, and JAX Python libraries, and the ReverseDiff.jl and Zygote.jl Julia packages.\n\n\n\n\nAn example of computing the gradient of an esoteric function using Zygote.jl (from its documentation)"
  },
  {
    "objectID": "06_calculus.html#optimization",
    "href": "06_calculus.html#optimization",
    "title": "6  Calculus",
    "section": "6.2 Optimization",
    "text": "6.2 Optimization\nOptimization allows us to find the minimum or maximum values (or extrema) a function takes. It has many applications in the social sciences:\n\nFormal theory: utility maximization, continuous choices\nOrdinary Least Squares (OLS): Focuses on minimizing the squared errors between observed data and model-estimated values\nMaximum Likelihood Estimation (MLE): Focuses on maximizing a likelihood function, given observed values.\n\n\n6.2.1 Extrema\nOn extrema: informally, a maximum is just the highest value a function takes, and a minimum is the lowest value.\nIn some situations, it can be easy to identify extrema intuitively by looking at a graph of the function.\n\nMaxima are high points (“peaks”)\nMinima are low points (“valleys”)\n\nWe can use derivatives (rates of change!) to get at extrema.\n\n\n6.2.2 Critical points and the First-Order Condition\nAt critical points (or stationary points), the derivative is zero or fails to exist. At these, the function has usually reached a (local) maximum or minimum.\n\nAt a maximum, the function must be increasing before the point and decreasing after it.\nAt a minimum, the function must be decreasing before the point and increasing after it.\n\n\n\n\n\n\n\nWarning\n\n\n\nLocal extrema occur at critical points, but not all critical points are extrema. For instance, sometimes the graph is changing between concave and convex (“inflection points”). Or sometimes the function is not differentiable at that point for other reasons.\n\n\nWe can find the local maxima and/or minima of a function by taking the derivative, setting it equal to zero, and solving for \\(x\\) (or whatever variable). This gives us the First-Order Condition (FOC).\n\\[FOC: f'(x)=0\\]\n\n\n6.2.3 Second-Order Condition\nNotice that after this we only know that there is a critical point. BUT we don’t know if we’ve found a maximum or minimum, or even if we’ve found an extremum.\nTo determine whether a we are seeing a (local) maximum or minimum, we can use the Second Derivative Test:\n\nStart by identifying \\(f''(x)\\)\nSubstitute in the stationary points \\((x^*)\\) identified from the FOC.\n\n\\(f''(x^*) &gt; 0\\) we have a local minimum\n\\(f''(x^*) &lt; 0\\) we have a local maximum\n\\(f''(x^*) = 0\\) we (may) have an inflection point - need to calculate higher-order derivatives (don’t worry about this now)\n\n\nCollectively these give us the Second-Order Condition (SOC).\nLet’s do this procedure and obtain the FOC and SOC for \\(\\displaystyle y=\\frac{1}{2} x^3 + 3 x^2 - 2\\) on the board. What do we learn? Compare this with the plot of the function on Desmos.\n\n\n6.2.4 Local or global extrema?\nNow when it comes to knowing whether extrema are local or global:\n\nHere we use the Extreme value theorem, which states that if a real-valued function is continuous on a closed and bounded (i.e., finite) interval, the function must have a global minimum and a global minimum on that interval at least once. Importantly, in this situation the global extrema exist, and they are either at the local extrema or at the boundaries (where we cannot even find critical points).\nSo to find the minimum/maximum on some interval, compare the local min/max to the value of the function at the interval’s endpoints. So, e.g., if the interval is \\((-\\infty, +\\infty)\\), check the function’s limits as it approaches \\(-\\infty\\) and \\(+\\infty\\).\n\nLet’s try this last step for our example above, \\(\\displaystyle y=\\frac{1}{2} x^3 + 3 x^2 - 2\\), to get the global extrema in the entire domain.\n\n\n\n\n\n\nExercise\n\n\n\nIdentify the global extrema of the function \\(\\displaystyle \\frac{x^3}{3} - \\frac{3}{2}x^2 -10x\\) in the interval \\([-6, 6]\\)."
  },
  {
    "objectID": "06_calculus.html#integrals",
    "href": "06_calculus.html#integrals",
    "title": "6  Calculus",
    "section": "6.3 Integrals",
    "text": "6.3 Integrals\nInformally, we can think of integrals as the flip side of derivatives.\nWe can motivate integrals as a way of finding the area under a curve. Sometimes finding the area is easy. What’s the area under the curve between \\(x=-1\\) and \\(x=1\\) for this function?\n\\[\nf(x) =\n\\begin{cases}\n\\frac{1}{3} & \\text{for } x \\in [0, 3] \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nNormally, finding the area under a curve is much harder. But this is basically the question behind integration.\n\n6.3.1 Integrals are about infinitesimals too\nLet’s say we have a function \\(y = x^2\\) And we want to find the area under the curve from \\(x=0\\) to \\(x=1\\). How would we do this?\n\nggplot() +\n  # draw main function\n  stat_function(fun = function(x){x ^ 2}, xlim = c(-2, 2)) +\n  # fill area under the curve between x = 0 and x = 1\n  geom_area(mapping = aes(x = 0), stat = \"function\",\n            fun = function(x){x ^ 2}, xlim = c(0, 1), fill = \"red\")\n\n\n\n\nOne way to approximate this area is by drawing narrow rectangles that cover the area in red. Let’s draw this on the board.\nOur approximation is rough, but it gets better and better the narrower the rectangles are:\n\\[\nArea = lim_{\\Delta x \\to 0}\\sum_i^n{f(x) \\cdot \\Delta x}\n\\]\n, where \\(\\Delta x\\) is the width of the rectangles and \\(n\\) is their number.\nThis is actually one way to define the definite integral, \\(\\displaystyle\\int_a^bf(x)dx\\) (also known as the Riemann integral). We’ll learn how to compute these in a few moments.\n\n\n6.3.2 Indefinite integrals as antiderivatives\nThe indefinite integral, also known as the antiderivative, \\(F(x)\\) is the inverse of the function \\(f'(x)\\). \\[F(x)= \\displaystyle\\int f(x) \\text{ } dx\\]\nThis means if you take the derivative of \\(F(x)\\), you wind up back at \\(f(x)\\). \\[F' = f \\text{ or } \\displaystyle\\frac{dF(x)}{dx} = f(x)\\]\nFor example, what is the antiderivative for a constant function \\(f(x) = 1\\)? Is there just one? (this example comes from Moore and Siegel, 2013, p. 137).\nThis process is called anti-differentiation. We can use this concept to help us solve definite integrals!\n\n\n6.3.3 Solving definite integrals\nOne way to calculate definite integrals, known as the “fundamental theorem of calculus,” is shown below:\n\\[\\displaystyle\\int_{a}^{b} f(x) \\text{ } dx = F(b)-F(a) = F(x)\\bigg|_{a}^{b}\\]\nFirst we determine the antiderivative (indefinite integral) of \\(f(x)\\) (and represent it \\(F(x)\\)), substitute the upper limit first and then the lower limit one by one, and subtract the results in order.\n\n\n\n\n\n\nWarning\n\n\n\n\\(C\\) in the following definitions and rules is the called the “constant of integration.” We need to add it when we define all antiderivatives (integrals) of a function because the anti-derivative “undoes” the derivative.\nRemember that the derivative of any constant is zero. So if we find an integral \\(F(x)\\) whose derivative is \\(f(x)\\), adding (or subtracting) any constant will give us another integral \\(F(x)+C\\) whose derivative is also \\(f(x)\\).\n\n\n\n\n6.3.4 Rules of integration\nMany of the rules of integetration have counterparts in differentiation.\nCoefficient rule: \\(\\displaystyle\\int c f(x)\\,dx = c \\int f(x)\\,dx\\)\nSum/difference rule: \\(\\displaystyle\\int (f(x) \\pm g(x))\\,dx = \\int f(x)\\,dx \\pm \\int g(x)\\,dx\\)\nConstant rule: \\(\\displaystyle\\int c\\,dx = cx + C\\)\nPower rule: \\(\\displaystyle\\int x^n\\,dx = \\frac{x^{n+1}}{n+1} + C \\qquad \\forall n \\neq -1\\)\nInverse rule:\\(\\displaystyle\\int \\frac{1}{x}\\,dx = \\ln |x| + C\\)\nExponent and logarithm rules:\n\\[\n\\begin{aligned}\n\\displaystyle \\int e^x \\,dx &= e^x+C \\\\\n\\displaystyle \\int c^x \\,dx &= \\frac{c^x}{ln(c)}+C\\\\\n\\\\\n\\displaystyle \\int ln(x) \\,dx &= x \\cdot ln(x) - x+C \\\\\n\\displaystyle \\int log_c(x) \\,dx &= \\frac{x \\cdot log_c(x) - x}{log_c(x)}+C\n\\end{aligned}\n\\]\nThe final two rules are analog to the product rule and the chain rule:\nIntegration by parts: \\(\\displaystyle \\int f(x)g'(x)\\,dx = f(x)g(x) - \\int f'(x)g(x)\\,dx\\)\nIntegration by substitution:\n\\[\n\\begin{aligned}\n1.& \\text{ Have }\\displaystyle \\int f(g(x))g'(x)\\,dx \\\\\n2.& \\text{ Set u=g(x)} \\\\\n3.& \\text{ Compute } \\int f(u)\\,du \\\\\n4.& \\text{ Replace u for g(x)}\n\\end{aligned}\n\\]\nLet’s do an example on the board: \\(\\displaystyle \\int e^{x^2} 2x \\,dx\\).\n\n\n6.3.5 Solving the problem\nRemember our function \\(y=x^2\\) and our goal of finding the area under the curve from \\(x=0\\) to \\(x=1\\). We can describe this problem as \\(\\displaystyle \\int_0^1 x^2 dx\\)\nFind the indefinite integral, \\(F(x)\\):\n\\[\n\\displaystyle\\int x^2 \\text{ } dx = \\displaystyle\\frac{x^3}{3}+C\n\\]\nNow we’ll use the fundamental theory of calculus. Evaluate at our lowest and highest points, \\(F(0)\\) and \\(F(1)\\):\n\n\\(F(0) = 0\\)\n\\(F(1) = \\displaystyle\\frac{1}{3}\\)\nTechnically \\(0 + C\\) and \\(\\displaystyle\\frac{1}{3} + C\\), but the C’s will fall out in the next step\n\nCalculate \\(F(1) - F(0)\\) \\[\\displaystyle\\frac{1}{3} - 0 = \\displaystyle\\frac{1}{3}\\]\n\n\n\n\n\n\nExercise\n\n\n\nSolve the following indefinite integrals:\n\n\\(\\int x^2 \\text{ } dx\\)\n\\(\\int 3x^2\\text{ } dx\\)\n\\(\\int x\\text{ } dx\\)\n\\(\\int (3x^2 + 2x - 7\\text{ })dx\\)\n\\(\\int \\dfrac{2}{x}\\text{ }dx\\)\n\nAnd solve the following definite integrals:\n\n\\(\\displaystyle\\int_{1}^{7} x^2 \\text{ } dx\\)\n\\(\\displaystyle\\int_{1}^{10} 3x^2 \\text{ } dx\\)\n\\(\\displaystyle\\int_7^7 x\\text{ } dx\\)\n\\(\\displaystyle\\int_{1}^{5} 3x^2 + 2x - 7\\text{ }dx\\)\n\\(\\int_{1}^{e} \\dfrac{2}{x}\\text{ }dx\\)\n\n\n\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. First edition. Beijing ; Boston: O’Reilly.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/."
  },
  {
    "objectID": "07_probability_stats_sims.html#what-is-probability",
    "href": "07_probability_stats_sims.html#what-is-probability",
    "title": "7  Probability, statistics, and simulations",
    "section": "7.1 What is probability?",
    "text": "7.1 What is probability?\n\nInformally, a probability is a number that describes how likely an event is.\n\nIt is, by definition, between 0 and 1.\nWhat is the probability that a fair coin flip will result in heads?\n\nWe can also think of a probability as an outcome’s relative frequency after repeating an “experiment” many times.1\n\nIn this setting, an experiment is “an action or a set of actions that produce stochastic [random] events of interest” (Imai and Williams 2022, p. 281). Not to confuse with scientific experiments!\nIf we were to flip a million fair coins, what will be the proportion of heads?\n\n\n\nA probability space \\((\\Omega, S, P)\\) is a formal way to talk about a random process:\n\nThe sample space (\\(\\Omega\\)) is the set of all possible outcomes.\nThe event space (\\(S\\)) is a collection of events (an event is a subset of \\(\\Omega\\)).\nThe probability measure (\\(P\\)) is a function that assigns a probability in \\(\\mathbb{R}\\) to every event in \\(S\\). So \\(P: S \\rightarrow \\mathbb{R}\\).\n\nWe can formalize our intuitions with the probability axioms (sometimes called Kolmogorov’s axioms):\n\n\\(P(A) \\ge 0, \\; \\forall A \\in S\\).\n\nProbabilities must be non-negative.\n\n\\(P(\\Omega) = 1\\).\n\nSomething has to happen!\nProbabilities sum/integrate to 1.\n\n\\(P(A \\cup B) = P(A) + P(B), \\; \\forall A, B \\in S, \\; A\\cup B = \\emptyset\\).\n\nThe probability of disjoint (mutually exclusive) events is equal to the sum of their individual probabilities.\n\n\n\n\n7.1.1 Definitions and properties of probability\n\nJoint probability: \\(P(A \\cap B)\\). The probability that the two events will occur in one realization of the experiment.\nLaw of total probability: \\(P(A) = P(A \\cap B) + P(A \\cap B^C)\\).\nAddition rule: \\(P(A \\cup B) = P(A) + P(B) - P (A \\cap B)\\).\nConditional probability: \\(\\displaystyle P(A|B)=\\frac{P(A \\cap B)}{P(B)}\\)\nBayes theorem: \\(\\displaystyle P(A|B) = \\frac{P(A) \\cdot P(B|A)}{P(B)}\\)\n\n\n\n7.1.2 Random variables and probability distributions\n\nRandom variables are functions (\\(X: \\Omega \\to \\mathbb{R}\\)) of the outcome of a random generative process. Informally, they are “placeholders” for whatever will be the output of a process we’re studying.\nProbability distributions describe how the random variable assigns probabilities to outcomes.\nRandom variables (and probability distributions) can be discrete or continuous.\n\n\n7.1.2.1 Discrete random variables and probability distributions\n\nA sample space in which there are a (finite or infinite) countable number of outcomes\nEach realization of random process has a discrete probability of occurring.\n\n\\(f(X=x_i)=P(X=x_i)\\) is the probability the variable takes the value \\(x_i\\).\n\n\n\nAn example\n\nWhat’s the probability that we’ll roll a 3 on one die roll: \\[Pr(y=3) = \\dfrac{1}{6}\\]\nIf one roll of the die is an “experiment,” we can think of a 3 as a “success.”\n\\(Y \\sim Bernoulli \\left(\\frac{1}{6} \\right)\\)\nFair coins are \\(\\sim Bernoulli(.5)\\), for example.\nMore generally, \\(Bernoulli(\\pi )\\). We’ll talk about other probability distributions soon.\n\n\\(\\pi\\) represents the probability of success.\n\n\nLet’s do another example on the board, using the sum of two fair dice.\n\n\n\n7.1.2.2 Continuous random variables and probability distributions\n\nWhat happens when our outcome is continuous?\nThere are an infinite number of outcomes. This makes the denominator of our fraction difficult to work with.\nThe probability of the whole space must equal 1.\nThe domain may not span -\\(\\infty\\) to \\(\\infty\\).\n\nEven space between 0 and 1 is infinite!\n\nTwo common examples are the uniform and normal probability distributions, which we will discuss below.\n\n\n\n\n7.1.3 Functions describing probability distributions\n\n7.1.3.1 Probability Mass Function (PMF)\nProbability of each occurrence encoded in probability mass function (PMF)\n\n\\(0 \\leq f(x_i) \\leq 1\\)\n\nProbability of any value occurring must be between 0 and 1.\n\n\\(\\displaystyle\\sum_{x}f(x_i) = 1\\)\n\nProbabilities of all values must sum to 1.\n\n\n\n\n7.1.3.2 Probability Density Function (PDF)\n\nSimilar to PMF from before, but for continuous variables.\nUsing integration, it gives the probability a value falls within a particular interval\n\n\\(P[a\\le X\\le b] = \\displaystyle\\int_a^b f(x) \\, dx\\)\nTotal area under the curve is 1.\n\\(P(a &lt; X &lt; b)\\) is the area under the curve between \\(a\\) and \\(b\\) (where \\(b &gt; a\\)).\n\n\n\n\n7.1.3.3 Cumulative Density Function (CDF)\n\nDiscrete\n\nCumulatve density function is probability X will take a value of x or lower.\nPDF is written \\(f(x)\\), and CDF is written \\(F'(x)\\). \\[F_X(x) = Pr(X\\leq x)\\]\nFor discrete CDFs, that means summing up over all values.\nWhat is the probability of rolling a 6 or lower with two dice? \\(F(6)=?\\)\n\n\n\nContinuous\n\nWe can’t sum probabilities for continuous distributions (remember the 0 problem).\nSolution: integration \\[F_Y(y) = \\int_{-\\infty}^{y} f(y) dy\\]\nExamples of uniform distribution.\n\n\n\n\n\n7.1.4 Common types of probability distributions\nThere are many useful probability distributions. In this section we will cover three of the most common ones: the binomial, uniform, and normal distributions.\n\n7.1.4.1 Binomial distribution\nA Binomial distribution is defined as follow: \\(X \\sim Binomial(n, p)\\)\nPMF:\n\\[\n{n \\choose k} p^k(1-p)^{n-k}\n\\] , where \\(n\\) is the number of trials, \\(p\\) is the probability of success, and \\(k\\) is the number of successes.\nRemember that:\n\\[\n{n \\choose k} = \\frac{n!}{k!(n-k)!}\n\\]\nFor example, let’s say that voters choose some candidate with probability 0.02. What is the probability of seeing exactly 0 voters of the candidate in a sample of 100 people?\nWe can compute the PMF of a binomial distribution using R’s dbinom() function.\n\ndbinom(x = 0, size = 100, prob = 0.01)\n\n[1] 0.3660323\n\n\n\ndbinom(x = 1, size = 100, prob = 0.01)\n\n[1] 0.3697296\n\n\nSimilarly, we can compute the CDF using R’s pbinom() function:\n\npbinom(q = 0, size = 100, prob = 0.01)\n\n[1] 0.3660323\n\n\n\npbinom(q = 100, size = 100, prob = 0.01)\n\n[1] 1\n\n\n\npbinom(q = 1, size = 100, prob = 0.01)\n\n[1] 0.735762\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCompute the probability of seeing between 1 and 10 voters of the candidate in a sample of 100 people.\n\n\n\n\n7.1.4.2 Uniform distribution\nA uniform distribution has two parameters: a minimum and a maximum. So \\(X \\sim U(a, b)\\).\n\nPDF:\n\n\\[\n\\displaystyle{\n\\begin{cases}\n  \\frac{1}{b-a} & \\text{, }{x \\in [a, b]}\\\\\n  0             & \\text{, otherwise}\n\\end{cases}\n}\n\\]\n\nCDF:\n\n\\[\n\\displaystyle{\n\\begin{cases}\n  0 & \\text{, }{x &lt; a}\\\\\n  \\frac{x-a}{b-a} & \\text{, }{x \\in [a, b]}\\\\\n  1             & \\text{, }{x&gt;b}\n\\end{cases}\n}\n\\]\nIn R, dunif() gives the PDF of a uniform distribution. By default, it is \\(X \\sim U(0, 1)\\).\n\nlibrary(tidyverse)\n\n\nggplot() +\n  stat_function(fun = dunif, xlim = c(-4, 4))\n\n\n\n\nMeanwhile, punif() evaluates the CDF of a uniform distribution.\n\npunif(q = .3)\n\n[1] 0.3\n\n\n\n\n\n\n\n\nExercise\n\n\n\nEvaluate the CDF of \\(Y \\sim U(-2, 2)\\) at point \\(y = 1\\). Use the formula and punif().\n\n\n\n\n7.1.4.3 Normal distribution\nA normal distribution has two parameters: a mean and a standard deviation. So \\(X \\sim N(\\mu, \\sigma)\\).\n\nPDF: \\(2 {\\displaystyle {\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}}\\)\n\nIn R, dnorm() gives us the PDF of a standard normal distribution (\\(Z \\sim N(0, 1)\\)):\n\nggplot() +\n  stat_function(fun = dnorm, xlim = c(-4, 4))\n\n\n\n\nLike you might expect, pnorm() computes the CDF of a normal distribution (by default, the standard normal).\n\npnorm(0)\n\n[1] 0.5\n\n\n\npnorm(1) - pnorm(-1)\n\n[1] 0.6826895\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat is the probability of obtaining a value above 1.96 or below -1.96 in a standard normal probability distribution? Hint: use the pnorm() function."
  },
  {
    "objectID": "07_probability_stats_sims.html#statistics",
    "href": "07_probability_stats_sims.html#statistics",
    "title": "7  Probability, statistics, and simulations",
    "section": "7.2 Statistics",
    "text": "7.2 Statistics\n\nThe problems considered by probability and statistics are inverse to each other. In probability theory we consider some underlying process which has some randomness or uncertainty modeled by random variables, and we figure out what happens. In statistics we observe something that has happened, and try to figure out what underlying process would explain those observations. (quote attributed to Persi Diaconis)\n\n\nIn statistics we try to learn about a data-generating process (DGP) using our observed data. Example: GDP statistics.\nUsually we are restrained to samples, while our DGPs of interest are population-based.\n\nSo we use random sampling or refer to superpopulations as a way to justify how the data we observe can reasonably approximate the population.\n\nStatistics has two main targets:\n\nEstimation: how we find a reasonable guess of an unknown property (parameter) of a DGP\nInference: how we describe uncertainty about our estimate\n\nWe use an estimator (\\(\\hat\\theta\\)), which is a function that summarizes data, as guess about a parameter \\(\\theta\\).\nTheoretical statistics is all about finding “good” estimators (let’s see an example of different estimators). A few properties of good estimators:\n\nUnbiasedness: Across multiple random samples, an unbiased estimator gets the right answer on average.\nLow variance: Across multiple random samples, a low-variance estimator is more concentrated around the true paramater.\nBUT it’s usually hard to get both unbiasedness and low variance. We usually quantify this via the mean squared error: \\(MSE = bias^2 + variance\\). Comparing two estimators, the one with the lowest MSE is said to be more efficient.\nConsistency: A consistent estimator convergences in probability to the true value. “If we had enough data, the probability that our estimate would be far from the truth would be close to zero” (Aronow and Miller 2019, p. 105).\n\nApplied statistics is about using these techniques reasonably in messy real-world situations…"
  },
  {
    "objectID": "07_probability_stats_sims.html#simulations",
    "href": "07_probability_stats_sims.html#simulations",
    "title": "7  Probability, statistics, and simulations",
    "section": "7.3 Simulations",
    "text": "7.3 Simulations\n\nIn simulations, we generate fake data following standard procedues. Why?\n\nTo better understand how our estimators work in different settings (the methods reason)\nTo get insights about complex processes with many moving parts (the substantive reason) (let’s talk about gerrymandering).\n\n\nBefore we jump into an example, we’ll review some R tools that will build up to simulations.\n\n7.3.1 Random sampling from data\nIn this module we will work with good ol’ mtcars, one of R’s most notable default datasets. We’ll assign it to an object so it shows in our Environment pane:\n\nmy_mtcars &lt;- mtcars\n\n\n\n\n\n\n\nTip\n\n\n\nDefault datasets such as mtcars and iris are useful because they are available to everyone, and once you become familiar with them, you can start thinking about the code instead of the intricacies of the data. These qualities also make default datasets ideal for building reproducible examples (see Wickham 2014)\n\n\nWe can use the function sample() to obtain random values from a vector. The size = argument specifies how many values we want. For example, let’s get one random value of the “mpg” column:\n\nsample(my_mtcars$mpg, size = 1)\n\n[1] 16.4\n\n\nEvery time we run this command, we can get a different result:\n\nsample(my_mtcars$mpg, size = 1)\n\n[1] 26\n\n\n\nsample(my_mtcars$mpg, size = 1)\n\n[1] 21.4\n\n\nIn some occasions we do want to get the same result consistently after running some random process multiple times. In this case, we set a seed, which takes advantage of R’s pseudo-random number generator capabilities. No matter how many times we run the following code block, the result will be the same:\n\nset.seed(123)\nsample(my_mtcars$mpg, size = 1)\n\n[1] 15\n\n\nSampling with replacement means that we can get the same value multiple times. For example:\n\nset.seed(12)\nsample(c(\"Banana\", \"Apple\", \"Orange\"), size = 3, replace = T)\n\n[1] \"Apple\"  \"Apple\"  \"Orange\"\n\n\n\nsample(my_mtcars$mpg, size = 100, replace = T)\n\n  [1] 26.0 15.2 18.7 18.7 30.4 21.0 24.4 26.0 32.4 15.8 32.4 19.2 18.1 16.4 19.2\n [16] 27.3 14.3 10.4 17.3 13.3 21.4 13.3 19.2 24.4 15.0 27.3 17.8 15.2 15.8 14.3\n [31] 19.7 16.4 18.7 15.8 19.2 21.0 14.3 15.2 14.3 27.3 21.4 33.9 33.9 21.4 30.4\n [46] 33.9 21.4 17.3 17.3 10.4 26.0 18.7 15.2 30.4 10.4 10.4 15.5 14.3 26.0 17.3\n [61] 33.9 26.0 24.4 18.7 30.4 32.4 21.5 30.4 15.2 27.3 13.3 17.3 21.4 24.4 13.3\n [76] 22.8 33.9 13.3 21.5 14.3 19.2 30.4 24.4 26.0 15.8 10.4 24.4 14.3 15.2 10.4\n [91] 19.2 21.0 16.4 19.2 24.4 19.7 18.7 10.4 18.7 17.8\n\n\nIn order to sample not from a vector but from a data frame’s rows, we can use the slice_sample() function from dplyr:\n\nmy_mtcars |&gt; \n  slice_sample(n = 2) # a number of rows\n\n                  mpg cyl disp  hp drat   wt  qsec vs am gear carb\nDodge Challenger 15.5   8  318 150 2.76 3.52 16.87  0  0    3    2\nDatsun 710       22.8   4  108  93 3.85 2.32 18.61  1  1    4    1\n\n\n\nmy_mtcars |&gt; \n  slice_sample(prop = 0.5) # a proportion of rows\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n\n\nAgain, we can also use seeds here to ensure that we’ll get the same result each time:\n\nset.seed(123)\nmy_mtcars |&gt; \n  slice_sample(prop = 0.5)\n\n                    mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMaserati Bora      15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nCadillac Fleetwood 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nHonda Civic        30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nMerc 450SLC        15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nDatsun 710         22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 280           19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nFiat 128           32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nDodge Challenger   15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 280C          17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nHornet Sportabout  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nToyota Corolla     33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFord Pantera L     15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nAMC Javelin        15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nFerrari Dino       19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 230           22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nLotus Europa       30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n\n\nAnd we can also sample with replacement:\n\nset.seed(123)\nmy_mtcars |&gt; \n  slice_sample(prop = 1, replace = T)\n\n                        mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMaserati Bora          15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nCadillac Fleetwood     10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nHonda Civic...3        30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nMerc 450SLC...4        15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nDatsun 710...5         22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 280...6           19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nFiat 128               32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nDodge Challenger...8   15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 280C              17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nHornet Sportabout...10 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nToyota Corolla         33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nMerc 450SLC...12       15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nDodge Challenger...13  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nPontiac Firebird...14  19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9...15         27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2...16     26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nVolvo 142E             21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nHornet Sportabout...18 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nHonda Civic...19       30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nPorsche 914-2...20     26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nPontiac Firebird...21  19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nLotus Europa           30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nPontiac Firebird...23  19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nMerc 230...24          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFord Pantera L         15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDatsun 710...26        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D              24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nFiat X1-9...28         27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nDuster 360             14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 280...30          19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 230...31          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFerrari Dino           19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n\n\n\n\n7.3.2 Random sampling from theoretical distributions\nWe can also draw sample numbers from theoretical distributions.\n\nUniform distribution\nFor the uniform distribution, the arguments specify how many draws we want and the boundaries\n\nrunif(n = 20, min = -3, max = 3)\n\n [1]  1.1442317  1.7728045 -2.8523179 -0.1332242  1.5507572 -1.7015524\n [7] -1.0909140 -1.6102453 -2.1431999 -0.5127220 -0.5176540 -0.7869273\n[13] -2.0853315 -2.1671636 -1.6017954 -0.2042253 -1.4041642  2.1469663\n[19] -2.7250130 -0.3467996\n\n\nWhen we draw a million times from the distribution, we can then plot it and see that it does look as we would expect:\n\nset.seed(123)\nmy_runif &lt;- runif(n = 1000000, min = -3, max = 3)\n\n\nggplot(data.frame(my_runif), aes(x = my_runif)) +\n  geom_histogram(binwidth = 0.25, boundary = 0, closed = \"right\") +\n  scale_x_continuous(breaks = seq(-5, 5, 1), limits = c(-5, 5))\n\n\n\n\n\n\nBinomial distribution\nFor the binomial distribution, we can specify the number of draws, how many trials each draw will have, and the probability of success.\nFor instance, we can ask R to do the following twenty times: flip a fair coin one hundred times, and count the number of tails.\n\nrbinom(n = 20, size = 100, prob = 0.5)\n\n [1] 48 45 54 50 58 50 42 58 48 57 53 49 52 51 49 40 57 53 52 41\n\n\nWith prob = , we can implement unfair coins:\n\nrbinom(n = 20, size = 100, prob = 0.9)\n\n [1] 88 87 93 95 93 92 91 94 87 91 90 92 93 89 90 95 91 90 86 88\n\n\n\n\nNormal distribution\nFor the Normal or Gaussian distribution, we specify the number of draws, the mean, and standard deviation:\n\nrnorm(n = 20, mean = 0, sd = 1)\n\n [1]  1.10455864  0.06386693 -1.59684275  1.86298270 -0.90428935 -1.55158044\n [7]  1.27986282 -0.32420495 -0.70015076  2.17271578  0.89778913 -0.01338538\n[13] -0.74074395  0.36772316 -0.66453402 -1.11498344 -1.15067439 -0.55098894\n[19]  0.10503154 -0.27183645\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCompute and plot my_rnorm, a vector with one million draws from a Normal distribution \\(Z\\) with mean equal to zero and standard deviation equal to one (\\(Z\\sim N(0,1)\\)). You can recycle code from what we did for the uniform distribution!\n\n\n\n\n\n7.3.3 Loops\nLoops allow us to repeat operations in R. The most common construct is the for-loop:\n\nfor (i in 1:10){\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\n\nWe talked about loops and various extensions in one of our methods workshops last year: Speedy R.\n\n\n\n7.3.4 An example simulation: bootstrapping a sample mean\nBootstrap (and its relatives) is one way in which we can do inference. We’ll go through the intuition on the board.\n\nbootstrapped_means &lt;- vector(mode = \"numeric\", length = 10000)\nfor (i in 1:10000){\n  m &lt;- my_mtcars |&gt; slice_sample(prop = 1, replace = T)\n  bootstrapped_means[i] &lt;- mean(m$mpg)\n}\n\n\nggplot(data.frame(bootstrapped_means), aes(x = bootstrapped_means)) +\n  geom_histogram(binwidth = 0.25, boundary = 0, closed = \"right\")\n\n\n\n\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. First edition. Beijing ; Boston: O’Reilly.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/."
  },
  {
    "objectID": "07_probability_stats_sims.html#footnotes",
    "href": "07_probability_stats_sims.html#footnotes",
    "title": "7  Probability, statistics, and simulations",
    "section": "",
    "text": "This is sometimes called the frequentist interpretation of probability. There are other possibilities, such as Bayesian interpretations of probability, which describe probabilities as degrees of belief.↩︎"
  },
  {
    "objectID": "08_text_analysis.html#strings",
    "href": "08_text_analysis.html#strings",
    "title": "8  Text analysis",
    "section": "8.1 Strings",
    "text": "8.1 Strings\n\nIn R, a piece of text is represented as a sequence of characters (letters, numbers, and symbols).\nA string is a sequence of characters, which is used for storing text.\n\nFor example, “methods” is a string that includes characters: m, e, t, h, o, d, s.\n\nCreating strings is very straightforward in RStudio. We assign character values to a variable, being sure to enclose the character values (the text) in double or single quotation marks.\n\nWe can create strings of single words, or whole sentences if we so wish.\n\n\n\nstring1 &lt;- \"camp\" \nstring1\n\n[1] \"camp\"\n\nstring2 &lt;- \"I love methods camps.\"\nstring2\n\n[1] \"I love methods camps.\"\n\n\n\nWe can also create a vector of strings.\n\n\nstring3 &lt;- c(\"I\", \"love\", \"methods\", \"camp\", \".\")\nstring3\n\n[1] \"I\"       \"love\"    \"methods\" \"camp\"    \".\""
  },
  {
    "objectID": "08_text_analysis.html#string-manipulation",
    "href": "08_text_analysis.html#string-manipulation",
    "title": "8  Text analysis",
    "section": "8.2 String manipulation",
    "text": "8.2 String manipulation\n\nOften, strings, and more broadly text, contain information that we want to extract for the purpose of our research.\n\nFor example, perhaps we wanted to count the number of times a certain country was mentioned during the U.S. President’s annual State of the Union Address.\n\nFor tasks such as these, we can use regular expressions (also known as ‘regex’), which search for one or more specified pattern of characters.\n\nThese patterns can be exact matches, or more general.\n\n\n\ntest &lt;- \"test\"\n\n\nRegular expressions can be used to:\n\nExtract information from text.\nParse text.\nClean/replace strings.\n\n\n\n\n\n\n\n\nNote\n\n\n\nFortunately, the syntax for regular expressions is relatively stable across all programming languages (e.g., Java, Python, R).\n\n\n\n8.2.1 Using the stringr package\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nstringr comes with the tidyverse and provides functions for both (a) basic string manipulations and (b) regular expression operations. Some basic functions are listed below:\n\n\n\n\nFunction\nDescription\n\n\n\n\nstr_c()\nstring concatenation\n\n\nstr_length()\nnumber of characters\n\n\nstr_sub()\nextracts substrings\n\n\nstr_dup()\nduplicates characters\n\n\nstr_trim()\nremoves leading and trailing whitespace\n\n\nstr_pad()\npads a string\n\n\nstr_wrap()\nwraps a string paragraph\n\n\nstr_trim()\ntrims a string\n\n\n\n\nLet’s try some examples of basic string manipulation using stringr:\n\n\nmy_string &lt;- \"I know people who have seen the Barbie movie 2, 3, even 4 times!\"\nmy_string\n\n[1] \"I know people who have seen the Barbie movie 2, 3, even 4 times!\"\n\n\n\nOne common thing we want to do with strings is lowercase them:\n\n\nlower_string &lt;- str_to_lower(my_string)\nlower_string\n\n[1] \"i know people who have seen the barbie movie 2, 3, even 4 times!\"\n\n\n\nWe can also combine (concatenate) strings using the str_c() command:\n\n\nmy_string2 &lt;- \"I wonder if they have seen Oppenheimer, too.\"\ncat_string &lt;- str_c(my_string, my_string2, sep = \" \")\ncat_string\n\n[1] \"I know people who have seen the Barbie movie 2, 3, even 4 times! I wonder if they have seen Oppenheimer, too.\"\n\n\n\nWe can also split up strings on a particular character sequence.\n\n! denotes where split occurs and deletes the “!” The double bracket instructs to grab the first part of the split string.\n\n\n\nmy_string_vector &lt;- str_split(cat_string, \"!\")[[1]] \nmy_string_vector\n\n[1] \"I know people who have seen the Barbie movie 2, 3, even 4 times\"\n[2] \" I wonder if they have seen Oppenheimer, too.\"                  \n\n\n\nWe can also find which strings in a vector contain a particular character or sequence of characters.\n\nThe grep() (Globally search for Regular Expression and Print) command will return any instance that (partially) matches the provided pattern.\nClosely related to the grep() function is the grepl() function, which returns a logical for whether a string contains a character or sequence of characters.\n\n\n\ngrep(\"Barbie\",\n     cat_string,\n     value = FALSE,\n     ignore.case = TRUE)\n\n[1] 1\n\n# To search for some special characters (e.g., \"!\"), you need to \"escape\" it\ngrep(\"\\\\!\", cat_string, value = TRUE)\n\n[1] \"I know people who have seen the Barbie movie 2, 3, even 4 times! I wonder if they have seen Oppenheimer, too.\"\n\ngrepl(\"\\\\!\", cat_string)\n\n[1] TRUE\n\n\n\nThe str_replace_all function can be used to replace all instances of a given string, with an alternative string.\n\n\nstr_replace_all(cat_string, \"e\", \"_\")\n\n[1] \"I know p_opl_ who hav_ s__n th_ Barbi_ movi_ 2, 3, _v_n 4 tim_s! I wond_r if th_y hav_ s__n Opp_nh_im_r, too.\"\n\n\n\nWe can also pull out all sub-strings matching a given string argument.\n\nThis becomes especially useful when we generalize the patterns of interest.\n\n\n\nstr_extract_all(cat_string, \"have\")\n\n[[1]]\n[1] \"have\" \"have\"\n\nstr_extract_all(cat_string,\"[0-9]+\")[[1]] \n\n[1] \"2\" \"3\" \"4\"\n\n#   The square brackets define a set of possibilities.\n#   The \"0-9\" says the possibilities are any digit from 0 to 9.\n#   The \"+\" means \"one or more of the just-named thing\"\n\nstr_extract_all(cat_string,\"\\\\d+\")[[1]] #   Instead of 0-9, we can just say \"\\\\d\" for digits\n\n[1] \"2\" \"3\" \"4\"\n\nstr_extract_all(cat_string,\"[a-zA-Z]+\")[[1]] # letters\n\n [1] \"I\"           \"know\"        \"people\"      \"who\"         \"have\"       \n [6] \"seen\"        \"the\"         \"Barbie\"      \"movie\"       \"even\"       \n[11] \"times\"       \"I\"           \"wonder\"      \"if\"          \"they\"       \n[16] \"have\"        \"seen\"        \"Oppenheimer\" \"too\"        \n\nstr_extract_all(cat_string,\"\\\\w+\")[[1]] # \"word\" characters\n\n [1] \"I\"           \"know\"        \"people\"      \"who\"         \"have\"       \n [6] \"seen\"        \"the\"         \"Barbie\"      \"movie\"       \"2\"          \n[11] \"3\"           \"even\"        \"4\"           \"times\"       \"I\"          \n[16] \"wonder\"      \"if\"          \"they\"        \"have\"        \"seen\"       \n[21] \"Oppenheimer\" \"too\"        \n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat score (out of 10) would you give Barbie or Oppenheimer? Write your score in one sentence (e.g., “I would give Barbie seven of ten stars”.) If you have not seen either, write a sentence about which you would like to see more.\nStore that text as a string (string3) and combine it with our existing cat_string to produce a new concatenated string called cat_string2. Finally, count the total of characters within cat_string2. Your code:"
  },
  {
    "objectID": "08_text_analysis.html#simple-text-analysis",
    "href": "08_text_analysis.html#simple-text-analysis",
    "title": "8  Text analysis",
    "section": "8.3 Simple text analysis",
    "text": "8.3 Simple text analysis\n\nWe can use the tidytext package to conduct some basic text analysis using tidy data principles.\nAs Wickham 2014 reminds us, tidy data has a specific structure:\n\nEach variable is a column.\nEach observation is a row.\nEach type of observational unit is a table.\n\nWe can thus define the format as a table with one-token-per-row.\n\nA token is a unit of text (e.g., word) that we use for analysis. Tokenization is the process of turning text into tokens.\n\nAs Silge and Robinson (2017) remind us, it is important to contrast this structure with the alternative ways that text is often structured and stored in text analysis:\n\nString: Text can be stored as strings, i.e., character vectors. Text data is often first read into memory in this form.\nCorpus: These objects usually contain raw strings annotated with metadata and details.\nDocument-term matrix: This sparse matrix describe a collection (i.e., a corpus) of documents with one row for each document and one column for each term. The value in the matrix is typically word count or tf-idf (term frequency-inverse document frequency).\n\nLet’s try an example. To create a tidy text dataset, we need to first put some text into a data frame.\n\nWe print out each line as a “tibble,” which has a convenient print method that does not convert strings to factors or use row names.\n\n\n\nlibrary(dplyr)\n\nbarbie &lt;- c(\"I'm a Barbie girl in the Barbie world\",\n            \"Life in plastic, it's fantastic\",\n            \"You can brush my hair, undress me everywhere\",\n            \"Imagination, life is your creation\")\nbarbie\n\n[1] \"I'm a Barbie girl in the Barbie world\"       \n[2] \"Life in plastic, it's fantastic\"             \n[3] \"You can brush my hair, undress me everywhere\"\n[4] \"Imagination, life is your creation\"          \n\n\n\nbarbie_df &lt;- tibble(line = 1:4, text = barbie)\nbarbie_df\n\n# A tibble: 4 × 2\n   line text                                        \n  &lt;int&gt; &lt;chr&gt;                                       \n1     1 I'm a Barbie girl in the Barbie world       \n2     2 Life in plastic, it's fantastic             \n3     3 You can brush my hair, undress me everywhere\n4     4 Imagination, life is your creation          \n\n\n\nWe then break the text into individual tokens (tokenization) using tidytext’s unnest_tokens() function.\n\nThe two basic arguments for the unnest_tokens() function are column names. We have the output column, word, created by unnesting the text, and we have the input column, text, where the text being unnested comes from.\n\n\n\ninstall.packages(\"tidytext\")\n\n\nlibrary(tidytext)\n\nbarbie_df |&gt;\n  unnest_tokens(word, text)\n\n# A tibble: 26 × 2\n    line word  \n   &lt;int&gt; &lt;chr&gt; \n 1     1 i'm   \n 2     1 a     \n 3     1 barbie\n 4     1 girl  \n 5     1 in    \n 6     1 the   \n 7     1 barbie\n 8     1 world \n 9     2 life  \n10     2 in    \n# ℹ 16 more rows\n\n\n\n8.3.1 Counts\n\nOnce we have our tidy structure, we can then perform very simple tasks such as finding the most common words in our text as a whole. Let’s instead work with a short passage from a famous 1965 interview with J. Robert Oppenheimer (Pontin 2007).\n\nWe can use the count() function from the dplyr package with ease here.\n\n\n\noppenheimer &lt;- c(\"We knew the world would not be the same.\",\n                 \"A few people laughed, a few people cried, most people were silent.\",\n                 \"I remembered the line from the Hindu scripture, the Bhagavad-Gita.\",\n                 \"Vishnu is trying to persuade the Prince that he should do his duty and to impress him \n                 takes on his multi-armed form and says, “Now, I am become Death, the destroyer of\n                 worlds.”\", \n                 \"I suppose we all thought that one way or another.\")\n\nopp_df &lt;- tibble(line = 1:5, text = oppenheimer)\n\n\nopp_tok &lt;- unnest_tokens(opp_df, word, text)\n\nopp_tok |&gt;\n  count(word, sort = TRUE)\n\n# A tibble: 59 × 2\n   word       n\n   &lt;chr&gt;  &lt;int&gt;\n 1 the        7\n 2 i          3\n 3 people     3\n 4 a          2\n 5 and        2\n 6 few        2\n 7 his        2\n 8 that       2\n 9 to         2\n10 we         2\n# ℹ 49 more rows\n\n\n\nOur word counts are stored in a tidy data frame, which allows us to pipe these data directly to the ggplot2 package and create a simple visualization of the most common words in the short excerpt.\n\n\nopp_tok |&gt;\n  count(word, sort = TRUE) |&gt;\n  filter(n &gt; 1) |&gt;\n  mutate(word = reorder(word, n)) |&gt;\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nLook up the lyrics to your favorite song at the moment (no guilty pleasures here!). Then, follow the process described above to count the words: store the text as a string, convert to a tibble, tokenize, and count.\nWhen you are done counting, create a visualization for the chorus using the ggplot code above. Your code:\n\n\nIf you are curious about the repetitiveness of lyrics in pop music over time, I might recommend checking out this fun article and analysis done by Colin Morris at The Pudding.\n\n\n8.3.2 tf-idf\n\nAnother way to quantify what a document is about is to calculate a term’s inverse document frequency (idf), which decreases the weight for commonly used words and increases the weight for words that are not used as frequently in a corpus.\nIf we multiply together the term frequency (tf) with the idf, we can calculate the tf-idf, the frequency of a term adjusted for how infrequently it is used.\n\nThe tf-idf statistic measures how important a word is to document that is part of a corpus.\n\nWe are going to take a look at the published novels of Jane Austen, an example from Silge and Robinson (2017).\n\nLet’s start by calculating the term frequency.\n\n\n\nlibrary(janeaustenr)\n\nbook_words &lt;- austen_books() |&gt;\n  unnest_tokens(word, text) |&gt;\n  count(book, word, sort = TRUE)\n\ntotal_words &lt;- book_words |&gt; \n  summarize(total = sum(n), .by = book)\n\nbook_words &lt;- left_join(book_words, total_words)\n\nbook_words\n\n# A tibble: 40,379 × 4\n   book              word      n  total\n   &lt;fct&gt;             &lt;chr&gt; &lt;int&gt;  &lt;int&gt;\n 1 Mansfield Park    the    6206 160460\n 2 Mansfield Park    to     5475 160460\n 3 Mansfield Park    and    5438 160460\n 4 Emma              to     5239 160996\n 5 Emma              the    5201 160996\n 6 Emma              and    4896 160996\n 7 Mansfield Park    of     4778 160460\n 8 Pride & Prejudice the    4331 122204\n 9 Emma              of     4291 160996\n10 Pride & Prejudice to     4162 122204\n# ℹ 40,369 more rows\n\n\n\nWe can then take these data and visualize them for each of the books in the dataset.\n\n\nggplot(book_words, aes(x = n/total, fill = book)) +\n  geom_histogram(show.legend = FALSE) +\n  scale_x_continuous(limits = c(NA, 0.0009)) + # removes some observations\n  facet_wrap(~book, ncol = 2, scales = \"free_y\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 896 rows containing non-finite values (`stat_bin()`).\n\n\nWarning: Removed 6 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\nThe bind_tf_idf() function in the tidytext package then takes a dataset as input with one row per token (term) per document, calculating the tf-idf statistics. Let’s look at terms with high scores.\n\nBelow we see all proper nouns, mostly names of characters. None of them occur across all of Jane Austen’s novels, which is why they are important, defining terms for each of the texts.\n\n\n\nbook_tf_idf &lt;- book_words |&gt;\n  bind_tf_idf(word, book, n)\n\nbook_tf_idf |&gt;\n  select(-total) |&gt;\n  arrange(-tf_idf)\n\n# A tibble: 40,379 × 6\n   book                word          n      tf   idf  tf_idf\n   &lt;fct&gt;               &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Sense & Sensibility elinor      623 0.00519  1.79 0.00931\n 2 Sense & Sensibility marianne    492 0.00410  1.79 0.00735\n 3 Mansfield Park      crawford    493 0.00307  1.79 0.00551\n 4 Pride & Prejudice   darcy       373 0.00305  1.79 0.00547\n 5 Persuasion          elliot      254 0.00304  1.79 0.00544\n 6 Emma                emma        786 0.00488  1.10 0.00536\n 7 Northanger Abbey    tilney      196 0.00252  1.79 0.00452\n 8 Emma                weston      389 0.00242  1.79 0.00433\n 9 Pride & Prejudice   bennet      294 0.00241  1.79 0.00431\n10 Persuasion          wentworth   191 0.00228  1.79 0.00409\n# ℹ 40,369 more rows\n\n\n\nLet’s end with a visualization for the high tf-idf words in each of Jane Austen’s novels.\n\nThese results highlight that what distinguishes one novel from another within the collection of her works (the corpus) are the proper nouns, mainly the names of people and places. These are the terms that are “important” for defining the character of each document.\n\n\n\nbook_tf_idf |&gt;\n  slice_max(tf_idf, n = 15, by = book) |&gt;\n  ggplot(aes(x = tf_idf, y = fct_reorder(word, tf_idf), fill = book)) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~book, ncol = 2, scales = \"free\") +\n    labs(x = \"tf-idf\", y = \"\")\n\n\n\n\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. First edition. Beijing ; Boston: O’Reilly.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/."
  },
  {
    "objectID": "09_wrapup.html#project-management",
    "href": "09_wrapup.html#project-management",
    "title": "9  Wrap up",
    "section": "9.1 Project management",
    "text": "9.1 Project management\n\n9.1.1 RStudio projects\n\nRStudio projects are an excellent way to keep all the files associated with a project (data, R scripts, results, figures, etc.) in one place on your computer.\nThis is one of the best ways to improve your workflow in RStudio, allowing you to:\n\nCreate a project for each paper or data analysis project.\nStore data files in one place.\nSave, edit, and run scripts.\nKeep outputs such as plots and cleaned data.\n\nTo create a new project file, click File &gt; New Project, then:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCall your project some version of “methodscamptest” and choose carefully where you wish to store the project on your machine.\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you don’t store your project (and your other files, too!) somewhere reasonable, it will be hard to find it in the future! We recommend creating a clear organizational scheme for yourself early on.\n\n\n\n9.1.1.1 Using RStudio projects\nWhen using an RStudio project, you should see its name in the top-right corner of RStudio, next to a light blue icon. You can check with R the folder in which your project operates:\n\ngetwd()\n\n\nNow, as an example, let’s run the following commands in the script editor and save the files into the project directory.\n\n\nlibrary(tidyverse)\n\nmy_plot &lt;- ggplot(mtcars, aes(wt, mpg)) + \n  geom_point()\n\nggsave(plot = my_plot,\n       filename = \"plot_mtcars.pdf\")\n\nwrite_csv(mtcars, \"mtcars.csv\")\n\n\nQuit RStudio and check out the folder associated with the project.\nYou should see the PDF file for the plot, the .csv file for the data, and the .Rproj file for the project itself.\nDouble-click the .Rproj file to reopen the project and pick up where you left off! Everything you need should be ready to go."
  },
  {
    "objectID": "09_wrapup.html#other-resources",
    "href": "09_wrapup.html#other-resources",
    "title": "9  Wrap up",
    "section": "9.2 Other resources",
    "text": "9.2 Other resources\n\n9.2.1 Overleaf\n\n\n\n\nOverleaf is a collaborative cloud-based LaTeX editor designed for writing, editing, and publishing documents.\n\nLaTeX is a software used for typesetting technical documents. It is used widely in our discipline for the preparation for manuscripts to journals and other publishing venues.\n\nUT Austin actually provides free access to Overleaf Professional to all graduate students using your UT email.\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate an Overleaf Professional account using your UT email address. You can do so here.\n\n\n\nOverleaf Professional upgrades include:\n\nReal-time collaboration\nReal-time track changes and visible collaborator cursor(s)\nReal-time PDF preview of your document while editing and writing\nFull history view of your documents\nTwo-way sync with Dropbox and GitHub\nReference manager sync and advanced reference search.\nUT Austin resource portal, including UT Austin templates, FAQs, and resource links\n\n\n\n\n\n\n\n\nImportant\n\n\n\nLaTeX is actually the markup language that powers this website! If you are curious about general syntax and commands, you can access our repository at any time to get a closer look.\n\n\n\n\n9.2.2 Zotero\n\n\n\n\nZotero is an open-source reference manager used to store, manage, and cite bibliographic references, such as books and articles.\nWhen it is time to write, you can insert your sources directly into your paper as in-text citations via a word processor plugin, which generates a bibliography in your style of choice.\n\nThis can save a lot of time, especially when you have to change citation styles for submission to another journal.\n\nYou can download the software for free here.\n\nYou can also find a guide on how to install it here.\n\n\n\n\n\n\n\n\nNote\n\n\n\nZotero is one of many other reference managers out there. Alternatives include Mendeley and EndNote, among others. You should choose whatever option best suits your needs.\n\n\n\n9.2.2.1 Benefits of Zotero\n\nIf you have not yet chosen a reference manager or are considering switching, below are some advantages of Zotero:\n\nWorks as a standalone desktop software with plugins for Chrome, Safari, and Firefox\nFull compatibility with Google docs\nFree plugin for Word and LibreOffice included\nIncludes most popular citation styles with more styles available on the Zotero Style Repository\nDrag and drop PDF files into the library, extracting metadata such as authors, year, etc.\nAllows advanced searches of all content in your library using full-text PDF indexing\nUse cloud storage (optional) and sync libraries across devices\nCreate unlimited private or public groups and collaborate by sharing files and citations\n300MB of free cloud storage and 2GB of storage for $20 USD/year (equal to $1.67 per month)\n\nHere is a comprehensive guide to unlocking all of Zotero’s potential."
  },
  {
    "objectID": "09_wrapup.html#methods-at-ut",
    "href": "09_wrapup.html#methods-at-ut",
    "title": "9  Wrap up",
    "section": "9.3 Methods at UT",
    "text": "9.3 Methods at UT\n\n9.3.1 Required methods courses\n\nScope and Methods of Political Science\n\nStatistics I (Statistics/linear regression)\n\nStatistics II (Linear regression and more)\nStatistics III (Maximum likelihood estimation)\n\nOnly required if your major field is methods\n\n\n\n\n9.3.2 Other methods courses\n\nStatistics/econometrics:\n\nBayesian Statistics\nCausal Inference\nMath Methods for Political Analysis\nTime Series and Panel Data\nPanel and Multilevel Analysis\n\n\n\n\n9.3.3 More courses\n\nFormal Theory\n\nIntro to Formal Political Analysis\nFormal Political Analysis II\nFormal Theories of International Relations\n\nEverything else\n\nConceptualization and Measurement\nExperimental Methods in Political Science\nQualitative Methods\nNetwork Analysis\nSeminar in Field Experiments\n\n\n\n\n9.3.4 Other departments at UT\nYou can also take courses through the Economics, Mathematics, or Statistics (Statistics and Data Science) departments.\n\nM.S. in Statistics\n\nSoftware and Topic Short Courses - R, Python, Stata, etc.\n\nMore info here.\n\n\n\n9.3.5 Other resources\nSummer programs at UT:\n\nShort courses in statistics\n\nDepartment sometimes offers scholarships to cover part of the cost.\n\n\nSummer programs outside UT:\n\nICPSR (Inter-university Consortium for Political and Social Research)\n\nAnn Arbor, Michigan\n\nEITM (Empirical Implications of Theoretical Models)\n\nHouston and other locations (Michigan, Duke, Berkeley, Emory)\n\nIQMR (Institute for Qualitative and Multi-Method Research)\n\nSyracuse, NY\n\n\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. First edition. Beijing ; Boston: O’Reilly.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/."
  },
  {
    "objectID": "solutions.html#intro-to-r",
    "href": "solutions.html#intro-to-r",
    "title": "Solutions to exercises",
    "section": "1. Intro to R",
    "text": "1. Intro to R\n\nExercise\nCreate your own code block below and run a math operation.\n\n\npi * 2\n\n[1] 6.283185\n\n\n\nExercise\nExamine the help file of the log() function. How can we compute the the base-10 logarithm of my_object? Your code:\n\n\n\nCode\n# setup: these steps were executed before the exercise\nmy_object &lt;- 10\n\n\n\nExamine the log() function.\n\n\n?log\n\n\nCompute the base-10 logarithm of my_object.\n\n\nlog10(my_object)\n\n[1] 1\n\n\n\nExercise\nObtain the maximum value of water content per 100g in the data. Your code:\n\n\n\nCode\n# setup: these steps were executed before the exercise\nmy_character_vector &lt;- c(\"Apple\", \"Orange\", \"Watermelon\", \"Banana\")\nmy_data_frame &lt;- data.frame(fruit = my_character_vector,\n                            calories_per_100g = c(52, 47, 30, 89),\n                            water_per_100g = c(85.6, 86.8, 91.4, 74.9))\nmy_data_frame\n\n\n\nmax(my_data_frame$water_per_100g)\n\n[1] 91.4"
  },
  {
    "objectID": "solutions.html#tidy-data-analysis-i",
    "href": "solutions.html#tidy-data-analysis-i",
    "title": "Solutions to exercises",
    "section": "2. Tidy data analysis I",
    "text": "2. Tidy data analysis I\n\nExercise\nSelect the variables last_name, party, num_votes, and agree from the data frame. Your code:\n\n\n\nCode\n# setup: these steps were executed before the exercise\n\n\n\nExercise\n\nAdd a new column to the data frame, called diff_agree, which subtracts agree and agree_pred. How would you create abs_diff_agree, defined as the absolute value of diff_agree? Your code:\nFilter the data frame to only get senators for which we have information on fewer than (or equal to) five votes. Your code:\nFilter the data frame to only get Democrats who agreed with Trump in at least 30% of votes. Your code:\n\n\n\n\nCode\n# setup: these steps were executed before the exercise\n\n\n\nAdd a new column to the data frame, called diff_agree, which subtracts agree and agree_pred. How would you create abs_diff_agree, defined as the absolute value of diff_agree? Your code:\n\n\nFilter the data frame to only get senators for which we have information on fewer than (or equal to) five votes. Your code:\n\n\nFilter the data frame to only get Democrats who agreed with Trump in at least 30% of votes. Your code:\n\n\nExercise\nArrange the data by diff_pred, the difference between agreement and predicted agreement with Trump. (You should have code on how to create this variable from the last exercise). Your code:\n\n\n\nCode\n# setup: these steps were executed before the exercise\n\n\n\nExercise\nObtain the maximum absolute difference in agreement with Trump (the abs_diff_agree variable from before) for each party.\n\n\n\nCode\n# setup: these steps were executed before the exercise\n\n\n\nExercise\nDraw a column plot with the agreement with Trump of Bernie Sanders and Ted Cruz. What happens if you use last_name as the y aesthetic mapping and agree in the x aesthetic mapping? Your code:\n\n\n\nCode\n# setup: these steps were executed before the exercise"
  },
  {
    "objectID": "solutions.html#matrices",
    "href": "solutions.html#matrices",
    "title": "Solutions to exercises",
    "section": "3. Matrices",
    "text": "3. Matrices\n\nExercise\nGet the product of the first three elements of vector \\(d\\). Write the notation by hand and use R to obtain the number.\n\\[\\overrightarrow d = \\begin{bmatrix}\n12 & 7 & -2 & 3 & 1\n\\end{bmatrix}\\]\n\n\n\nCode\n# setup: these steps were executed before the exercise\nvector_d &lt;- c(12, 7, -2, 3, -1)\n\n\n\\[\\prod_{i=1}^3 d_i = 12 \\cdot 7 \\cdot (-2) = -168\\]\n\nprod(vector_d[1:3])\n\n[1] -168\n\n\n\nExercise\n\nCalculate \\(A + B\\) \\[A= \\begin{bmatrix}\n1 & 0 \\\\\n-2 & -1\n\\end{bmatrix}\\]\n\n\\[B = \\begin{bmatrix}\n5 & 1 \\\\\n2 & -1\n\\end{bmatrix}\\]\n\n\nCalculate \\(A - B\\) \\[A= \\begin{bmatrix}\n6 & -2 & 8 & 12 \\\\\n4 & 42 & 8 & -6\n\\end{bmatrix}\\] \\[B = \\begin{bmatrix}\n18 & 42 & 3 & 7 \\\\\n0 & -42 & 15 & 4\n\\end{bmatrix}\\]\n\n\n\nA1 &lt;- matrix(c(1,-2,0,-1), nrow = 2)\nB1 &lt;- matrix(c(5,2,1,-1), nrow = 2)\nA1 + B1\n\n     [,1] [,2]\n[1,]    6    1\n[2,]    0   -2\n\n\n\nA2 &lt;- matrix(c(6,4,-2,42,8,8,12,-6), nrow = 2)\nB2 &lt;- matrix(c(18,0,42,-42,3,15,7,4), nrow = 2)\nA2 - B2\n\n     [,1] [,2] [,3] [,4]\n[1,]  -12  -44    5    5\n[2,]    4   84   -7  -10\n\n\n\nExercise\nCalculate \\(2\\times A\\) and \\(-3 \\times B\\). Again, do one by hand and the other one using R. \\[A= \\begin{bmatrix}\n1 & 4 & 8 \\\\\n0 & -1 & 3\n\\end{bmatrix}\\] \\[ B = \\begin{bmatrix}\n-15 & 1 & 5 \\\\\n2 & -42 & 0 \\\\\n7 & 1 & 6\n\\end{bmatrix}\\]\n\n\nA3 &lt;- matrix(c(1,0,4,-1,8,3), nrow = 2)\n2 * A3\n\n     [,1] [,2] [,3]\n[1,]    2    8   16\n[2,]    0   -2    6\n\n\n\nB3 &lt;- matrix(c(-15,2,7,1,-42,1,5,0,6), nrow = 3)\n-3 * B3\n\n     [,1] [,2] [,3]\n[1,]   45   -3  -15\n[2,]   -6  126    0\n[3,]  -21   -3  -18"
  },
  {
    "objectID": "solutions.html#tidy-data-analysis-ii",
    "href": "solutions.html#tidy-data-analysis-ii",
    "title": "Solutions to exercises",
    "section": "4. Tidy data analysis II",
    "text": "4. Tidy data analysis II\n\nExercise\n\nCreate a dummy variable, d_large_pop, for whether the country-year has a population of more than 1 million. Then compute its mean. Your code:\nWhich countries are recorded as “Never colonized”? Change their values to other reasonable codings and compute a tabulation with count(). Your code:\n\n\n\n\nCode\n# setup: these steps were executed before the exercise\nlibrary(tidyverse)\nqog_csv &lt;- read_csv(\"data/sample_qog_bas_ts_jan23.csv\")\nqog &lt;- qog_csv\n\n\n\nCreate the dummy variable d_large_pop.\n\n\nqog |&gt; \n  mutate(d_large_pop = if_else(wdi_pop &gt;= 1000000, 1, 0)) |&gt; \n  count(d_large_pop)\n\n# A tibble: 2 × 2\n  d_large_pop     n\n        &lt;dbl&gt; &lt;int&gt;\n1           0   341\n2           1   744\n\n\n\nChange the coding of “Never colonized” countries to something else, and compute a tabulation with count().\n\n\nExercise\nCalculate the median value of the corruption variable for each region (i.e., perform a grouped summary). Your code:\n\n\nqog |&gt; \n  summarize(med_corr = median(vdem_corr, na.rm = T), .by = region)\n\n# A tibble: 4 × 2\n  region           med_corr\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Caribbean          0.301 \n2 South America      0.531 \n3 Central America    0.734 \n4 Northern America   0.0505\n\n\n\nExercise\nConvert back gdp_long to a wide format using pivot_wider(). Check out the help file using ?pivot_wider. Your code:\n\n\n\nCode\n# setup: these steps were executed before the exercise\nlibrary(readxl)\ngdp &lt;- read_excel(\"data/wdi_gdp_ppp.xlsx\")\ngdp_long &lt;- gdp |&gt; \n  pivot_longer(cols = -c(country_name, country_code), \n               names_to = \"year\", \n               values_to = \"wdi_gdp_ppp\", \n               names_transform = as.integer) \n\n\n\nExercise\nThere is a dataset on country’s CO2 emissions, again from the World Bank (2023), in “data/wdi_co2.csv”. Load the dataset into R and add a new variable with its information, wdi_co2, to our qog_plus data frame. Finally, compute the average values of CO2 emissions per capita, by country. Tip: this exercise requires you to do many steps—plan ahead before you start coding! Your code:\n\n\n\nCode\n# setup: these steps were executed before the exercise\nlibrary(tidyverse)\nqog &lt;- read_csv(\"data/sample_qog_bas_ts_jan23.csv\")\ngdp &lt;- readxl::read_excel(\"data/wdi_gdp_ppp.xlsx\")\n\ngdp_long &lt;- gdp |&gt; \n  pivot_longer(cols = -c(country_name, country_code),\n               names_to = \"year\",\n               values_to = \"wdi_gdp_ppp\", \n               names_transform = as.integer)\n\nqog_plus &lt;- left_join(qog,\n                      gdp_long,\n                      by = c(\"ccodealp\" = \"country_code\",\n                             \"year\"))\n\n\n\nLoad data (notice the .csv format):\n\n\nemissions &lt;- read_csv(\"data/wdi_co2.csv\")\n\nRows: 266 Columns: 35\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): country_name, country_code\ndbl (31): 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, ...\nlgl  (2): 2021, 2022\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nPivot data to long, creating the “wdi_co2” variable:\n\n\nemissions_long &lt;- emissions |&gt; \n  pivot_longer(cols = -c(country_name, country_code),\n               names_to = \"year\",\n               values_to = \"wdi_co2\",  \n               names_transform = as.integer)\n\n\nMerge-in information to our existing qog_plus data frame:\n\n\nqog_plus2 &lt;- left_join(qog_plus,\n                       emissions_long,\n                      by = c(\"ccodealp\" = \"country_code\",\n                             \"year\"))\n\n\nCreate column for emissions per capita (here we do per 1,000 people).\nSummarize information to get mean values at the country level (remember that na.rm = T is always a conscious decision):\n\n\nqog_plus2 |&gt; \n  mutate(emissions_pc = 1000 * wdi_co2 / wdi_pop) |&gt; \n  summarize(emissions_pc_country = mean(emissions_pc, na.rm = T),\n            .by = cname)\n\n# A tibble: 35 × 2\n   cname               emissions_pc_country\n   &lt;chr&gt;                              &lt;dbl&gt;\n 1 Antigua and Barbuda                 4.60\n 2 Argentina                           3.71\n 3 Bahamas (the)                       6.17\n 4 Barbados                            4.53\n 5 Bolivia                             1.36\n 6 Brazil                              1.84\n 7 Belize                              1.74\n 8 Canada                             15.8 \n 9 Chile                               3.64\n10 Colombia                            1.54\n# ℹ 25 more rows\n\n\n\nExercise\nDraw a scatterplot with time in the x-axis and democracy scores in the y-axis. Your code:\n\n\nggplot(qog_plus2) + aes(year, vdem_polyarchy) + geom_point()\n\nWarning: Removed 248 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nExercise\nUsing your merged dataset from the previous section, plot the trajectories of C02 per capita emissions for the US and Haiti. Use adequate scales.\n\n\nggplot(qog_plus2 |&gt; filter(cname %in% c(\"Haiti\", \"United States\")), \n       aes(x = year, y = 1000 * wdi_co2 / wdi_pop)) +\n  geom_line() +\n  facet_wrap(~cname, scales = \"free_y\") +\n  labs(x = \"Year\", y = \"CO2 Emissions Per Capita\",\n       title = \"CO2 Emissions Per Capita in Haiti and the United States\",\n       caption = \"Source: World Development Indicators (World Bank, 2023) in QOG dataset.\")"
  },
  {
    "objectID": "solutions.html#functions",
    "href": "solutions.html#functions",
    "title": "Solutions to exercises",
    "section": "5. Functions",
    "text": "5. Functions\n\nExercise When graphed, vertical lines cannot touch functions at more than one point. Why? Which of the following represent functions?\n\n\n\nFunction ✅\nFunction ✅\nNOT a function 🚫\nFunction ✅\nFunction ✅\nNOT a function 🚫\nFunction ✅\nNOT a function 🚫\n\n\nExercise\nCreate a function that calculates the area of a circle from its diameter. So your_function(d = 6) should yield the same result as the example above. Your code:\n\n\n\nCode\n# setup: these steps were executed before the exercise\ncirc_area_r &lt;- function(r){\n    pi * r ^ 2\n}\ncirc_area_r(r = 3)\n\n\n[1] 28.27433\n\n\n\ncirc_area_d &lt;- function(d){\n    pi * (d/2) ^ 2\n}\ncirc_area_d(d = 6)\n\n[1] 28.27433\n\n\n\nExercise\nGraph the function \\(y = x^2 + 2x - 10\\), i.e., a quadratic function with \\(a=1\\), \\(b=2\\), and \\(c=-10\\). Next, try switching up these values and the xlim = argument. How do they each alter the function (and plot)?\n\n\n\nCode\n# setup: these steps were executed before the exercise\nlibrary(ggplot2)\n\n\n\nGraph \\(y = x^2 + 2x - 10\\).\n\n\nggplot() +\n  stat_function(fun = function(x){x^2 + 2*x - 10},\n                xlim = c(-5, 5)) \n\n\n\n\n\nSwitch up the values and the xlim = argument.\n\n\nggplot() +\n  stat_function(fun = function(x){-3*x^2 - 6*x + 9},\n                xlim = c(-10, 10)) \n\n\n\n\n\nExercise\nWe’ll briefly introduce Desmos, an online graphing calculator. Use Desmos to graph the following function \\(y = 1x^3 + 1x^2 + 1x + 1\\). What happens when you change the \\(a\\), \\(b\\), \\(c\\), and \\(d\\) parameters?\n\n\nGraph \\(y = 1x^3 + 1x^2 + 1x + 1\\).\n\n\nggplot() +\n  stat_function(fun = function(x){x^3 + x^2 + x + 1},\n                xlim = c(-10, 10)) \n\n\n\n\n\nSwitch up the values.\n\n\nggplot() +\n  stat_function(fun = function(x){-2*x^3 + 4*x^2 + 8*x + 16},\n                xlim = c(-10, 10)) \n\n\n\n\n\nExercise\nSolve the problems below, simplifying as much as you can. \\[log_{10}(1000)\\] \\[log_2(\\dfrac{8}{32})\\] \\[10^{log_{10}(300)}\\] \\[ln(1)\\] \\[ln(e^2)\\] \\[ln(5e)\\]\n\n\nlog10(1000)\n\n[1] 3\n\n\n\nlog2(8/32)\n\n[1] -2\n\n\n\n10^(log10(300))\n\n[1] 300\n\n\n\nlog(1)\n\n[1] 0\n\n\n\nlog(exp(2))\n\n[1] 2\n\n\n\nlog(5*exp(1))\n\n[1] 2.609438\n\n\n\nExercise\nCompute g(f(5)) using the definitions above. First do it manually, and then check your answer with R.\n\n\n\nCode\n# setup: these steps were executed before the exercise\nf &lt;- function(x){x ^ 2}\ng &lt;- function(x){x - 3}\n\n\n\\[f(5) = 5^2 = 25\\] \\[g(25) = 25 - 3 = 22\\]\n\ng(f(5)) # no pipeline approach\n\n[1] 22\n\nf(5) |&gt; g() # pipeline approach\n\n[1] 22"
  },
  {
    "objectID": "solutions.html#calculus",
    "href": "solutions.html#calculus",
    "title": "Solutions to exercises",
    "section": "6. Calculus",
    "text": "6. Calculus\n\nExercise\n\nUse the slope formula to calculate the rate of change between 5 and 6.\nUse the slope formula to calculate the rate of change between 5 and 5.5.\nUse the slope formula to calculate the rate of change between 5 and 5.1.\n\n\n\nExercise\nUse the differentiation rules we have covered so far to calculate the derivatives of \\(y\\) with respect to \\(x\\) of the following functions:\n\n\\(y = 2x^2 + 10\\)\n\\(y = 5x^4 - \\frac{2}{3}x^3\\)\n\\(y = 9 \\sqrt x\\)\n\\(y = \\frac{4}{x^2}\\)\n\\(y = ax^3 + b\\), where \\(a\\) and \\(b\\) are constants.\n\\(y = \\frac{2w}{5}\\)\n\n\n\nExercise\nCompute the following:\n\n\\(\\frac{d}{dx}(10e^x)\\)\n\\(\\frac{d}{dx}(ln(x) - \\frac{e^2}{3})\\)\n\n\n\nExercise\nUse the differentiation rules we have covered so far to calculate the derivatives of \\(y\\) with respect to \\(x\\) of the following functions:\n\n\\(x^3 \\cdot x\\)\n\\(e^x \\cdot x^2\\)\n\\((3x^4-8)^2\\)\n\n\n\nExercise\nTake the partial derivative with respect to \\(x\\) and with respect to \\(z\\) of the following functions. What would the notation for each look like?\n\n\\(y = 3xz - x\\)\n\\(x^3+z^3+x^4z^4\\)\n\\(e^{xz}\\)\n\n\n\nExercise\nIdentify the global extrema of the function \\(\\displaystyle \\frac{x^3}{3} - \\frac{3}{2}x^2 -10x\\) in the interval \\([-6, 6]\\).\n\n\nExercise\nSolve the following indefinite integrals:\n\n\\(\\int x^2 \\text{ } dx\\)\n\\(\\int 3x^2\\text{ } dx\\)\n\\(\\int x\\text{ } dx\\)\n\\(\\int (3x^2 + 2x - 7\\text{ })dx\\)\n\\(\\int \\dfrac{2}{x}\\text{ }dx\\)\n\nAnd solve the following definite integrals:\n\n\\(\\displaystyle\\int_{1}^{7} x^2 \\text{ } dx\\)\n\\(\\displaystyle\\int_{1}^{10} 3x^2 \\text{ } dx\\)\n\\(\\displaystyle\\int_7^7 x\\text{ } dx\\)\n\\(\\displaystyle\\int_{1}^{5} 3x^2 + 2x - 7\\text{ }dx\\)\n\\(\\int_{1}^{e} \\dfrac{2}{x}\\text{ }dx\\)"
  },
  {
    "objectID": "solutions.html#probability-statistics-and-simulations",
    "href": "solutions.html#probability-statistics-and-simulations",
    "title": "Solutions to exercises",
    "section": "7. Probability, statistics, and simulations",
    "text": "7. Probability, statistics, and simulations\n\nExercise\nCompute the probability of seeing between 1 and 10 voters of the candidate in a sample of 100 people.\n\n\nExercise\nEvaluate the CDF of \\(Y \\sim U(-2, 2)\\) at point \\(y = 1\\). Use the formula and punif().\n\n\nExercise\nWhat is the probability of obtaining a value above 1.96 or below -1.96 in a standard normal probability distribution? Hint: use the pnorm() function.\n\n\nExercise\nCompute and plot my_rnorm, a vector with one million draws from a Normal distribution \\(Z\\) with mean equal to zero and standard deviation equal to one (\\(Z\\sim N(0,1)\\)). You can recycle code from what we did for the uniform distribution!"
  },
  {
    "objectID": "solutions.html#text-analysis",
    "href": "solutions.html#text-analysis",
    "title": "Solutions to exercises",
    "section": "8. Text analysis",
    "text": "8. Text analysis\n\nExercise\nWhat score (out of 10) would you give Barbie or Oppenheimer? Write your score in one sentence (e.g., I would give Barbie seven of ten stars.) If you have not seen either, write a sentence about which you would like to see more.\nStore that text as a string (string3) and combine it with our existing cat_string to produce a new concatenated string called cat_string2. Finally, count the total of characters within cat_string2. Your code:\n\n\n\nCode\n# setup: these steps were executed before the exercise\nlibrary(stringr)\nmy_string &lt;- \"I know people who have seen the Barbie movie 2, 3, even 4 times!\"\nmy_string2 &lt;- \"I wonder if they have seen Oppenheimer, too.\"\ncat_string &lt;- str_c(my_string, my_string2, sep = \" \")\n\n\n\nExercise\nLook up the lyrics to your favorite song at the moment (no guilty pleasures here!). Then, follow the process described above to count the words: store the text as a string, convert to a tibble, tokenize, and count.\nWhen you are done counting, create a visualization for the chorus using the ggplot code above. Your code:\n\n\nStore the text as a string.\n\n\nConvert to a tibble.\n\n\nTokenize.\n\n\nCount.\n\n\nVisualize.\n\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. First edition. Beijing ; Boston: O’Reilly.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Arel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018.\n“Countrycode: An r Package to Convert Country Names and Country\nCodes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of\nAgnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul,\nand Jeffrey Mark Siskind. 2017. “Automatic Differentiation in\nMachine Learning: A Survey.” The Journal of Machine Learning\nResearch 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I.\nLindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022.\n“V-Dem Codebook V12.” Varieties of Democracy (V-Dem)\nProject. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia\nAlvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The\nQuality of Government Basic Dataset, Version Jan23.” University\nof Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government\ndoi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress\nIn The Age Of\nTrump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social\nScience: An Introduction in Tidyverse. Princeton; Oxford: Princeton\nUniversity Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for\nPolitical and Social Research. Princeton, NJ: Princeton University\nPres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.”\nMIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact\nMatching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a\nToolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with\nR: A Tidy Approach. First edition. Beijing ; Boston:\nO’Reilly.\n\n\nSmith, Danny. 2020. Survey Research Datasets and\nR. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service.\n2019. “Department of Agriculture Agricultural Research\nService.” https://fdc.nal.usda.gov/.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of\nStatistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023.\nGgplot2: Elegant Graphics for\nData Analysis. 3rd ed. https://ggplot2-book.org/."
  }
]