[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methods Camp",
    "section": "",
    "text": "Class schedule\nOn class days, we will have a lunch break from 12:00-1:00 PM. We’ll also take short breaks periodically during the morning and afternoon sessions as needed.",
    "crumbs": [
      "Class schedule"
    ]
  },
  {
    "objectID": "index.html#class-schedule",
    "href": "index.html#class-schedule",
    "title": "Methods Camp",
    "section": "",
    "text": "Date\nTime\nLocation\n\n\n\n\nFri, Aug. 16\n9:00 AM - 4:00 PM\nRLP 2.606\n\n\nSat, Aug. 17\nNo class\n-\n\n\nSun, Aug. 18\nNo class\n-\n\n\nMon, Aug. 19\n9:00 AM - 4:00 PM\nBAT 5.108\n\n\nTue, Aug. 20\n9:00 AM - 4:00 PM\nRLP 2.606\n\n\nWed, Aug. 21\n9:00 AM - 4:00 PM\nBAT 5.108\n\n\nThu, Aug. 22\n9:00 AM - 4:00 PM\nRLP 2.606",
    "crumbs": [
      "Class schedule"
    ]
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "Methods Camp",
    "section": "Description",
    "text": "Description\nWelcome to Introduction to Methods for Political Science, aka “Methods Camp”! Methods Camp is designed to give everyone a chance to brush up on some skills in preparation for the introductory Statistics and Formal Theory courses. The other goal of Methods Camp is to allow you to get to know your cohort. We hope that matrix algebra and the chain rule will still prove to be good bonding exercises!\nAs you can see from the above schedule, we’ll be meeting on Friday, August 16th as well as from Monday, August 19th through Thursday, August 22nd. Classes at UT begin the start of the following week on Monday, August 26th. Below is a tentative schedule outlining what will be covered in the class, although we may rearrange things if we find we’re going too slowly or too quickly through the material.",
    "crumbs": [
      "Class schedule"
    ]
  },
  {
    "objectID": "index.html#course-outline",
    "href": "index.html#course-outline",
    "title": "Methods Camp",
    "section": "Course outline",
    "text": "Course outline\n1 Friday morning: Intro to R\n\nIntroductions\nR and RStudio: basics\nObjects (vectors, matrices, data frames, etc.)\nBasic functions (mean(), length(), etc.)\nPackages: installation and loading (including the tidyverse)\n\n2 Friday afternoon: Tidy data analysis I\n\nTidy data\nData wrangling with dplyr\nData visualization basics with ggplot2\n\n3 Monday morning: Functions\n\nDefinitions\nFunctions in R\nCommon types of functions\nLogarithms and exponents\nComposite functions\n\n4 Monday afternoon: Calculus\n\nDerivatives\nOptimization\nIntegrals\n\n5 Tuesday morning: Matrices\n\nMatrices\nSystems of linear equations\nMatrix operations (multiplication, transpose, inverse, determinant)\nSolving systems of linear equations in matrix form (and why that’s cool)\nIntroduction to OLS\n\n6 Tuesday afternoon: Tidy data analysis II\n\nLoading data in different formats (.csv, R, Excel, Stata, SPSS)\nRecoding values (if_else(), case_when())\nHandling missing values\nPivoting data\nMerging data\nPlotting extensions (trend graphs, facets, customization)\n\n7 Wednesday morning: Probability\n\nProbability: basic concepts\nRandom variables, probability distributions, and their properties\nCommon probability distributions\n\n8 Wednesday afternoon: Statistics and simulations\n\nStatistics: basic concepts\nRandom sampling and loops in R\nSimulation example: bootstrapping\n\n9 Thursday morning: Text analysis\n\nString manipulation with stringr\nSimple text analysis and visualization with tidytext\n\n10 Thursday morning: Coding with AI\n\nVisualization tools\n⁠Statistical testing and simulation\n⁠Text analysis examples\n\n11 Thursday afternoon: Wrap-up\n\nProject management fundamentals\nSelf-study resources and materials\nOther software (Overleaf, Zotero, etc.)\nMethods resources at UT",
    "crumbs": [
      "Class schedule"
    ]
  },
  {
    "objectID": "index.html#contact-info",
    "href": "index.html#contact-info",
    "title": "Methods Camp",
    "section": "Contact info",
    "text": "Contact info\nIf you have any questions during or outside of methods camp, you can contact us via email. Or if you are curious about our research, you can also check out our respective websites and Twitter accounts (or should we say X…):\n\nAndrés Cruz: andres.cruz at utexas dot edu [Website] [Twitter]\nMeiying Xu: xu.meiying at utexas dot edu [Website] [Twitter]",
    "crumbs": [
      "Class schedule"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Methods Camp",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe thank previous Methods Camp instructors for their accumulated experience and materials, which we have based ours upon. UT Gov Prof. Max Goplerud gave us amazing feedback for this iteration of Methods Camp (2024). All errors remain our own (and will hopefully be fixed with your help!).",
    "crumbs": [
      "Class schedule"
    ]
  },
  {
    "objectID": "index.html#materials-from-previous-editions",
    "href": "index.html#materials-from-previous-editions",
    "title": "Methods Camp",
    "section": "Materials from previous editions",
    "text": "Materials from previous editions\n\n2024: co-taught by Andrés Cruz and Meiying Xu.\n2023: co-taught by Andrés Cruz and Matt Martin.\n\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWhittinghill, Dexter C, and Robert V Hogg. 2001. “A Little Uniform Density with Big Instructional Potential.” Journal of Statistics Education 9 (2).\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/.",
    "crumbs": [
      "Class schedule"
    ]
  },
  {
    "objectID": "00_setup.html",
    "href": "00_setup.html",
    "title": "Setup",
    "section": "",
    "text": "Installing R and RStudio\nR is a programming language optimized for statistics and data analysis. Most people use R from RStudio, a graphical user interface (GUI) that includes a file pane, a graphics pane, and other goodies. Both R and RStudio are open source, i.e., free as in beer and free as in freedom!\nYour first steps should be to install R and RStudio, in that order (if you have installed these programs before, make sure that your versions are up-to-date—if they are not, simply follow the instructions below to re-install them):\nAfter these two steps, you can open RStudio in your system, as you would with any program. You should see something like this:\nThat’s it for the installation! We also strongly recommend that you change a couple of RStudio’s default settings.1 You can change settings by clicking on Tools &gt; Global Options in the menubar. Here are our recommendations:",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "00_setup.html#installing-r-and-rstudio",
    "href": "00_setup.html#installing-r-and-rstudio",
    "title": "Setup",
    "section": "",
    "text": "Download and install R from the official website, CRAN. Click on “Download R for &lt;Windows/MacOS&gt;” and follow the instructions. If you have a Mac, make sure to select the version appropriate for your system (Apple Silicon for newer M1/M2/M3 Macs and Intel for older Macs).\nDownload and install RStudio from the official website. Scroll down and select the installer for your operating system (most likely the .exe for Windows 10/11 or the .dmg for macOS 12+).\n\n\n\n\n\n\n\n\nFigure 1: How RStudio looks after a clean installation.\n\n\n\n\n\n\n\n\n\nNote for Windows users\n\n\n\nWhile the installation steps above should be enough for most tasks, we also suggest that Windows users install RTools (click on the “Rtools44 installer” link at the middle of the package to get the .exe file). Rtools is needed on Windows to install some advanced packages, so it is a good idea to have it on your system.\n\n\n\n\nGeneral &gt; Uncheck \"Restore .RData into workspace at startup\"\nGeneral &gt; Save workspace to .RData on Exit &gt; Select \"Never\"\nCode &gt; Check \"Use native pipe operator\"\nTools &gt; Global Options &gt; Appearance to change to a dark theme, if you want! Pros: better for night sessions, hacker vibes…",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "00_setup.html#setting-up-for-methods-camp",
    "href": "00_setup.html#setting-up-for-methods-camp",
    "title": "Setup",
    "section": "Setting up for Methods Camp",
    "text": "Setting up for Methods Camp\nAll materials for Methods Camp are both on this website and available as RStudio projects for you to execute locally. An RStudio project is simply a folder where one keeps scripts, datasets, and other files needed for a data analysis project.\nBelow are RStudio projects for you to download, available as .zip compressed files. On MacOS, the file will be uncompressed automatically. On Windows, you should do Right click &gt; Extract all.\n\nDownload Part 1 of the class materials.\nDownload Part 2 of the class materials.\nDownload Part 3 of the class materials.\nDownload Part 4 of the class materials.\n\n\n\n\n\n\n\nWarning\n\n\n\nMake sure to properly unzip the materials. Double-clicking the .zip file on most Windows systems will not unzip the folder—you must do Right click &gt; Extract all.\n\n\nYou should now have a folder called methodscamp_part1/ on your computer. Navigate to the methodscamp_part1.Rproj file within it and open it. RStudio should open the project right away. You should see methodscamp_part1 on the top-right of RStudio—this indicates that you are working in our RStudio project.\n\n\n\n\n\n\nFigure 2: How the bottom-right corner of RStudio looks after opening our project.\n\n\n\nThat’s all for setup! We can now start coding. After opening our RStudio project, we’ll begin by opening the 01_r_intro.qmd file from the “Files” panel, in the bottom-right portion of RStudio. This is a Quarto document,2 which contains both code and explanations (you can also read the materials in the next chapter of this website).\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWhittinghill, Dexter C, and Robert V Hogg. 2001. “A Little Uniform Density with Big Instructional Potential.” Journal of Statistics Education 9 (2).\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/.",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "00_setup.html#footnotes",
    "href": "00_setup.html#footnotes",
    "title": "Setup",
    "section": "",
    "text": "The idea behind these settings (or at least the first two) is to force R to start from scratch with each new session. No lingering objects from previous coding sessions avoids misunderstandings and helps with reproducibility!↩︎\nPerhaps you have used R Markdown before. Quarto is the next iteration of R Markdown, and is both more flexible and more powerful!↩︎",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "01_r_intro.html",
    "href": "01_r_intro.html",
    "title": "1  Intro to R",
    "section": "",
    "text": "1.1 Objects\nIn Quarto documents like this one, we can write comments by just using plain text. In contrast, code needs to be within code blocks, like the one below. To execute a code block, you can click on the little “Play” button or press Cmd/Ctrl + Shift + Enter when your keyboard is hovering the code block.\nThat was our first R command, a simple math operation. Of course, we can also do more complex arithmetic:\nIn order to create a code block, you can press Cmd/Ctrl + Alt + i or click on the little green “+C” icon on top of the script.\nA huge part of R is working with objects. Let’s see how they work:\nmy_object &lt;- 10 # opt/alt + minus sign will make the arrow\nmy_object # to print the value of an object, just call its name\n\n[1] 10\nWe can now use this object in our operations:\n2 ^ my_object\n\n[1] 1024\nOr even create another object out of it:\nmy_object2 &lt;- my_object * 2\nmy_object2\n\n[1] 20\nYou can delete objects with the rm() function (for “remove”):\nrm(my_object2)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to R</span>"
    ]
  },
  {
    "objectID": "01_r_intro.html#vectors-and-functions",
    "href": "01_r_intro.html#vectors-and-functions",
    "title": "1  Intro to R",
    "section": "1.2 Vectors and functions",
    "text": "1.2 Vectors and functions\nObjects can be of different types. One of the most useful ones is the vector, which holds a series of values. To create one manually, we can use the c() function (for “combine”):\n\nmy_vector &lt;- c(6, -11, my_object, 0, 20)\n\n\nmy_vector\n\n[1]   6 -11  10   0  20\n\n\nOne can also define vectors by sequences:\n\n3:10\n\n[1]  3  4  5  6  7  8  9 10\n\n\nWe can use square brackets to retrieve parts of vectors:\n\nmy_vector[4] # fourth element\n\n[1] 0\n\n\n\nmy_vector[1:2] # first two elements\n\n[1]   6 -11\n\n\nLet’s check out some basic functions we can use with numbers and numeric vectors:\n\nsqrt(my_object) # squared root\n\n[1] 3.162278\n\n\n\nlog(my_object) # logarithm (natural by default)\n\n[1] 2.302585\n\n\n\nabs(-5) # absolute value\n\n[1] 5\n\n\n\nmean(my_vector)\n\n[1] 5\n\n\n\nmedian(my_vector)\n\n[1] 6\n\n\n\nsd(my_vector) # standard deviation\n\n[1] 11.53256\n\n\n\nsum(my_vector)\n\n[1] 25\n\n\n\nmin(my_vector) # minimum value\n\n[1] -11\n\n\n\nmax(my_vector) # maximum value\n\n[1] 20\n\n\n\nlength(my_vector) # length (number of elements)\n\n[1] 5\n\n\nNotice that if we wanted to save any of these results for later, we would need to assign them:\n\nmy_mean &lt;- mean(my_vector)\n\n\nmy_mean\n\n[1] 5\n\n\nThese functions are quite simple: they take one object and do one operation. A lot of functions are a bit more complex—they take multiple objects or take options. For example, see the sort() function, which by default sorts a vector increasingly:\n\nsort(my_vector)\n\n[1] -11   0   6  10  20\n\n\nIf we instead want to sort our vector decreasingly, we can use the decreasing = TRUE argument (T also works as an abbreviation for TRUE).\n\nsort(my_vector, decreasing = TRUE)\n\n[1]  20  10   6   0 -11\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you use the argument values in order, you can avoid writing the argument names (see below). This is sometimes useful, but can also lead to confusing code—use it with caution.\n\nsort(my_vector, T)\n\n[1]  20  10   6   0 -11\n\n\n\n\nA useful function to create vectors in sequence is seq(). Notice its arguments:\n\nseq(from = 30, to = 100, by = 5)\n\n [1]  30  35  40  45  50  55  60  65  70  75  80  85  90  95 100\n\n\nTo check the arguments of a function, you can examine its help file: look the function up on the “Help” panel on RStudio or use a command like the following: ?sort.\n\n\n\n\n\n\nExercise\n\n\n\nExamine the help file of the log() function. How can we compute the the base-10 logarithm of my_object? Your code:\n\n\nOther than numeric vectors, character vectors are also useful:\n\nmy_character_vector &lt;- c(\"Apple\", \"Orange\", \"Watermelon\", \"Banana\")\n\n\nmy_character_vector[3]\n\n[1] \"Watermelon\"\n\n\n\nnchar(my_character_vector) # count number of characters\n\n[1]  5  6 10  6",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to R</span>"
    ]
  },
  {
    "objectID": "01_r_intro.html#data-frames-and-lists",
    "href": "01_r_intro.html#data-frames-and-lists",
    "title": "1  Intro to R",
    "section": "1.3 Data frames and lists",
    "text": "1.3 Data frames and lists\nAnother useful object type is the data frame. Data frames can store multiple vectors in a tabular format. We can manually create one with the data.frame() function:\n\nmy_data_frame &lt;- data.frame(fruit = my_character_vector,\n                            calories_per_100g = c(52, 47, 30, 89),\n                            water_per_100g = c(85.6, 86.8, 91.4, 74.9))\n\n\nmy_data_frame\n\n       fruit calories_per_100g water_per_100g\n1      Apple                52           85.6\n2     Orange                47           86.8\n3 Watermelon                30           91.4\n4     Banana                89           74.9\n\n\nNow we have a little 4x3 data frame of fruits with their calorie counts and water composition. We gathered the nutritional information from the USDA (2019).\nWe can use the data_frame$column construct to access the vectors within the data frame:\n\nmean(my_data_frame$calories_per_100g)\n\n[1] 54.5\n\n\n\n\n\n\n\n\nExercise\n\n\n\nObtain the maximum value of water content per 100g in the data. Your code:\n\n\nSome useful commands to learn attributes of our data frame:\n\ndim(my_data_frame)\n\n[1] 4 3\n\n\n\nnrow(my_data_frame)\n\n[1] 4\n\n\n\nnames(my_data_frame) # column names\n\n[1] \"fruit\"             \"calories_per_100g\" \"water_per_100g\"   \n\n\nWe will learn much more about data frames in our next module on data analysis.\nAfter talking about vectors and data frames, the last object type that we will cover is the list. Lists are super flexible objects that can contain just about anything:\n\nmy_list &lt;- list(my_object, my_vector, my_data_frame)\n\n\nmy_list\n\n[[1]]\n[1] 10\n\n[[2]]\n[1]   6 -11  10   0  20\n\n[[3]]\n       fruit calories_per_100g water_per_100g\n1      Apple                52           85.6\n2     Orange                47           86.8\n3 Watermelon                30           91.4\n4     Banana                89           74.9\n\n\nTo retrieve the elements of a list, we need to use double square brackets:\n\nmy_list[[1]]\n\n[1] 10\n\n\nLists are sometimes useful due to their flexibility, but are much less common in routine data analysis compared to vectors or data frames.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to R</span>"
    ]
  },
  {
    "objectID": "01_r_intro.html#packages",
    "href": "01_r_intro.html#packages",
    "title": "1  Intro to R",
    "section": "1.4 Packages",
    "text": "1.4 Packages\nThe R community has developed thousands of packages, which are specialized collections of functions, datasets, and other resources. To install one, you should use the install.packages() command. Below we will install the tidyverse package, a suite for data analysis that we will use in the next modules. You just need to install packages once, and then they will be available system-wide.\n\ninstall.packages(\"tidyverse\") # this can take a couple of minutes\n\nIf you want to use an installed package in your script, you must load it with the library() function. Some packages, as shown below, will print descriptive messages once loaded.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember that install.packages(\"package\") needs to be executed just once, while library(package) needs to be in each script in which you plan to use the package. In general, never include install.packages(\"package\") as part of your scripts or Quarto documents!\n\n\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWhittinghill, Dexter C, and Robert V Hogg. 2001. “A Little Uniform Density with Big Instructional Potential.” Journal of Statistics Education 9 (2).\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to R</span>"
    ]
  },
  {
    "objectID": "02_tidy_data1.html",
    "href": "02_tidy_data1.html",
    "title": "2  Tidy data analysis I",
    "section": "",
    "text": "2.1 Loading data\nThe tidyverse is a suite of packages that streamline data analysis in R. After installing the tidyverse with install.packages(\"tidyverse\") (see the previous module), you can load it with:\nThroughout this module, we will use tidyverse functions to load, wrangle, and visualize real data.\nThroughout this module we will work with a dataset of senators during the Trump presidency, which was adapted from FiveThirtyEight (2021).\nWe have stored the dataset in .csv format under the data/ subfolder. Loading it into R is simple (notice that we need to assign it to an object):\ntrump_scores &lt;- read_csv(\"data/trump_scores_538.csv\")\n\nRows: 122 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): bioguide, last_name, state, party\ndbl (4): num_votes, agree, agree_pred, margin_trump\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ntrump_scores\n\n# A tibble: 122 × 8\n   bioguide last_name  state party num_votes agree agree_pred margin_trump\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 A000360  Alexander  TN    R           118 0.890      0.856       26.0  \n 2 B000575  Blunt      MO    R           128 0.906      0.787       18.6  \n 3 B000944  Brown      OH    D           128 0.258      0.642        8.13 \n 4 B001135  Burr       NC    R           121 0.893      0.560        3.66 \n 5 B001230  Baldwin    WI    D           128 0.227      0.510        0.764\n 6 B001236  Boozman    AR    R           129 0.915      0.851       26.9  \n 7 B001243  Blackburn  TN    R           131 0.885      0.889       26.0  \n 8 B001261  Barrasso   WY    R           129 0.891      0.895       46.3  \n 9 B001267  Bennet     CO    D           121 0.273      0.417       -4.91 \n10 B001277  Blumenthal CT    D           128 0.203      0.294      -13.6  \n# ℹ 112 more rows\nLet’s review the dataset’s columns:\nWe can inspect our data by using the interface above. An alternative is to run the command View(trump_scores) or click on the object in RStudio’s environment panel (in the top-right section).\nDo you have any questions about the data?\nBy the way, the tidyverse works amazingly with tidy data. If you can get your data to this format (and we will see ways to do this), your life will be much easier:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tidy data analysis I</span>"
    ]
  },
  {
    "objectID": "02_tidy_data1.html#loading-data",
    "href": "02_tidy_data1.html#loading-data",
    "title": "2  Tidy data analysis I",
    "section": "",
    "text": "bioguide: A unique ID for each politician, from the Congress Bioguide.\nlast_name\nstate\nparty\nnum_votes: Number of votes for which data was available.\nagree: Proportion (0-1) of votes in which the senator voted in agreement with Trump.\nagree_pred: Predicted proportion of vote agreement, calculated using Trump’s margin (see next variable).\nmargin_trump: Margin of victory (percentage points) of Trump in the senator’s state.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: Illustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tidy data analysis I</span>"
    ]
  },
  {
    "objectID": "02_tidy_data1.html#wrangling-data-with-dplyr",
    "href": "02_tidy_data1.html#wrangling-data-with-dplyr",
    "title": "2  Tidy data analysis I",
    "section": "2.2 Wrangling data with dplyr",
    "text": "2.2 Wrangling data with dplyr\nWe often need to modify data to conduct our analyses, e.g., creating columns, filtering rows, etc. In the tidyverse, these operations are conducted with multiple verbs, which we will review now.\n\n2.2.1 Selecting columns\nWe can select specific columns in our dataset with the select() function. All dplyr wrangling verbs take a data frame as their first argument—in this case, the columns we want to select are the other arguments.\n\nselect(trump_scores, last_name, party)\n\n# A tibble: 122 × 2\n   last_name  party\n   &lt;chr&gt;      &lt;chr&gt;\n 1 Alexander  R    \n 2 Blunt      R    \n 3 Brown      D    \n 4 Burr       R    \n 5 Baldwin    D    \n 6 Boozman    R    \n 7 Blackburn  R    \n 8 Barrasso   R    \n 9 Bennet     D    \n10 Blumenthal D    \n# ℹ 112 more rows\n\n\nThis is a good moment to talk about “pipes.” Notice how the code below produces the same output as the one above, but with a slightly different syntax. Pipes (|&gt;) “kick” the object on the left of the pipe to the first argument of the function on the right. One can read pipes as “then,” so the code below can be read as “take trump_scores, then select the columns last_name and party.” Pipes are very useful to chain multiple operations, as we will see in a moment.\n\ntrump_scores |&gt; \n  select(last_name, party)\n\n# A tibble: 122 × 2\n   last_name  party\n   &lt;chr&gt;      &lt;chr&gt;\n 1 Alexander  R    \n 2 Blunt      R    \n 3 Brown      D    \n 4 Burr       R    \n 5 Baldwin    D    \n 6 Boozman    R    \n 7 Blackburn  R    \n 8 Barrasso   R    \n 9 Bennet     D    \n10 Blumenthal D    \n# ℹ 112 more rows\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can insert a pipe with the Cmd/Ctrl + Shift + M shortcut. If you have not changed the default RStudio settings, an “old” pipe (%&gt;%) might appear. While most of the functionality is the same, the |&gt; “new” pipes are more readable and don’t need any extra packages (to use %&gt;% you need the tidyverse or one of its packages). You can change this RStudio option in Tools &gt; Global Options &gt; Code &gt; Use native pipe operator. Make sure to check the other suggested settings in our Setup module!\n\n\nGoing back to selecting columns, you can select ranges:\n\ntrump_scores |&gt; \n  select(bioguide:party)\n\n# A tibble: 122 × 4\n   bioguide last_name  state party\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;\n 1 A000360  Alexander  TN    R    \n 2 B000575  Blunt      MO    R    \n 3 B000944  Brown      OH    D    \n 4 B001135  Burr       NC    R    \n 5 B001230  Baldwin    WI    D    \n 6 B001236  Boozman    AR    R    \n 7 B001243  Blackburn  TN    R    \n 8 B001261  Barrasso   WY    R    \n 9 B001267  Bennet     CO    D    \n10 B001277  Blumenthal CT    D    \n# ℹ 112 more rows\n\n\nYou can also deselect columns using a minus sign:\n\ntrump_scores |&gt; \n  select(-last_name)\n\n# A tibble: 122 × 7\n   bioguide state party num_votes agree agree_pred margin_trump\n   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 A000360  TN    R           118 0.890      0.856       26.0  \n 2 B000575  MO    R           128 0.906      0.787       18.6  \n 3 B000944  OH    D           128 0.258      0.642        8.13 \n 4 B001135  NC    R           121 0.893      0.560        3.66 \n 5 B001230  WI    D           128 0.227      0.510        0.764\n 6 B001236  AR    R           129 0.915      0.851       26.9  \n 7 B001243  TN    R           131 0.885      0.889       26.0  \n 8 B001261  WY    R           129 0.891      0.895       46.3  \n 9 B001267  CO    D           121 0.273      0.417       -4.91 \n10 B001277  CT    D           128 0.203      0.294      -13.6  \n# ℹ 112 more rows\n\n\nAnd use a few helper functions, like matches():\n\ntrump_scores |&gt; \n  select(last_name, matches(\"agree\"))\n\n# A tibble: 122 × 3\n   last_name  agree agree_pred\n   &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 Alexander  0.890      0.856\n 2 Blunt      0.906      0.787\n 3 Brown      0.258      0.642\n 4 Burr       0.893      0.560\n 5 Baldwin    0.227      0.510\n 6 Boozman    0.915      0.851\n 7 Blackburn  0.885      0.889\n 8 Barrasso   0.891      0.895\n 9 Bennet     0.273      0.417\n10 Blumenthal 0.203      0.294\n# ℹ 112 more rows\n\n\nOr everything(), which we usually use to reorder columns:\n\ntrump_scores |&gt; \n  select(last_name, everything())\n\n# A tibble: 122 × 8\n   last_name  bioguide state party num_votes agree agree_pred margin_trump\n   &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 Alexander  A000360  TN    R           118 0.890      0.856       26.0  \n 2 Blunt      B000575  MO    R           128 0.906      0.787       18.6  \n 3 Brown      B000944  OH    D           128 0.258      0.642        8.13 \n 4 Burr       B001135  NC    R           121 0.893      0.560        3.66 \n 5 Baldwin    B001230  WI    D           128 0.227      0.510        0.764\n 6 Boozman    B001236  AR    R           129 0.915      0.851       26.9  \n 7 Blackburn  B001243  TN    R           131 0.885      0.889       26.0  \n 8 Barrasso   B001261  WY    R           129 0.891      0.895       46.3  \n 9 Bennet     B001267  CO    D           121 0.273      0.417       -4.91 \n10 Blumenthal B001277  CT    D           128 0.203      0.294      -13.6  \n# ℹ 112 more rows\n\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that all these commands have not edited our existent objects—they have just printed the requested outputs to the screen. In order to modify objects, you need to use the assignment operator (&lt;-). For example:\n\ntrump_scores_reduced &lt;- trump_scores |&gt; \n  select(last_name, matches(\"agree\"))\n\n\ntrump_scores_reduced\n\n# A tibble: 122 × 3\n   last_name  agree agree_pred\n   &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 Alexander  0.890      0.856\n 2 Blunt      0.906      0.787\n 3 Brown      0.258      0.642\n 4 Burr       0.893      0.560\n 5 Baldwin    0.227      0.510\n 6 Boozman    0.915      0.851\n 7 Blackburn  0.885      0.889\n 8 Barrasso   0.891      0.895\n 9 Bennet     0.273      0.417\n10 Blumenthal 0.203      0.294\n# ℹ 112 more rows\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSelect the variables last_name, party, num_votes, and agree from the data frame. Your code:\n\n\n\n\n2.2.2 Renaming columns\nWe can use the rename() function to rename columns, with the syntax new_name = old_name. For example:\n\ntrump_scores |&gt; \n  rename(prop_agree = agree, prop_agree_pred = agree_pred)\n\n# A tibble: 122 × 8\n   bioguide last_name  state party num_votes prop_agree prop_agree_pred\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;\n 1 A000360  Alexander  TN    R           118      0.890           0.856\n 2 B000575  Blunt      MO    R           128      0.906           0.787\n 3 B000944  Brown      OH    D           128      0.258           0.642\n 4 B001135  Burr       NC    R           121      0.893           0.560\n 5 B001230  Baldwin    WI    D           128      0.227           0.510\n 6 B001236  Boozman    AR    R           129      0.915           0.851\n 7 B001243  Blackburn  TN    R           131      0.885           0.889\n 8 B001261  Barrasso   WY    R           129      0.891           0.895\n 9 B001267  Bennet     CO    D           121      0.273           0.417\n10 B001277  Blumenthal CT    D           128      0.203           0.294\n# ℹ 112 more rows\n# ℹ 1 more variable: margin_trump &lt;dbl&gt;\n\n\nThis is a good occasion to show how pipes allow us to chain operations. How do we read the following code out loud? (Remember that pipes are read as “then”).\n\ntrump_scores |&gt; \n  select(last_name, matches(\"agree\")) |&gt; \n  rename(prop_agree = agree, prop_agree_pred = agree_pred)\n\n# A tibble: 122 × 3\n   last_name  prop_agree prop_agree_pred\n   &lt;chr&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n 1 Alexander       0.890           0.856\n 2 Blunt           0.906           0.787\n 3 Brown           0.258           0.642\n 4 Burr            0.893           0.560\n 5 Baldwin         0.227           0.510\n 6 Boozman         0.915           0.851\n 7 Blackburn       0.885           0.889\n 8 Barrasso        0.891           0.895\n 9 Bennet          0.273           0.417\n10 Blumenthal      0.203           0.294\n# ℹ 112 more rows\n\n\n\n\n2.2.3 Creating columns\nIt is common to want to create columns, based on existing ones. We can use mutate() to do so. For example, we could want our main variables of interest in terms of percentages instead of proportions:\n\ntrump_scores |&gt; \n  select(last_name, agree, agree_pred) |&gt; # select just for clarity\n  mutate(pct_agree = 100 * agree,\n         pct_agree_pred = 100 * agree_pred)\n\n# A tibble: 122 × 5\n   last_name  agree agree_pred pct_agree pct_agree_pred\n   &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;\n 1 Alexander  0.890      0.856      89.0           85.6\n 2 Blunt      0.906      0.787      90.6           78.7\n 3 Brown      0.258      0.642      25.8           64.2\n 4 Burr       0.893      0.560      89.3           56.0\n 5 Baldwin    0.227      0.510      22.7           51.0\n 6 Boozman    0.915      0.851      91.5           85.1\n 7 Blackburn  0.885      0.889      88.5           88.9\n 8 Barrasso   0.891      0.895      89.1           89.5\n 9 Bennet     0.273      0.417      27.3           41.7\n10 Blumenthal 0.203      0.294      20.3           29.4\n# ℹ 112 more rows\n\n\nWe can also use multiple columns for creating a new one. For example, let’s retrieve the total number of votes in which the senator agreed with Trump:\n\ntrump_scores |&gt; \n  select(last_name, num_votes, agree) |&gt; # select just for clarity\n  mutate(num_votes_agree = num_votes * agree)\n\n# A tibble: 122 × 4\n   last_name  num_votes agree num_votes_agree\n   &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;\n 1 Alexander        118 0.890           105  \n 2 Blunt            128 0.906           116  \n 3 Brown            128 0.258            33  \n 4 Burr             121 0.893           108  \n 5 Baldwin          128 0.227            29  \n 6 Boozman          129 0.915           118  \n 7 Blackburn        131 0.885           116  \n 8 Barrasso         129 0.891           115  \n 9 Bennet           121 0.273            33.0\n10 Blumenthal       128 0.203            26  \n# ℹ 112 more rows\n\n\n\n\n2.2.4 Filtering rows\nAnother common operation is to filter rows based on logical conditions. We can do so with the filter() function. For example, we can filter to only get Democrats:\n\ntrump_scores |&gt; \n  filter(party == \"D\")\n\n# A tibble: 55 × 8\n   bioguide last_name  state party num_votes agree agree_pred margin_trump\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 B000944  Brown      OH    D           128 0.258      0.642        8.13 \n 2 B001230  Baldwin    WI    D           128 0.227      0.510        0.764\n 3 B001267  Bennet     CO    D           121 0.273      0.417       -4.91 \n 4 B001277  Blumenthal CT    D           128 0.203      0.294      -13.6  \n 5 B001288  Booker     NJ    D           119 0.160      0.290      -14.1  \n 6 C000127  Cantwell   WA    D           128 0.242      0.276      -15.5  \n 7 C000141  Cardin     MD    D           128 0.25       0.209      -26.4  \n 8 C000174  Carper     DE    D           129 0.295      0.318      -11.4  \n 9 C001070  Casey      PA    D           129 0.287      0.508        0.724\n10 C001088  Coons      DE    D           128 0.289      0.319      -11.4  \n# ℹ 45 more rows\n\n\nNotice that == here is a logical operator, read as “is equal to.” So our full chain of operations says the following: take trump_scores, then filter it to get rows where party is equal to “D”.\nThere are other logical operators:\n\n\n\nLogical operator\nMeaning\n\n\n\n\n==\n“is equal to”\n\n\n!=\n“is not equal to”\n\n\n&gt;\n“is greater than”\n\n\n&lt;\n“is less than”\n\n\n&gt;=\n“is greater than or equal to”\n\n\n&lt;=\n“is less than or equal to”\n\n\n%in%\n“is contained in”\n\n\n&\n“and” (intersection)\n\n\n|\n“or” (union)\n\n\n\nLet’s see a couple of other examples.\n\ntrump_scores |&gt; \n  filter(agree &gt; 0.5)\n\n# A tibble: 69 × 8\n   bioguide last_name state party num_votes agree agree_pred margin_trump\n   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 A000360  Alexander TN    R           118 0.890      0.856        26.0 \n 2 B000575  Blunt     MO    R           128 0.906      0.787        18.6 \n 3 B001135  Burr      NC    R           121 0.893      0.560         3.66\n 4 B001236  Boozman   AR    R           129 0.915      0.851        26.9 \n 5 B001243  Blackburn TN    R           131 0.885      0.889        26.0 \n 6 B001261  Barrasso  WY    R           129 0.891      0.895        46.3 \n 7 B001310  Braun     IN    R            44 0.909      0.713        19.2 \n 8 C000567  Cochran   MS    R            68 0.971      0.830        17.8 \n 9 C000880  Crapo     ID    R           125 0.904      0.870        31.8 \n10 C001035  Collins   ME    R           129 0.651      0.441        -2.96\n# ℹ 59 more rows\n\n\n\ntrump_scores |&gt; \n  filter(state %in% c(\"CA\", \"TX\"))\n\n# A tibble: 4 × 8\n  bioguide last_name state party num_votes agree agree_pred margin_trump\n  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n1 C001056  Cornyn    TX    R           129 0.922      0.659         9.00\n2 C001098  Cruz      TX    R           126 0.921      0.663         9.00\n3 F000062  Feinstein CA    D           128 0.242      0.201       -30.1 \n4 H001075  Harris    CA    D           116 0.164      0.209       -30.1 \n\n\n\ntrump_scores |&gt; \n  filter(state == \"WV\" & party == \"D\")\n\n# A tibble: 1 × 8\n  bioguide last_name state party num_votes agree agree_pred margin_trump\n  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n1 M001183  Manchin   WV    D           129 0.504      0.893         42.2\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nAdd a new column to the data frame, called diff_agree, which subtracts agree and agree_pred. How would you create abs_diff_agree, defined as the absolute value of diff_agree? Your code:\nFilter the data frame to only get senators for which we have information on fewer than (or equal to) five votes. Your code:\nFilter the data frame to only get Democrats who agreed with Trump in at least 30% of votes. Your code:\n\n\n\n\n\n2.2.5 Ordering rows\nThe arrange() function allows us to order rows according to values. For example, let’s order based on the agree variable:\n\ntrump_scores |&gt; \n  arrange(agree)\n\n# A tibble: 122 × 8\n   bioguide last_name    state party num_votes agree agree_pred margin_trump\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 H000273  Hickenlooper CO    D             2 0         0.0302        -4.91\n 2 H000601  Hagerty      TN    R             2 0         0.115         26.0 \n 3 L000570  Luján        NM    D           186 0.124     0.243         -8.21\n 4 G000555  Gillibrand   NY    D           121 0.124     0.242        -22.5 \n 5 M001176  Merkley      OR    D           129 0.155     0.323        -11.0 \n 6 W000817  Warren       MA    D           116 0.155     0.216        -27.2 \n 7 B001288  Booker       NJ    D           119 0.160     0.290        -14.1 \n 8 S000033  Sanders      VT    D           112 0.161     0.221        -26.4 \n 9 H001075  Harris       CA    D           116 0.164     0.209        -30.1 \n10 M000133  Markey       MA    D           127 0.165     0.213        -27.2 \n# ℹ 112 more rows\n\n\nMaybe we only want senators with more than a few data points. Remember that we can chain operations:\n\ntrump_scores |&gt; \n  filter(num_votes &gt;= 10) |&gt; \n  arrange(agree)\n\n# A tibble: 115 × 8\n   bioguide last_name  state party num_votes agree agree_pred margin_trump\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 L000570  Luján      NM    D           186 0.124      0.243        -8.21\n 2 G000555  Gillibrand NY    D           121 0.124      0.242       -22.5 \n 3 M001176  Merkley    OR    D           129 0.155      0.323       -11.0 \n 4 W000817  Warren     MA    D           116 0.155      0.216       -27.2 \n 5 B001288  Booker     NJ    D           119 0.160      0.290       -14.1 \n 6 S000033  Sanders    VT    D           112 0.161      0.221       -26.4 \n 7 H001075  Harris     CA    D           116 0.164      0.209       -30.1 \n 8 M000133  Markey     MA    D           127 0.165      0.213       -27.2 \n 9 W000779  Wyden      OR    D           129 0.186      0.323       -11.0 \n10 B001277  Blumenthal CT    D           128 0.203      0.294       -13.6 \n# ℹ 105 more rows\n\n\nBy default, arrange() uses increasing order (like sort()). To use decreasing order, add a minus sign:\n\ntrump_scores |&gt; \n  filter(num_votes &gt;= 10) |&gt; \n  arrange(-agree)\n\n# A tibble: 115 × 8\n   bioguide last_name state party num_votes agree agree_pred margin_trump\n   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 M001198  Marshall  KS    R           183 0.973      0.933        20.6 \n 2 C000567  Cochran   MS    R            68 0.971      0.830        17.8 \n 3 H000338  Hatch     UT    R            84 0.964      0.825        18.1 \n 4 M001197  McSally   AZ    R           136 0.949      0.562         3.55\n 5 P000612  Perdue    GA    R           119 0.941      0.606         5.16\n 6 C001096  Cramer    ND    R           135 0.941      0.908        35.7 \n 7 R000307  Roberts   KS    R           127 0.937      0.818        20.6 \n 8 C001056  Cornyn    TX    R           129 0.922      0.659         9.00\n 9 H001061  Hoeven    ND    R           129 0.922      0.883        35.7 \n10 C001047  Capito    WV    R           127 0.921      0.896        42.2 \n# ℹ 105 more rows\n\n\nYou can also order rows by more than one variable. What this does is to order by the first variable, and resolve any ties by ordering by the second variable (and so forth if you have more than two ordering variables). For example, let’s first order our data frame by party, and then within party order by agreement with Trump:\n\ntrump_scores |&gt; \n  filter(num_votes &gt;= 10) |&gt; \n  arrange(party, agree)\n\n# A tibble: 115 × 8\n   bioguide last_name  state party num_votes agree agree_pred margin_trump\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 L000570  Luján      NM    D           186 0.124      0.243        -8.21\n 2 G000555  Gillibrand NY    D           121 0.124      0.242       -22.5 \n 3 M001176  Merkley    OR    D           129 0.155      0.323       -11.0 \n 4 W000817  Warren     MA    D           116 0.155      0.216       -27.2 \n 5 B001288  Booker     NJ    D           119 0.160      0.290       -14.1 \n 6 S000033  Sanders    VT    D           112 0.161      0.221       -26.4 \n 7 H001075  Harris     CA    D           116 0.164      0.209       -30.1 \n 8 M000133  Markey     MA    D           127 0.165      0.213       -27.2 \n 9 W000779  Wyden      OR    D           129 0.186      0.323       -11.0 \n10 B001277  Blumenthal CT    D           128 0.203      0.294       -13.6 \n# ℹ 105 more rows\n\n\n\n\n\n\n\n\nExercise\n\n\n\nArrange the data by diff_pred, the difference between agreement and predicted agreement with Trump. (You should have code on how to create this variable from the last exercise). Your code:\n\n\n\n\n2.2.6 Summarizing data\ndplyr makes summarizing data a breeze using the summarize() function:\n\ntrump_scores |&gt; \n  summarize(mean_agree = mean(agree),\n            mean_agree_pred = mean(agree_pred))\n\n# A tibble: 1 × 2\n  mean_agree mean_agree_pred\n       &lt;dbl&gt;           &lt;dbl&gt;\n1      0.592           0.572\n\n\nTo make summaries, we can use any function that takes a vector and returns one value. Another example:\n\ntrump_scores |&gt; \n  filter(num_votes &gt;= 5) |&gt; # to filter out senators with few data points\n  summarize(max_agree = max(agree),\n            min_agree = min(agree))\n\n# A tibble: 1 × 2\n  max_agree min_agree\n      &lt;dbl&gt;     &lt;dbl&gt;\n1         1     0.124\n\n\nGrouped summaries allow us to disaggregate summaries according to other variables (usually categorical):\n\ntrump_scores |&gt; \n  filter(num_votes &gt;= 5) |&gt; # to filter out senators with few data points\n  summarize(mean_agree = mean(agree),\n            max_agree = max(agree),\n            min_agree = min(agree),\n            .by = party) # to group by party\n\n# A tibble: 2 × 4\n  party mean_agree max_agree min_agree\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 R          0.876     1         0.651\n2 D          0.272     0.548     0.124\n\n\n\n\n\n\n\n\nExercise\n\n\n\nObtain the maximum absolute difference in agreement with Trump (the abs_diff_agree variable from before) for each party.\n\n\n\n\n2.2.7 Overview\n\n\n\nFunction\nPurpose\n\n\n\n\nselect()\nSelect columns\n\n\nrename()\nRename columns\n\n\nmutate()\nCreating columns\n\n\nfilter()\nFiltering rows\n\n\narrange()\nOrdering rows\n\n\nsummarize()\nSummarizing data\n\n\nsummarize(…, .by = )\nSummarizing data (by groups)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tidy data analysis I</span>"
    ]
  },
  {
    "objectID": "02_tidy_data1.html#visualizing-data-with-ggplot2",
    "href": "02_tidy_data1.html#visualizing-data-with-ggplot2",
    "title": "2  Tidy data analysis I",
    "section": "2.3 Visualizing data with ggplot2",
    "text": "2.3 Visualizing data with ggplot2\nggplot2 is the package in charge of data visualization in the tidyverse. It is extremely flexible and allows us to draw bar plots, box plots, histograms, scatter plots, and many other types of plots (see examples at R Charts).\nThroughout this module we will use a subset of our data frame, which only includes senators with more than a few data points:\n\ntrump_scores_ss &lt;- trump_scores |&gt; \n  filter(num_votes &gt;= 10)\n\nThe ggplot2 syntax provides a unifying interface (the “grammar of graphics” or “gg”) for drawing all different types of plots. One draws plots by adding different “layers,” and the core code always includes the following:\n\nA ggplot() command with a data = argument specifying a data frame and a mapping = aes() argument specifying “aesthetic mappings,” i.e., how we want to use the columns in the data frame in the plot (for example, in the x-axis, as color, etc.).\n“geoms,” such as geom_bar() or geom_point(), specifying what to draw on the plot.\n\nSo all ggplot2 commands will have at least three elements: data, aesthetic mappings, and geoms.\n\n2.3.1 Univariate plots: categorical\nLet’s see an example of a bar plot with a categorical variable:\n\nggplot(data = trump_scores_ss, mapping = aes(x = party)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAs with any other function, we can drop the argument names if we specify the argument values in order. This is common in ggplot2 code:\n\nggplot(trump_scores_ss, aes(x = party)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nNotice how geom_bar() automatically computes the number of observations in each category for us. Sometimes we want to use numbers in our data frame as part of a bar plot. Here we can use the geom_col() geom specifying both x and y aesthetic mappings, in which is sometimes called a “column plot:”\n\nggplot(trump_scores_ss |&gt; filter(state == \"ME\"),\n       aes(x = last_name, y = agree)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nDraw a column plot with the agreement with Trump of Bernie Sanders and Ted Cruz. What happens if you use last_name as the y aesthetic mapping and agree in the x aesthetic mapping? Your code:\n\n\nA common use of geom_col() is to create “ranking plots.” For example, who are the senators with highest agreement with Trump? We can start with something like this:\n\nggplot(trump_scores_ss,\n       aes(x = agree, y = last_name)) +\n  geom_col()\n\n\n\n\n\n\n\n\nWe might want to (1) select the top 10 observations and (2) order the bars according to the agree values. We can do these operations with slice_max() and fct_reorder(), as shown below:\n\nggplot(trump_scores_ss |&gt; slice_max(agree, n = 10),\n       aes(x = agree, y = fct_reorder(last_name, agree))) +\n  geom_col()\n\n\n\n\n\n\n\n\nWe can also plot the senators with the lowest agreement with Trump using slice_min() and fct_reorder() with a minus sign in the ordering variable:\n\nggplot(trump_scores_ss |&gt; slice_min(agree, n = 10),\n       aes(x = agree, y = fct_reorder(last_name, -agree))) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n2.3.2 Univariate plots: numerical\nWe can draw a histogram with geom_histogram():\n\nggplot(trump_scores_ss, aes(x = agree)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nNotice the warning message above. It’s telling us that, by default, geom_histogram() will draw 30 bins. Sometimes we want to modify this behavior. The following code has some common options for geom_histogram() and their explanations:\n\nggplot(trump_scores_ss, aes(x = agree)) +\n  geom_histogram(binwidth = 0.05,   # draw bins every 0.05 jumps in x\n                 boundary = 0,      # don't shift bins to integers\n                 closed   = \"left\") # close bins on the left\n\n\n\n\n\n\n\n\nSometimes we want to manually alter a scale. This is accomplished with the scale_*() family of ggplot2 functions. Here we use the scale_x_continuous() function to make the x-axis go from 0 to 1:\n\nggplot(trump_scores_ss, aes(x = agree)) +\n  geom_histogram(binwidth = 0.05, boundary = 0, closed   = \"left\") +   \n  scale_x_continuous(limits = c(0, 1))\n\n\n\n\n\n\n\n\nAdding the fill aesthetic mapping to a histogram will divide it according to a categorical variable. This is actually a bivariate plot!\n\nggplot(trump_scores_ss, aes(x = agree, fill = party)) +\n  geom_histogram(binwidth = 0.05, boundary = 0, closed   = \"left\") +   \n  scale_x_continuous(limits = c(0, 1)) +\n  # change default colors:\n  scale_fill_manual(values = c(\"D\" = \"blue\", \"R\" = \"red\"))\n\n\n\n\n\n\n\n\n\n\n2.3.3 Bivariate plots\nAnother common bivariate plot for categorical and numerical variables is the grouped box plot:\n\nggplot(trump_scores_ss, aes(x = agree, y = party)) +\n  geom_boxplot() +\n  scale_x_continuous(limits = c(0, 1)) # same change as before\n\n\n\n\n\n\n\n\nFor bivariate plots of numerical variables, scatter plots are made with geom_point():\n\nggplot(trump_scores_ss, aes(x = margin_trump, y = agree)) +\n  geom_point()\n\n\n\n\n\n\n\n\nWe can add the color aesthetic mapping to add a third variable:\n\nggplot(trump_scores_ss, aes(x = margin_trump, y = agree, color = party)) +\n  geom_point() +\n  scale_color_manual(values = c(\"D\" = \"blue\", \"R\" = \"red\"))\n\n\n\n\n\n\n\n\nLet’s finish our plot with the labs() function, which allows us to add labels to our aesthetic mappings, as well as titles and notes:\n\nggplot(trump_scores, aes(x = margin_trump, y = agree, color = party)) +\n  geom_point() +\n  scale_color_manual(values = c(\"D\" = \"blue\", \"R\" = \"red\")) +\n  labs(x = \"Trump margin in the senator's state (p.p.)\",\n       y = \"Votes in agreement with Trump (prop.)\",\n       color = \"Party\",\n       title = \"Relationship between Trump margins and senators' votes\",\n       caption = \"Data source: FiveThirtyEight (2021)\")\n\n\n\n\n\n\n\n\nWe will review a few more customization options, including text labels and facets, in a subsequent module.\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWhittinghill, Dexter C, and Robert V Hogg. 2001. “A Little Uniform Density with Big Instructional Potential.” Journal of Statistics Education 9 (2).\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tidy data analysis I</span>"
    ]
  },
  {
    "objectID": "03_functions.html",
    "href": "03_functions.html",
    "title": "3  Functions",
    "section": "",
    "text": "3.1 Basics",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "03_functions.html#basics",
    "href": "03_functions.html#basics",
    "title": "3  Functions",
    "section": "",
    "text": "3.1.1 What is a function?\nInformally, a function is anything that takes input(s) and gives one defined output. There are always three main parts:\n\nThe input (\\(x\\) values, or each value in the domain)\nThe relationship of interest\nThe output (\\(y\\) values, or a unique value in the range)\n\n\n\n\n\n\n\nFigure 3.1: Function machine. Source: Bill Bailey on Wikimedia Commons.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n“\\(f(x) = \\space ...\\) is the classic notation for writing a function, but we can also use”\\(y = \\space ...\\)“. This is because \\(y\\) is”a function of” \\(x\\), so \\(y=f(x)\\).\n\n\nLet’s take a look at an example and break down the structure:\n\\[f(x) = 3x + 4\\]\n\n\\(x\\) is the input (some value) that the function takes.\nFor any \\(x\\), we multiply by three and add 4, which is the relationship.\nFinally, \\(f(x)\\) or \\(y\\) is the unique result, or the output.\n\nThe most common name to give a function is, predictably, “\\(f\\)”, but we can have other names such as “\\(g\\)” or “\\(h\\)”. The choice is yours.\n\n\n\n\n\n\nImportant\n\n\n\nWhen reading out loud, we say “[name of function] of x equals [relationship]. For example, \\(f(x) = x^2\\) is referred to as”f of x equals x squared.”\n\n\n\n\n3.1.2 Vertical line test\n\n\n\n\n\n\nExercise\n\n\n\nWhen graphed, vertical lines cannot touch functions at more than one point. Why?\nWhich of the following represent functions?\n\n\n\n\n\n\nFigure 3.2: Vertical line test: examples.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "03_functions.html#functions-in-r",
    "href": "03_functions.html#functions-in-r",
    "title": "3  Functions",
    "section": "3.2 Functions in R",
    "text": "3.2 Functions in R\nOften we need to create our own functions in R. To build them: we use the keyword function alongside the following syntax: function_name &lt;- function(argumentnames){ operation }\n\nfunction_name: the name of the function, that will be stored as an object in the R environment. Make the name concise and memorable!\nfunction(argumentnames): the inputs of the function.\n{ operation }: a set of commands that are run in a predefined order every time we call the function.\n\nFor example, we can create a function that multiplies a number by 2:\n\nmult_by_two &lt;- function(x){x * 2}\n\n\nmult_by_two(x = 5) # we can also omit the argument name (x =)\n\n[1] 10\n\n\nIf the function body works for vectors, our custom function will do too:\n\nmult_by_two(1:10)\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n\nWe can also automate more complicated tasks such as calculating the area of a circle from its radius:\n\ncirc_area_r &lt;- function(r){\n    pi * r ^ 2\n}\ncirc_area_r(r = 3)\n\n[1] 28.27433\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate a function that calculates the area of a circle from its diameter. So your_function(d = 6) should yield the same result as the example above. Your code:\n\n\nFunctions can take more than one argument/input. In a silly example, let’s generalize our first function:\n\nmult_by &lt;- function(x, mult){x * mult}\n\n\nmult_by(x = 1:5, mult = 10)\n\n[1] 10 20 30 40 50\n\n\n\nmult_by(1:5, mult = 10)\n\n[1] 10 20 30 40 50\n\n\n\nmult_by(1:5, 10)\n\n[1] 10 20 30 40 50\n\n\nTo graph a function, we’ll use our friend ggplot2 and stat_function():\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nggplot() +\n  stat_function(fun = mult_by_two, \n                xlim = c(-5, 5)) # domain over which we will plot the function\n\n\n\n\n\n\n\n\nUser-defined functions have endless possibilities! We encourage you to get creative and try to automate new tasks when possible, especially if they are repetitive.\n\n\n\n\n\n\nTip\n\n\n\nFunctions in R can also take non-numeric inputs. For example:\n\nsay_my_name &lt;- function(my_name){paste(\"My name is\", my_name)}\n\n\nsay_my_name(\"Inigo Montoya\")\n\n[1] \"My name is Inigo Montoya\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "03_functions.html#common-types-of-functions",
    "href": "03_functions.html#common-types-of-functions",
    "title": "3  Functions",
    "section": "3.3 Common types of functions",
    "text": "3.3 Common types of functions\n\n3.3.1 Linear functions\n\\[y=mx+b\\]\nLinear functions are those whose graph is a straight line (in two dimensions).\n\n\\(m\\) is the slope, or the rate of change (common interpretation: for every one unit increase in \\(x\\), \\(y\\) increases \\(m\\) units).\n\\(b\\) is the y intercept, or the constant term (the value of \\(y\\) when \\(x=0\\)).\n\nBelow is a graph of the function \\(y = 3x + 4\\):\n\nggplot() +\n  stat_function(fun = function(x){3 * x + 4}, # we don't need to create an object\n                xlim = c(-5, 5)) \n\n\n\n\n\n\n\n\n\n\n3.3.2 Quadratic functions\n\\[y=ax^2 + bx + c\\]\nQuadratic functions take “U” forms. If \\(a\\) is positive, it is a regular “U” shape. If \\(a\\) is negative, it is an “inverted U” shape.\nNote that \\(x^2\\) always returns positive values (or zero).\nBelow is a graph of the function \\(y = x^2\\):\n\nggplot() +\n  stat_function(fun = function(x){x ^ 2},\n                xlim = c(-5, 5)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSocial scientists commonly use linear or quadratic functions as theoretical simplifications of social phenomena. Can you give any examples?\n\n\n\n\n\n\n\n\nExercise\n\n\n\nGraph the function \\(y = x^2 + 2x - 10\\), i.e., a quadratic function with \\(a=1\\), \\(b=2\\), and \\(c=-10\\).\nNext, try switching up these values and the xlim = argument. How do they each alter the function (and plot)?\n\n\n\n\n3.3.3 Cubic functions\n\\[y=ax^3 + bx^2 + cx +d\\]\nThese lines (generally) have two curves (inflection points).\nBelow is a graph of the function \\(y = x^3\\):\n\nggplot() +\n  stat_function(fun = function(x){x ^ 3},\n                xlim = c(-5, 5)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWe’ll briefly introduce Desmos, an online graphing calculator. Use Desmos to graph the following function \\(y = 1x^3 + 1x^2 + 1x + 1\\). What happens when you change the \\(a\\), \\(b\\), \\(c\\), and \\(d\\) parameters?\n\n\n\n\n3.3.4 Polynomial functions\n\\[y=ax^n + bx^{n-1} + ... + c\\]\nThese functions have (a maximum of) \\(n-1\\) changes in direction (turning points). They also have (a maximum of) \\(n\\) x-intercepts.\nHigh-order polynomials can be made arbitrarily precise!\nBelow is a graph of the function \\(y = \\frac{1}{4}x^4 - 5 x^2 + x\\).\n\nggplot() +\n  stat_function(fun = function(x){1/4 * x ^ 4 - 5 * x ^ 2 + x},\n                xlim = c(-5, 5)) \n\n\n\n\n\n\n\n\n\n\n3.3.5 Exponential functions\n\\[y = ab^{x}\\]\nHere our input (\\(x\\)), is the exponent.\nBelow is a graph of the function \\(y = 2^x\\):\n\nggplot() +\n  stat_function(fun = function(x){2 ^ x},\n                xlim = c(-5, 5)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nExponential growth appears quite frequently social science theories. Which variables can be theorized to have exponential growth over time?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "03_functions.html#logarithms-and-exponents",
    "href": "03_functions.html#logarithms-and-exponents",
    "title": "3  Functions",
    "section": "3.4 Logarithms and exponents",
    "text": "3.4 Logarithms and exponents\n\n3.4.1 Logarithms\nLogarithms are the opposite/inverse of exponents. They ask how many times you must raise the base to get \\(x\\).\nSo \\(log_a(b)=x\\) is asking “a raised to what power x gives b?” For example, \\(\\log_3(81) = 4\\) because \\(3^4=81\\).\n\n\n\n\n\n\nWarning\n\n\n\nLogarithms are undefined if the base is \\(\\le 0\\) (at least in the real numbers).\n\n\n\n\n3.4.2 Relationships\nIf, \\[ log_ax=b\\] then, \\[a^{log_{a}x}=a^b\\] and \\[x=a^b\\]\n\n\n3.4.3 Basic rules\n\nChange of Base rule: \\(\\dfrac{\\log_x n}{\\log_x m} = \\log_m n\\)\nProduct Rule: \\(\\log_x(ab) = \\log_xa + \\log_xb\\)\nQuotient Rule: \\(\\log_x\\left(\\frac{a}{b}\\right) = \\log_xa - \\log_xb\\)\nPower Rule: \\(\\log_xa^b = b \\log_x a\\)\nLogarithm of 1: \\(\\log_x 1 = 0\\)\nLogarithm of the Base: \\(log_{x}x=1\\)\nExponential Identity: \\(m^{\\log_m(a)} = a\\)\n\n\n\n3.4.4 Natural logarithms\n\nWe most often use natural logarithms for our purposes.\nThis means \\(log_e(x)\\), which is usually written as \\(ln(x)\\).\n\n\n\n\n\n\n\nImportant\n\n\n\n\\(e \\approx 2.7183\\).\n\n\n\n\\(ln(x)\\) and its exponent opposite, \\(e^x\\), have nice properties when we perform calculus.\n\n\n\n3.4.5 Illustration of \\(e\\)\nImagine you invest $1 in a bank and receive 100% interest for one year, and the bank pays you back once a year: \\[(1+1)^1= 2\\].\nWhen it pays you twice a year with compound interest:\n\\[(1+1/2)^2=2.25\\]\nIf it pays you three times a year:\n\\[(1+1/3)^3=2.37...\\]\nWhat will happen when the bank pays you once a month? Once a day?\n\\[(1+\\frac{1}{n})^{n}\\]\nHowever, there is limit to what you can get.\n\\[\\lim_{n\\to\\infty} (1 + \\dfrac{1}{n})^n = 2.7183... = e\\]\nFor any interest rate \\(k\\) and number of times the bank pays you \\(t\\): \\[\\lim_{n\\to\\infty} (1 + \\dfrac{k}{n})^{nt} = e^{kt}\\]\n\n\\(e\\) is important for defining exponential growth. Since \\(ln(e^x) = x\\), the natural logarithm helps us turn exponential functions into linear ones.\n\n\n\n\n\n\n\nExercise\n\n\n\nSolve the problems below, simplifying as much as you can. \\[log_{10}(1000)\\] \\[log_2(\\dfrac{8}{32})\\] \\[10^{log_{10}(300)}\\] \\[ln(1)\\] \\[ln(e^2)\\] \\[ln(5e)\\]\n\n\n\n\n3.4.6 Logarithms in R\nBy default, R’s log() function computes natural logarithms:\n\nlog(100)\n\n[1] 4.60517\n\n\nWe can change this behavior with the base = argument:\n\nlog(100, base = 10)\n\n[1] 2\n\n\nWe can also plot logarithms. Remember that \\(ln(x)\\) \\(\\forall x&lt;0\\) is undefined (at least in the real numbers), and ggplot2 displays a nice warning letting us know!\n\nggplot() +\n  stat_function(fun = function(x){log(x)},\n                xlim = c(-5, 5)) \n\nWarning in log(x): NaNs produced\n\n\nWarning: Removed 50 rows containing missing values or values outside the scale range\n(`geom_function()`).\n\n\n\n\n\n\n\n\n\n\nggplot() +\n  stat_function(fun = function(x){log(x)},\n                xlim = c(1, 100))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "03_functions.html#composite-functions-functions-of-functions",
    "href": "03_functions.html#composite-functions-functions-of-functions",
    "title": "3  Functions",
    "section": "3.5 Composite functions (functions of functions)",
    "text": "3.5 Composite functions (functions of functions)\nFunctions can take other functions as inputs, e.g., \\(f(g(x))\\). This means that the outside function takes the output of the inside function as its input.\nSay we have the exterior function \\[f(x)=x^2\\] and the interior function \\[g(x)=x-3\\].\nThen if we want \\(f(g(x))\\), we would subtract 3 from any input, and then square the result or \\[f(g(x)) = (x-3)^2\\].\n\n\n\n\n\n\nWarning\n\n\n\nWe write this as \\((x-3)^2\\), not \\(x^2-3\\)!\n\n\nR can handle this just fine:\n\nf &lt;- function(x){x ^ 2}\ng &lt;- function(x){x - 3}\n\n\nf(g(5))\n\n[1] 4\n\n\nHere we can also use pipes to make this code more readable (imagine if we were chaining multiple functions…). Remember that pipes can be inserted with the Cmd/Ctrl + Shift + M shortcut.\n\n# compute g(5), THEN f() of that\ng(5) |&gt; f()\n\n[1] 4\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCompute g(f(5)) using the definitions above. First do it manually, and then check your answer with R.\n\n\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWhittinghill, Dexter C, and Robert V Hogg. 2001. “A Little Uniform Density with Big Instructional Potential.” Journal of Statistics Education 9 (2).\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "04_calculus.html",
    "href": "04_calculus.html",
    "title": "4  Calculus",
    "section": "",
    "text": "4.1 Derivatives\nIn this section we’ll focus on three big ideas from calculus: derivatives, optimization, and integrals.\nDerivatives are about (instantaneous) rate of change.\nLet’s dissect what Nixon might have said:\nA more graphical way to think about a derivatives is as a slope. Let’s consider a linear function of the form \\(y = 2 x\\):\nlibrary(tidyverse) # could also just do library(ggplot2)\nggplot() +\n  stat_function(fun = function(x){2 * x}, \n                xlim = c(-10, 10))\nWe can imagine any political variables in the x- and y-axes. What is the rate of change? In other words, what is the derivative? Remember that we can calculate the slope with:\n\\[\nm = \\frac{f(x_2) - f(x_1)}{x_2-x_1}\n\\]\nNow consider another slightly more complicated function, a quadratic one, \\(y = x^2\\):\nggplot() +\n  stat_function(fun = function(x){x ^ 2}, \n                xlim = c(-10, 10))\nWhat happens when we apply our slope function?\nTakeaway: here the derivative depends on the value of \\(x\\). It is actually \\(2x\\).\nDifferential calculus is about finding these derivatives in a more straightforward manner! We can generalize our slope formula as follows:\n\\[\nm = \\frac{f(x_1+ \\Delta x) - f(x_1)}{\\Delta x}\n\\]\nThe point is that when \\(\\Delta x\\) is arbitrarily small, we’ll get our rate of change. Formalizing this:\n\\[\n\\lim_{\\Delta x\\to0} \\frac{f(x_1+ \\Delta x) - f(x_1)}{\\Delta x} = \\frac{d}{dx} f(x) = \\frac{dy}{dx} = f'(x)\n\\]\nA few points on notation:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Calculus</span>"
    ]
  },
  {
    "objectID": "04_calculus.html#derivatives",
    "href": "04_calculus.html#derivatives",
    "title": "4  Calculus",
    "section": "",
    "text": "“In the fall of 1972 President Nixon announced that the rate of increase of inflation was decreasing. This was the first time a sitting president used the third derivative to advance his case for reelection” (Rossi 1996)\n\n\n\nInflation’s [first derivative, of prices] rate of increase [second derivative] is going down [third derivative].\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nUse the slope formula to calculate the rate of change between 5 and 6.\nUse the slope formula to calculate the rate of change between 5 and 5.5.\nUse the slope formula to calculate the rate of change between 5 and 5.1.\n\n\n\n\n\n\n\n\n\n\n\\(\\frac{d}{dx} f(x)\\) is read “The derivative of \\(f\\) of \\(x\\) with respect to \\(x\\).”\n\nThe variable with respect to which we’re differentiating is the one that appears in the bottom (in the case above, this is \\(x\\)).\n\n\n\n\n\n\n\nWarning\n\n\n\nWhile the above looks like a fraction, it’s really not. Do not try to cancel out the \\(d\\)s!\n\n\n\\(f'(x)\\) (read: “\\(f\\) prime \\(x\\)”) is the derivative of \\(f(x)\\). This is a more compact form to refer to derivatives when you have defined \\(f(x)\\) elsewhere.\n\n\n4.1.1 Rules of differentiation\nHow to compute derivatives? Sometimes you can try a bunch of numbers and get at the answer. Sometimes you can use the limit-based formula above, if you know a few properties of limits. But in most cases you will either use software (more on this later) or the rules of differentiation, which we will cover now.\nConstant rule: \\((c)' = 0\\).\nThere is no change in a constant:\n\nggplot() +\n  stat_function(fun = function(x){2}, xlim = c(-10, 10))\n\n\n\n\n\n\n\n\nCoefficient rule: \\((c \\cdot f(x))' = c \\cdot f'(x)\\).\n\nggplot() +\n  stat_function(fun = function(x){2 * x}, xlim = c(-10, 10), aes(color = \"y = 2x\")) +\n  stat_function(fun = function(x){4 * x}, xlim = c(-10, 10), aes(color = \"y = 4x\")) +\n  scale_color_manual(\"Function\", values = c(\"red\", \"blue\"))\n\n\n\n\n\n\n\n\nSum/difference rule: \\((f(x) \\pm g(x))' = f'(x) \\pm g'(x)\\).\nThe two rules above give us that the derivative is a linear operator.\nPower rule: \\((x^n)'=nx^{(n-1)}\\)\nRemember when we wanted to calculate the derivative of \\(y=x^2\\) above? We can use the power rule, with \\(n=2\\): \\(\\;nx^{(n-1)} = 2x^{(2-1)}=2x\\). Let’s try out \\(\\frac{d}{dx}4x^3\\) and \\(\\frac{d}{dx}(x^2 + 2x)\\) on the board.\n\n\n\n\n\n\nExercise\n\n\n\nUse the differentiation rules we have covered so far to calculate the derivatives of \\(y\\) with respect to \\(x\\) of the following functions:\n\n\\(y = 2x^2 + 10\\)\n\\(y = 5x^4 - \\frac{2}{3}x^3\\)\n\\(y = 9 \\sqrt x\\)\n\\(y = \\frac{4}{x^2}\\)\n\\(y = ax^3 + b\\), where \\(a\\) and \\(b\\) are constants.\n\\(y = \\frac{2w}{5}\\)\n\n\n\nExponent and logarithm rules:\n\\[\n\\begin{aligned}\n(c^x)' &= c^x \\cdot ln(c), & \\forall x&gt;0 \\\\\n(e^x)' &= e^x \\\\\n\\\\\n(log_a(x))' &= \\frac{1}{x \\cdot ln(a)}, & \\forall x&gt;0  \\\\\n(ln(x))' &= \\frac{1}{x}, & \\forall x&gt;0\n\\end{aligned}\n\\]\nWe saw previously how Euler’s number (\\(e\\)) arises from compound interest. The properties above make it very useful in a lot of calculus applications!\n\n\n\n\n\n\nExercise\n\n\n\nCompute the following:\n\n\\(\\frac{d}{dx}(10e^x)\\)\n\\(\\frac{d}{dx}(ln(x) - \\frac{e^2}{3})\\)\n\n\n\nNow we’ll get to a couple of more advanced (and powerful) rules.\n\nProduct rule: \\((f(x)g(x))'=f'(x)g(x) + g'(x)f(x)\\)\nQuotient rule: \\(\\displaystyle(\\frac{f(x)}{g(x)})' = \\frac{f'(x)g(x) + g'(x)f(x)}{[g(x)]^2}\\)\nChain rule: \\((f(g(x))' = f'(g(x)) \\cdot g'(x)\\)\n\n\nLet’s calculate \\(\\frac{d}{dx}(3 \\cdot ln(x) \\cdot x^2)\\) on the board.\n\n\nLet’s compute \\(\\frac{d}{dx}(e^{x^{2}})\\) on the board.\n\n\n\n\n\n\n\nExercise\n\n\n\nUse the differentiation rules we have covered so far to calculate the derivatives of \\(y\\) with respect to \\(x\\) of the following functions:\n\n\\(x^3 \\cdot x\\)\n\\(e^x \\cdot x^2\\)\n\\((3x^4-8)^2\\)\n\n\n\n\n\n4.1.2 Higher-order derivatives\nWe saw how politicians can refer to higher-order derivatives. To compute them, you simply “pass the outputs,” starting from the lowest order and going up.\nThe second derivative tells us whether the slope of a function is increasing, decreasing, or staying the same at any point \\(x\\) on the function’s domain. For example, when driving a car:\n\n\\(f(x)\\) = distance traveled at time \\(x\\)\n\\(f'(x)\\) = speed at time \\(x\\)\n\\(f''(x)\\) = acceleration at time \\(x\\)\n\nLet’s compute the following second derivative:\n\\[f''(x^4) = \\frac{d^2(x^4)}{dx^2}\\]\n\nFirst, we take the first derivative: \\(f'(x^4)=4x^3\\)\nThen we use that output to take the second derivative: \\(f''(x^4)=f'(4x^3)=12x^2\\)\nWe can keep going… for example, the third derivative: \\[f'''(x^4) = f'(12x^2) = 24x\\]\n\n\n\n\n\n\n\nExercise\n\n\n\nCompute the following:\n\n\\(\\frac{d^3}{dx^3}(x^5)\\)\n\\(f''(4x^{3/2})\\)\n\\(f''(4 \\cdot ln(x))\\)\n\n\n\n\n\n4.1.3 Partial derivatives\nFor a function \\(f(x,z)\\), we might want to know how the function changes with respect to \\(x\\). We call this a partial derivative:\n\\[\n\\frac{\\partial}{\\partial_x}f(x,z) = \\frac{\\partial_y}{\\partial_x} = \\partial_x f\n\\]\nTo obtain a partial derivative, we treat all other variables as constants and take the derivative with respect to the variable of interest (here \\(x\\)). For example:\n\\[\n\\begin{aligned}\ny = f(x,z) &= xz \\\\\n\\frac{\\partial_y}{\\partial_x} &= z\n\\end{aligned}\n\\]\nWhat is \\(\\displaystyle\\frac{\\partial_y}{\\partial_z}?\\)\nLet’s solve \\(\\displaystyle\\frac{\\partial (x^2y+xy^2-x)}{\\partial x}\\) and \\(\\displaystyle\\frac{\\partial (x^2y+xy^2-x)}{\\partial y}\\) on the board.\n\n\n\n\n\n\nExample\n\n\n\nLet’s say that \\(y\\) is how much I like a movie, \\(d\\) is how many dogs a movie has, and \\(e\\) is how many explosions a movie has. I claim that how much I like a movie can be expressed by a function of the type \\(y = f(d, e)\\). Evaluate the following situations:\n\nI like dogs and I don’t care about action. So I believe that the true relationship is \\(y = f(d, e) = 3 \\cdot d\\). What is \\(\\displaystyle\\frac{\\partial_y}{\\partial_d}\\), and how can we interpret it?\nI like dogs and I like action. So I believe that the true relationship is \\(y = f(d, e) = 3 \\cdot d + 1 \\cdot e\\). What is \\(\\displaystyle\\frac{\\partial_y}{\\partial_d}\\), and how can we interpret it?\nI like dogs and I like action. But I definitely don’t like them together—I don’t want the dogs to be in danger! So I believe that the true relationship is \\(y = f(d, e) = 3 \\cdot d + 1 \\cdot e -10 \\cdot d \\cdot e\\). What is \\(\\displaystyle\\frac{\\partial_y}{\\partial_d}\\), and how can we interpret it?\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTake the partial derivative with respect to \\(x\\) and with respect to \\(z\\) of the following functions. What would the notation for each look like?\n\n\\(y = 3xz - x\\)\n\\(x^3+z^3+x^4z^4\\)\n\\(e^{xz}\\)\n\n\n\n\n\n4.1.4 Differentiability of functions\nNot all functions are differentiable at every point of their domains!\nAn important concept here is whether functions are continuous at a point:\n\nInformally: A function is continuous at a point if its graph has no holes or breaks at that point\nFormally: A function is continuous at a point \\(a\\) if: \\(\\lim_{x \\to a} f(x)=f(a)\\)\n\nWhen is a function differentiable at a point?\n\nIf a function is differentiable at a point, it is also continuous at that point.\nIf a function is continuous at a point, it is not necessarily differentiable at that point.\n\nImpossible to calculate derivative at sharp turns, cusps, or vertical tangents.\n\n\n\nggplot() +\n  stat_function(fun = function(x){abs(x) + 2}, xlim = c(-4, 4), \n                aes(color = \"y = |x| + 2\")) +\n  stat_function(fun = function(x){sqrt(abs(x)) + 1}, xlim = c(-4, 4), \n                aes(color = \"y = √(|x|) + 1\")) +\n  stat_function(fun = function(x){sign(x) * abs(x)^(1 / 3)}, xlim = c(-4, 4), \n                aes(color = \"y = ₃√x\")) +\n  scale_colour_manual(\"Function\", values = c(\"red\", \"blue\", \"black\")) +\n  labs(title = \"Examples of functions that are not differentiable at x=0\")\n\n\n\n\n\n\n\n\n\nInformally, functions need to be continuous and reasonably smooth to be differentiable.\n\n\n\n4.1.5 How do computers calculate derivatives?\nIn quite a few statistics and machine learning problems, computers need to compute derivatives of arbitrarily complex functions, perhaps millions of times. How do they do it? (see Baydin et al. 2018 for discussion of these three approaches)\n\nSymbolic differentiation: automatically combine the rules of differentiation (power rule, product rule, etc.). It is what math solvers use, e.g., WolframAlpha or (presumably) Symbolab.\nNumerical differentiation: infer the derivative by computing the function at different sample values (like we did with \\(y=x^2\\) before. This is what, for example, R’s optim() function does behind the scenes.\n\n\n# minimize the x^2 + 5 function:\noptim(par = 0, fn = function(x){x ^ 2 + 5}, method = \"L-BFGS-B\")\n\n$par\n[1] 0\n\n$value\n[1] 5\n\n$counts\nfunction gradient \n       1        1 \n\n$convergence\n[1] 0\n\n$message\n[1] \"CONVERGENCE: NORM OF PROJECTED GRADIENT &lt;= PGTOL\"\n\n\n\nAutomatic differentiation: track how every function is constructed from (differentiable) elementary computer operations (e.g., binary arithmetic), and get the result using the chain rule. Implemented in the torch R package, the TensorFlow, PyTorch, and JAX Python libraries, and the ReverseDiff.jl and Zygote.jl Julia packages.\n\n\n\n\nAn example of computing the gradient of an esoteric function using Zygote.jl (from its documentation)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Calculus</span>"
    ]
  },
  {
    "objectID": "04_calculus.html#optimization",
    "href": "04_calculus.html#optimization",
    "title": "4  Calculus",
    "section": "4.2 Optimization",
    "text": "4.2 Optimization\nOptimization allows us to find the minimum or maximum values (or extrema) a function takes. It has many applications in the social sciences:\n\nFormal theory: utility maximization, continuous choices\nOrdinary Least Squares (OLS): Focuses on minimizing the squared errors between observed data and model-estimated values\nMaximum Likelihood Estimation (MLE): Focuses on maximizing a likelihood function, given observed values.\n\n\n4.2.1 Extrema\nOn extrema: informally, a maximum is just the highest value a function takes, and a minimum is the lowest value.\nIn some situations, it can be easy to identify extrema intuitively by looking at a graph of the function.\n\nMaxima are high points (“peaks”)\nMinima are low points (“valleys”)\n\nWe can use derivatives (rates of change!) to get at extrema.\n\n\n4.2.2 Critical points and the First-Order Condition\nAt critical points (or stationary points), the derivative is zero or fails to exist. At these, the function has usually reached a (local) maximum or minimum.\n\nAt a maximum, the function must be increasing before the point and decreasing after it.\nAt a minimum, the function must be decreasing before the point and increasing after it.\n\n\n\n\n\n\n\nWarning\n\n\n\nLocal extrema occur at critical points, but not all critical points are extrema. For instance, sometimes the graph is changing between concave and convex (“inflection points”). Or sometimes the function is not differentiable at that point for other reasons.\n\n\nWe can find the local maxima and/or minima of a function by taking the derivative, setting it equal to zero, and solving for \\(x\\) (or whatever variable). This gives us the First-Order Condition (FOC).\n\\[FOC: f'(x)=0\\]\n\n\n4.2.3 Second-Order Condition\nNotice that after this we only know that there is a critical point. BUT we don’t know if we’ve found a maximum or minimum, or even if we’ve found an extremum.\nTo determine whether a we are seeing a (local) maximum or minimum, we can use the Second Derivative Test:\n\nStart by identifying \\(f''(x)\\)\nSubstitute in the stationary points \\((x^*)\\) identified from the FOC.\n\n\\(f''(x^*) &gt; 0\\) we have a local minimum\n\\(f''(x^*) &lt; 0\\) we have a local maximum\n\\(f''(x^*) = 0\\) we (may) have an inflection point - need to calculate higher-order derivatives (don’t worry about this now)\n\n\nCollectively these give us the Second-Order Condition (SOC).\nLet’s do this procedure and obtain the FOC and SOC for \\(\\displaystyle y=\\frac{1}{2} x^3 + 3 x^2 - 2\\) on the board. What do we learn? Compare this with the plot of the function on Desmos.\n\n\n4.2.4 Local or global extrema?\nNow when it comes to knowing whether extrema are local or global:\n\nHere we use the Extreme value theorem, which states that if a real-valued function is continuous on a closed and bounded (i.e., finite) interval, the function must have a global minimum and a global minimum on that interval at least once. Importantly, in this situation the global extrema exist, and they are either at the local extrema or at the boundaries (where we cannot even find critical points).\nSo to find the minimum/maximum on some interval, compare the local min/max to the value of the function at the interval’s endpoints. So, e.g., if the interval is \\((-\\infty, +\\infty)\\), check the function’s limits as it approaches \\(-\\infty\\) and \\(+\\infty\\).\n\nLet’s try this last step for our example above, \\(\\displaystyle y=\\frac{1}{2} x^3 + 3 x^2 - 2\\), to get the global extrema in the entire domain.\n\n\n\n\n\n\nExercise\n\n\n\nIdentify the global extrema of the function \\(\\displaystyle \\frac{x^3}{3} - \\frac{3}{2}x^2 -10x\\) in the interval \\([-6, 6]\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Calculus</span>"
    ]
  },
  {
    "objectID": "04_calculus.html#integrals",
    "href": "04_calculus.html#integrals",
    "title": "4  Calculus",
    "section": "4.3 Integrals",
    "text": "4.3 Integrals\nInformally, we can think of integrals as the flip side of derivatives.\nWe can motivate integrals as a way of finding the area under a curve. Sometimes finding the area is easy. What’s the area under the curve between \\(x=-1\\) and \\(x=1\\) for this function?\n\\[\nf(x) =\n\\begin{cases}\n\\frac{1}{3} & \\text{for } x \\in [0, 3] \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nNormally, finding the area under a curve is much harder. But this is basically the question behind integration.\n\n4.3.1 Integrals are about infinitesimals too\nLet’s say we have a function \\(y = x^2\\) And we want to find the area under the curve from \\(x=0\\) to \\(x=1\\). How would we do this?\n\nggplot() +\n  # draw main function\n  stat_function(fun = function(x){x ^ 2}, xlim = c(-2, 2)) +\n  # fill area under the curve between x = 0 and x = 1\n  geom_area(mapping = aes(x = 0), stat = \"function\",\n            fun = function(x){x ^ 2}, xlim = c(0, 1), fill = \"red\")\n\n\n\n\n\n\n\n\nOne way to approximate this area is by drawing narrow rectangles that cover the area in red. Let’s draw this on the board.\nOur approximation is rough, but it gets better and better the narrower the rectangles are:\n\\[\nArea = lim_{\\Delta x \\to 0}\\sum_i^n{f(x) \\cdot \\Delta x}\n\\]\n, where \\(\\Delta x\\) is the width of the rectangles and \\(n\\) is their number.\nThis is actually one way to define the definite integral, \\(\\displaystyle\\int_a^bf(x)dx\\) (also known as the Riemann integral). We’ll learn how to compute these in a few moments.\n\n\n4.3.2 Indefinite integrals as antiderivatives\nThe indefinite integral, also known as the antiderivative, \\(F(x)\\) is the inverse of the function \\(f'(x)\\). \\[F(x)= \\displaystyle\\int f(x) \\text{ } dx\\]\nThis means if you take the derivative of \\(F(x)\\), you wind up back at \\(f(x)\\). \\[F' = f \\text{ or } \\displaystyle\\frac{dF(x)}{dx} = f(x)\\]\nFor example, what is the antiderivative for a constant function \\(f(x) = 1\\)? Is there just one? (this example comes from Moore and Siegel, 2013, p. 137).\nThis process is called anti-differentiation. We can use this concept to help us solve definite integrals!\n\n\n4.3.3 Solving definite integrals\nOne way to calculate definite integrals, known as the “fundamental theorem of calculus,” is shown below:\n\\[\\displaystyle\\int_{a}^{b} f(x) \\text{ } dx = F(b)-F(a) = F(x)\\bigg|_{a}^{b}\\]\nFirst we determine the antiderivative (indefinite integral) of \\(f(x)\\) (and represent it \\(F(x)\\)), substitute the upper limit first and then the lower limit one by one, and subtract the results in order.\n\n\n\n\n\n\nWarning\n\n\n\n\\(C\\) in the following definitions and rules is the called the “constant of integration.” We need to add it when we define all antiderivatives (integrals) of a function because the anti-derivative “undoes” the derivative.\nRemember that the derivative of any constant is zero. So if we find an integral \\(F(x)\\) whose derivative is \\(f(x)\\), adding (or subtracting) any constant will give us another integral \\(F(x)+C\\) whose derivative is also \\(f(x)\\).\n\n\n\n\n4.3.4 Rules of integration\nMany of the rules of integetration have counterparts in differentiation.\nCoefficient rule: \\(\\displaystyle\\int c f(x)\\,dx = c \\int f(x)\\,dx\\)\nSum/difference rule: \\(\\displaystyle\\int (f(x) \\pm g(x))\\,dx = \\int f(x)\\,dx \\pm \\int g(x)\\,dx\\)\nConstant rule: \\(\\displaystyle\\int c\\,dx = cx + C\\)\nPower rule: \\(\\displaystyle\\int x^n\\,dx = \\frac{x^{n+1}}{n+1} + C \\qquad \\forall n \\neq -1\\)\nReciprocal rule:\\(\\displaystyle\\int \\frac{1}{x}\\,dx = \\ln(x) + C\\)\nExponent and logarithm rules:\n\\[\n\\begin{aligned}\n\\displaystyle \\int e^x \\,dx &= e^x+C \\\\\n\\displaystyle \\int c^x \\,dx &= \\frac{c^x}{ln(c)}+C\\\\\n\\\\\n\\displaystyle \\int ln(x) \\,dx &= x \\cdot ln(x) - x+C \\\\\n\\displaystyle \\int log_c(x) \\,dx &= \\frac{x \\cdot log_c(x) - x}{log_c(x)}+C\n\\end{aligned}\n\\]\nThe final two rules are analog to the product rule and the chain rule:\nIntegration by parts: \\(\\displaystyle \\int f(x)g'(x)\\,dx = f(x)g(x) - \\int f'(x)g(x)\\,dx\\)\nIntegration by substitution:\n\\[\n\\begin{aligned}\n1.& \\text{ Have }\\displaystyle \\int f(g(x))g'(x)\\,dx \\\\\n2.& \\text{ Set u=g(x)} \\\\\n3.& \\text{ Compute } \\int f(u)\\,du \\\\\n4.& \\text{ Replace u for g(x)}\n\\end{aligned}\n\\]\nLet’s do an example on the board: \\(\\displaystyle \\int e^{x^2} 2x  \\,dx\\).\n\n\n4.3.5 Solving the problem\nRemember our function \\(y=x^2\\) and our goal of finding the area under the curve from \\(x=0\\) to \\(x=1\\). We can describe this problem as \\(\\displaystyle \\int_0^1 x^2 dx\\)\nFind the indefinite integral, \\(F(x)\\):\n\\[\n\\displaystyle\\int x^2 \\text{ } dx = \\displaystyle\\frac{x^3}{3}+C\n\\]\nNow we’ll use the fundamental theory of calculus. Evaluate at our lowest and highest points, \\(F(0)\\) and \\(F(1)\\):\n\n\\(F(0) = 0\\)\n\\(F(1) = \\displaystyle\\frac{1}{3}\\)\nTechnically \\(0 + C\\) and \\(\\displaystyle\\frac{1}{3} + C\\), but the C’s will fall out in the next step\n\nCalculate \\(F(1) - F(0)\\) \\[\\displaystyle\\frac{1}{3} - 0 = \\displaystyle\\frac{1}{3}\\]\n\n\n\n\n\n\nExercise\n\n\n\nSolve the following indefinite integrals:\n\n\\(\\int x^2 \\text{ } dx\\)\n\\(\\int 3x^2\\text{ } dx\\)\n\\(\\int x\\text{ } dx\\)\n\\(\\int (3x^2 + 2x - 7\\text{ })dx\\)\n\\(\\int \\dfrac{2}{x}\\text{ }dx\\)\n\nAnd solve the following definite integrals:\n\n\\(\\displaystyle\\int_{1}^{7} x^2 \\text{ } dx\\)\n\\(\\displaystyle\\int_{1}^{10} 3x^2 \\text{ } dx\\)\n\\(\\displaystyle\\int_7^7 x\\text{ } dx\\)\n\\(\\displaystyle\\int_{1}^{5} 3x^2 + 2x - 7\\text{ }dx\\)\n\\(\\int_{1}^{e} \\dfrac{2}{x}\\text{ }dx\\)\n\n\n\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWhittinghill, Dexter C, and Robert V Hogg. 2001. “A Little Uniform Density with Big Instructional Potential.” Journal of Statistics Education 9 (2).\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Calculus</span>"
    ]
  },
  {
    "objectID": "05_matrices.html",
    "href": "05_matrices.html",
    "title": "5  Matrices",
    "section": "",
    "text": "5.1 Introduction\nMatrices are rectangular collections of numbers. In this module we will introduce them and review some basic operators, to then introduce a sneak peek of why matrices are useful (and cool).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "05_matrices.html#introduction",
    "href": "05_matrices.html#introduction",
    "title": "5  Matrices",
    "section": "",
    "text": "5.1.1 Scalars\nOne number (for example, 12) is referred to as a scalar.\n\\[\na = 12\n\\]\n\n\n5.1.2 Vectors\nWe can put several scalars together to make a vector. Here is an example:\n\\[\n\\overrightarrow b =\n\\begin{bmatrix}\n  12 \\\\\n  14 \\\\\n  15\n\\end{bmatrix}\n\\]\nSince this is a column of numbers, we cleverly refer to it as a column vector.\nHere is another example of a vector, this time represented as a row vector:\n\\[\n\\overrightarrow c = \\begin{bmatrix}\n  12 & 14 & 15\n\\end{bmatrix}\n\\]\nColumn vectors are possibly more common and useful, but we sometimes write things down using row vectors to\nVectors are fairly easy to construct in R. As we saw before, we can use the c() function to combine elements:\n\nc(5, 25, -2, 1)\n\n[1]  5 25 -2  1\n\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember that the code above does not create any objects. To do so, you’d need to use the assignment operator (&lt;-):\n\nvector_example &lt;- c(5, 25, -2, 1)\nvector_example\n\n[1]  5 25 -2  1\n\n\n\n\nOr we can also create vectors from sequences with the : operator or the seq() function:\n\n10:20\n\n [1] 10 11 12 13 14 15 16 17 18 19 20\n\n\n\nseq(from = 3, to = 27, by = 3)\n\n[1]  3  6  9 12 15 18 21 24 27",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "05_matrices.html#operators",
    "href": "05_matrices.html#operators",
    "title": "5  Matrices",
    "section": "5.2 Operators",
    "text": "5.2 Operators\n\n5.2.1 Summation\nThe summation operator \\(\\sum\\) (i.e., the uppercase Sigma letter) lets us perform an operation on a sequence of numbers, which is often but not always a vector.\n\\[\\overrightarrow d = \\begin{bmatrix}\n12 & 7 & -2 & 3 & -1\n\\end{bmatrix}\\]\nWe can then calculate the sum of the first three elements of the vector, which is expressed as follows: \\[\\sum_{i=1}^3 d_i\\]\nThen we do the following math: \\[12+7+(-2)=17\\]\nIt is also common to use \\(n\\) in the superscript to indicate that we want to sum all elements:\n\\[\n\\sum_{i=1}^n d_i = 12 + 7 + (-2) + 3 + (-1) = 19\n\\] We can perform these operations using the sum() function in R:\n\nvector_d &lt;- c(12, 7, -2, 3, -1)\n\n\nsum(vector_d[1:3])\n\n[1] 17\n\n\n\nsum(vector_d)\n\n[1] 19\n\n\n\n\n5.2.2 Product\nThe product operator \\(\\prod\\) (i.e., the uppercase Pi letter) can also perform operations over a sequence of elements in a vector. Recall our previous vector:\n\\[\\overrightarrow d = \\begin{bmatrix}\n12 & 7 & -2 & 3 & 1\n\\end{bmatrix}\\]\nWe might want to calculate the product of all its elements, which is expressed as follows: \\[\\prod_{i=1}^n d_i = 12 \\cdot 7 \\cdot (-2) \\cdot 3 \\cdot (-1) = 504\\]\nIn R, we can compute products using the prod() function:\n\nprod(vector_d)\n\n[1] 504\n\n\n\n\n\n\n\n\nExercise\n\n\n\nGet the product of the first three elements of vector \\(d\\). Write the notation by hand and use R to obtain the number.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "05_matrices.html#matrices",
    "href": "05_matrices.html#matrices",
    "title": "5  Matrices",
    "section": "5.3 Matrices",
    "text": "5.3 Matrices\n\n5.3.1 Basics\nWe can append vectors together to form a matrix:\n\\[A = \\begin{bmatrix}\n12 & 14 & 15 \\\\\n115 & 22 & 127 \\\\\n193 & 29 & 219\n\\end{bmatrix}\\]\nThe number of rows and columns of a matrix constitute the dimensions of the matrix. The first number is the number of rows (“r”) and the second number is the number of columns (“c”) in the matrix.\n\n\n\n\n\n\nImportant\n\n\n\nFind a way to remember “r x c” permanently. The order of the dimensions never changes.\n\n\nMatrix \\(A\\) above, for example, is a \\(3 \\times 3\\) matrix. Sometimes we’d refer to it as \\(A_{3 \\times 3}\\).\n\n\n\n\n\n\nTip\n\n\n\nIt is common to use capital letters (sometimes bold-faced) to represent matrices. In contrast, vectors are usually represented with either bold lowercase letters or lowercase letters with an arrow on top (e.g., \\(\\overrightarrow v\\)).\n\n\n\nConstructing matrices in R\nThere are different ways to create matrices in R. One of the simplest is via rbind() or cbind(), which paste vectors together (either by rows or by columns):\n\n# Create some vectors\nvector1 &lt;- 1:4\nvector2 &lt;- 5:8\nvector3 &lt;- 9:12\nvector4 &lt;- 13:16\n\n\n# Using rbind(), each vector will be a row \nrbind_mat &lt;- rbind(vector1, vector2, vector3, vector4)\nrbind_mat\n\n        [,1] [,2] [,3] [,4]\nvector1    1    2    3    4\nvector2    5    6    7    8\nvector3    9   10   11   12\nvector4   13   14   15   16\n\n\n\n# Using cbind(), each vector will be a column\ncbind_mat &lt;- cbind(vector1, vector2, vector3, vector4)\ncbind_mat\n\n     vector1 vector2 vector3 vector4\n[1,]       1       5       9      13\n[2,]       2       6      10      14\n[3,]       3       7      11      15\n[4,]       4       8      12      16\n\n\nAn alternative is to use to properly named matrix() function. The basic syntax is matrix(data, nrow, ncol, byrow):\n\ndata is the input vector which becomes the data elements of the matrix.\nnrow is the number of rows to be created.\nncol is the number of columns to be created.\nbyrow is a logical clue. If TRUE then the input vector elements are arranged by row. By default (FALSE), elements are arranged by column.\n\nLet’s see some examples:\n\n# Elements are arranged sequentially by row.\nM &lt;- matrix(c(1:12), nrow = 4, byrow = T)\nM\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n[4,]   10   11   12\n\n\n\n# Elements are arranged sequentially by column (byrow = F by default).\nN &lt;- matrix(c(1:12), nrow = 4)\nN\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n\n\n\n\n5.3.2 Structure\nHow do we refer to specific elements of the matrix? For example, matrix \\(A\\) is an \\(m\\times n\\) matrix where \\(m=n=3\\). This is sometimes called a square matrix.\nMore generally, matrix \\(B\\) is an \\(m\\times n\\) matrix where the elements look like this: \\[B=\n\\begin{bmatrix}\nb_{11} & b_{12} & b_{13} & \\ldots & b_{1n} \\\\\nb_{21} & b_{22} & b_{23} & \\ldots & b_{2n} \\\\\n\\vdots & \\vdots & \\vdots & \\ldots & \\vdots \\\\\nb_{m1} & b_{m2} & b_{m3} & \\ldots & b_{mn}\n\\end{bmatrix}\\]\nThus \\(b_{23}\\) refers to the second unit down and third across. More generally, we refer to row indices as \\(i\\) and to column indices as \\(j\\).\nIn R, we can access a matrix’s elements using square brackets:\n\n# In matrix N, access the element at 1st row and 3rd column.\nN[1,3]\n\n[1] 9\n\n\n\n# In matrix N, access the element at 4th row and 2nd column.\nN[4,2]\n\n[1] 8\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen trying to identify a specific element, the first subscript is the element’s row and the second subscript is the element’s column (always in that order).\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn R, indexing is 1-based, meaning that the first element of a vector, matrix, or any other data structure is accessed with index 1. In other programming tools such as Python, indexing is 0-based, meaning that the first element of a list, array, or any other data structure is accessed with index 0.\n\n# Create a 2x2 matrix in R\nmatrix_A &lt;- matrix(1:4, nrow = 2, ncol = 2)\n\n# Access the element in the first row, first column\nelement &lt;- matrix_A[1, 1]  # This will return 1\n\n# Create a list in Python\nvector = [10, 20, 30, 40]\n\n# Access the first element\nfirst_element = vector[0]  # This will return 10\nimport numpy as np\n\n# Create a 2x2 matrix in Python with NumPy\nmatrix_A = np.array([[1, 2], [3, 4]])\n\n# Access the element in the first row, first column\nelement = matrix_A[0, 0]  # This will return 1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "05_matrices.html#matrix-operations",
    "href": "05_matrices.html#matrix-operations",
    "title": "5  Matrices",
    "section": "5.4 Matrix operations",
    "text": "5.4 Matrix operations\n\n5.4.1 Addition and subtraction\n\nAddition and subtraction are straightforward operations.\nMatrices must have exactly the same dimensions for both of these operations.\nWe add or subtract each element with the corresponding element from the other matrix.\nThis is expressed as follows:\n\n\\[A \\pm B=C\\]\n\\[c_{ij}=a_{ij} \\pm b_{ij} \\text{ }\\forall i,j\\]\n\\[\\begin{bmatrix}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix}\n\\pm\n\\begin{bmatrix}\nb_{11} & b_{12} & b_{13}\\\\\nb_{21} & b_{22} & b_{23}\\\\\nb_{31} & b_{32} & b_{33}\n\\end{bmatrix}\\] \\[=\\] \\[\\begin{bmatrix}\na_{11}\\pm b_{11} & a_{12}\\pm b_{12} & a_{13}\\pm b_{13}\\\\\na_{21}\\pm b_{21} & a_{22}\\pm b_{22} & a_{23}\\pm b_{23}\\\\\na_{31}\\pm b_{31} & a_{32}\\pm b_{32} & a_{33}\\pm b_{33}\n\\end{bmatrix}\\]\n\nAddition and subtraction in R\nWe start by creating two 2x3 matrices:\n\\[A= \\begin{bmatrix}\n3 & -1 & 2  \\\\\n9 & 4 & 6\n\\end{bmatrix}\\]\n\n# Create two 2x3 matrices.\nmatrix1 &lt;- matrix(c(3, 9, -1, 4, 2, 6), nrow = 2)\nmatrix1\n\n     [,1] [,2] [,3]\n[1,]    3   -1    2\n[2,]    9    4    6\n\n\nAnd \\[B= \\begin{bmatrix}\n5 & 2 & 0  \\\\\n9 & 3 & 4\n\\end{bmatrix}\\]\n\nmatrix2 &lt;- matrix(c(5, 2, 0, 9, 3, 4), nrow = 2)\nmatrix2\n\n     [,1] [,2] [,3]\n[1,]    5    0    3\n[2,]    2    9    4\n\n\nWe can simply use the + and - operators for addition and substraction:\n\nmatrix1 + matrix2\n\n     [,1] [,2] [,3]\n[1,]    8   -1    5\n[2,]   11   13   10\n\n\n\nmatrix1 - matrix2\n\n     [,1] [,2] [,3]\n[1,]   -2   -1   -1\n[2,]    7   -5    2\n\n\n\n\n\n\n\n\nExercise\n\n\n\n(Use code for one of these and do the other one by hand!)\n1) Calculate \\(A + B\\)\n\\[A= \\begin{bmatrix}\n1 & 0 \\\\\n-2 & -1\n\\end{bmatrix}\\]\n\\[B = \\begin{bmatrix}\n5 & 1 \\\\\n2 & -1\n\\end{bmatrix}\\]\n\n2) Calculate \\(A - B\\)\n\\[A= \\begin{bmatrix}\n6 & -2 & 8 & 12 \\\\\n4 & 42 & 8 & -6\n\\end{bmatrix}\\]\n\\[B = \\begin{bmatrix}\n18 & 42 & 3 & 7 \\\\\n0 & -42 & 15 & 4\n\\end{bmatrix}\\]\n\n\n\n\n\n5.4.2 Scalar multiplication\nScalar multiplication is very intuitive. As we know, a scalar is a single number. We multiply each value in the matrix by the scalar to perform this operation.\nFormally, this is expressed as follows: \\[A =\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix}\\] \\[cA =\n\\begin{bmatrix}\nca_{11} & ca_{12} & ca_{13}\\\\\nca_{21} & ca_{22} & ca_{23}\\\\\nca_{31} & ca_{32} & ca_{33}\n\\end{bmatrix}\\]\nIn R, all we need to do is take an established matrix and multiply it by some scalar:\n\n# matrix1 from our previous example\nmatrix1\n\n     [,1] [,2] [,3]\n[1,]    3   -1    2\n[2,]    9    4    6\n\n\n\nmatrix1 * 3\n\n     [,1] [,2] [,3]\n[1,]    9   -3    6\n[2,]   27   12   18\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCalculate \\(2\\times A\\) and \\(-3 \\times B\\). Again, do one by hand and the other one using R.\n\\[A= \\begin{bmatrix}\n    1 & 4 & 8 \\\\\n    0 & -1 & 3\n    \\end{bmatrix}\\] \\[ B = \\begin{bmatrix}\n    -15 & 1 & 5 \\\\\n    2 & -42 & 0 \\\\\n    7 & 1 & 6\n    \\end{bmatrix}\\]\n\n\n\n\n5.4.3 Matrix multiplication\n\nMultiplying matrices is slightly trickier than multiplying scalars.\nTwo matrices must be conformable for them to be multiplied together. This means that the number of columns in the first matrix equals the number of rows in the second.\nWhen multiplying \\(A \\times B\\), if \\(A\\) is \\(m \\times n\\), \\(B\\) must have \\(n\\) rows.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe conformability requirement never changes. Before multiplying anything, check to make sure the matrices are indeed conformable.\n\n\n\nThe resulting matrix will have the same number of rows as the first matrix and the number of columns in the second. For example, if \\(A\\) is \\(i \\times k\\) and \\(B\\) is \\(k \\times j\\), then \\(A \\times B\\) will be \\(i \\times j\\).\n\n\nWhich of the following can we multiply? What will be the dimensions of the resulting matrix? \\[\\begin{aligned}\nB_{4 \\times 1}=\n\\begin{bmatrix}\n2 \\\\\n3\\\\\n4\\\\\n1\n\\end{bmatrix}\nM_{3 \\times 3} =\n\\begin{bmatrix}\n1 & 0 & 2\\\\\n1 & 2 & 4\\\\\n2 & 3 & 2\n\\end{bmatrix}\nL_{2 \\times 3} =\n\\begin{bmatrix}\n6 & 5 & -1\\\\\n1 & 4 & 3\n\\end{bmatrix}\n\\end{aligned}\\]\nThe only valid multiplication based on the provided matrices is \\(L \\times M\\), which results in a \\(2 \\times 3\\) matrix.\nWhy can’t we multiply in the opposite order?\nThe non-commutative property of matrix multiplication is a fundamental aspect in matrix algebra. The multiplication of matrices is sensitive to the order in which the matrices are multiplied due to the requirements of dimensional compatibility, the resulting dimensions, and the computation process itself.\n\n\n\n\n\n\nWarning\n\n\n\nWhen multiplying matrices, order matters. Even if multiplication is possible in both directions, in general \\(AB \\neq BA\\).\n\n\n\nMultiplication steps\n\nMultiply each row by each column, summing up each pair of multiplied terms.\n\n\n\n\n\n\n\nTip\n\n\n\nThis is sometimes to referred to as the “dot product,” where we multiply matching members, then sum up.\n\n\n\nThe element in position \\(ij\\) is the sum of the products of elements in the \\(i\\)th row of the first matrix (\\(A\\)) and the corresponding elements in the \\(j\\)th column of the second matrix (\\(B\\)). \\[c_{ij}=\\sum_{k=1}^n a_{ik}b_{kj}\\]\n\n\n\nExample\nSuppose a company manufactures two kinds of furniture: chairs and sofas.\n\nA chair costs $100 for wood, $270 for cloth, and $130 for feathers.\nEach sofa costs $150 for wood, $420 for cloth, and $195 for feathers.\n\n\n\n\n\nChair\nSofa\n\n\n\n\nWood\n100\n150\n\n\nCloth\n270\n420\n\n\nFeathers\n130\n195\n\n\n\nThe same information about unit cost (\\(C\\)) can be presented as a matrix.\n\\[C = \\begin{bmatrix}\n100 & 150\\\\\n270 & 420\\\\\n130 & 195\n\\end{bmatrix}\\]\nNote that each of the three rows of this 3 x 2 matrix represents a material (wood, cloth, or feathers), and each of the two columns represents a product (chair or coach). The elements are the unit cost (in USD).\n\nNow, suppose that the company will produce 45 chairs and 30 sofas this month. This production quantity can be represented in the following table, and also as a 2 x 1 matrix (\\(Q\\)):\n\n\n\nProduct\nQuantity\n\n\n\n\nChair\n45\n\n\nSofa\n30\n\n\n\n\\[Q = \\begin{bmatrix}\n45 \\\\\n30\n\\end{bmatrix}\\]\nWhat will be the company’s total cost? The “total expenditure” is equal to the “unit cost” times the “production quantity” (the number of units).\nThe total expenditure (\\(E\\)) for each material this month is calculated by multiplying these two matrices.\n\\[\\begin{aligned} E = CQ =\n\\begin{bmatrix}\n100 & 150\\\\\n270 & 420\\\\\n130 & 195\n\\end{bmatrix}\n\\begin{bmatrix}\n45 \\\\\n30\n\\end{bmatrix} =\n\\begin{bmatrix}\n(100)(45) + (150)(30) \\\\\n(270)(45) + (420)(30) \\\\\n(130)(45) + (195)(30)\n\\end{bmatrix} =\n\\begin{bmatrix}\n9,000 \\\\\n24,750 \\\\\n11,700\n\\end{bmatrix}\n\\end{aligned}\\]\nMultiplying the 3x2 Cost matrix (\\(C\\)) times the 2x1 Quantity matrix (\\(Q\\)) yields the 3x1 Expenditure matrix (\\(E\\)).\nAs a result of this matrix multiplication, we determine that this month the company will incur expenditures of:\n\n$9,000 for wood\n$24,750 for cloth\n$11,700 for feathers.\n\n\n\nMatrix multiplication in R\nBefore attempting matrix multiplication, we must make sure the matrices are conformable (as we do for our manual calculations).\nThen we can multiply our matrices together using the %*% operator.\n\nC &lt;- matrix(c(100, 270, 130, 150, 420, 195), nrow = 3)\nC\n\n     [,1] [,2]\n[1,]  100  150\n[2,]  270  420\n[3,]  130  195\n\n\n\nQ &lt;- matrix(c(45, 30), nrow = 2)\nQ\n\n     [,1]\n[1,]   45\n[2,]   30\n\n\n\nC %*% Q\n\n      [,1]\n[1,]  9000\n[2,] 24750\n[3,] 11700\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you have a missing value or NA in one of the matrices you are trying to multiply (something we will discuss in further detail in the next module), you will have NAs in your resulting matrix.\n\n\n\n\n\n\n5.4.4 Properties of operations\n\nAddition and subtraction:\n\nAssociative: \\((A \\pm B) \\pm C = A \\pm (B \\pm C)\\)\nCommunicative: \\(A \\pm B = B \\pm A\\)\n\nMultiplication:\n\n\\(AB \\neq BA\\)\n\\(A(BC) = (AB)C\\)\n\\(A(B+C) = AB + AC\\)\n\\((A+B)C = AC + BC\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "05_matrices.html#special-matrices",
    "href": "05_matrices.html#special-matrices",
    "title": "5  Matrices",
    "section": "5.5 Special matrices",
    "text": "5.5 Special matrices\nSquare matrix\n\\[\nA = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix}\n\\]\n\nIn a square matrix, the number of rows equals the number of columns (\\(m=n\\)):\nThe diagonal of a matrix is a set of numbers consisting of the elements on the line from the upper-left-hand to the lower-right-hand corner of the matrix, as in $ d(A)=[1,5,9] $. Diagonals are particularly useful in square matrices.\nThe trace of a matrix, denoted as \\(tr(A)\\), is the sum of the diagonal elements of the matrix. \\(tr(A) = 1+5+9 = 15\\)\n\nDiagonal matrix:\n\nIn a diagonal matrix, all of the elements of the matrix that are not on the diagonal are equal to zero. \\[\nD = \\begin{bmatrix}\n4 & 0 & 0 \\\\\n0 & 5 & 0 \\\\\n0 & 0 & 6\n\\end{bmatrix}\n\\]\n\nScalar matrix:\n\nA scalar matrix is a diagonal matrix where the diagonal elements are all equal to each other. In other words, we’re really only concerned with one scalar (or element) held in the diagonal.\n\n\\[\nS = \\begin{bmatrix}\n7 & 0 & 0 \\\\\n0 & 7 & 0 \\\\\n0 & 0 & 7\n\\end{bmatrix}\n\\]\nIdentity matrix:\n\\[\nI = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\n\nThe identity matrix is a scalar matrix with all of the diagonal elements equal to one.\nRemember that, as with all diagonal matrices, the off-diagonal elements are equal to zero.\nThe capital letter \\(I\\) is reserved for the identity matrix. For convenience, a 3x3 identity matrix can be denoted as \\(I_3\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "05_matrices.html#transpose",
    "href": "05_matrices.html#transpose",
    "title": "5  Matrices",
    "section": "5.6 Transpose",
    "text": "5.6 Transpose\nThe transpose is the original matrix with the rows and the columns interchanged.\nThe notation is either \\(J'\\) (“J prime”) or \\(J^T\\) (“J transpose”).\n\\[J =\n\\begin{bmatrix}\n4 & 5\\\\\n3 & 0\\\\\n7 & -2\n\\end{bmatrix}\\]\n\\[J' = J^T =\n\\begin{bmatrix}\n4 & 3 & 7 \\\\\n5 & 0 & -2\n\\end{bmatrix}\\]\nIn R, we use t() to get the transpose.\n\nJ &lt;- matrix(c(4, 3, 7, 5, 0, -2), ncol = 2)\nJ\n\n     [,1] [,2]\n[1,]    4    5\n[2,]    3    0\n[3,]    7   -2\n\n\n\nt(J)\n\n     [,1] [,2] [,3]\n[1,]    4    3    7\n[2,]    5    0   -2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "05_matrices.html#inverse",
    "href": "05_matrices.html#inverse",
    "title": "5  Matrices",
    "section": "5.7 Inverse",
    "text": "5.7 Inverse\n\nJust like a number has a reciprocal, a matrix has an inverse.\nWhen we multiply a matrix by its inverse we get the identity matrix (which is like “1” for matrices).\n\n\\[A × A^{-1} = I\\]\n\nThe inverse of A is \\(A^{-1}\\) only when:\n\n\\[AA^{-1} = A^{-1}A = I\\]\n\nSometimes there is no inverse at all.\n\n\n\n\n\n\n\nNote\n\n\n\nFor now, don’t worry about calculating the inverse of a matrix manually. This is the type of task we use R for.\n\n\n\nIn R, we use the solve() function to calculate the inverse of a matrix:\n\n\nA &lt;- matrix(c(3, 2, 5, 2, 3, 2, 5, 2, 4), ncol = 3)\nA\n\n     [,1] [,2] [,3]\n[1,]    3    2    5\n[2,]    2    3    2\n[3,]    5    2    4\n\n\n\nsolve(A)\n\n            [,1]        [,2]       [,3]\n[1,] -0.29629630 -0.07407407  0.4074074\n[2,] -0.07407407  0.48148148 -0.1481481\n[3,]  0.40740741 -0.14814815 -0.1851852",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "05_matrices.html#linear-systems-and-matrices",
    "href": "05_matrices.html#linear-systems-and-matrices",
    "title": "5  Matrices",
    "section": "5.8 Linear systems and matrices",
    "text": "5.8 Linear systems and matrices\n\nA system of equations can be represented by an augmented matrix.\nSystem of equations: \\[{\\color{red}{3}}x + {\\color{green}{6}}y = {\\color{blue}{12}}\\] \\[{\\color{red}{5}}x + {\\color{green}{10}}y = {\\color{blue}{25}}\\]\nIn an augmented matrix, each row represents one equation in the system and each column represents a variable or the constant terms. \\[\\begin{bmatrix}\n{\\color{red}{3}} & {\\color{green}{6}} & {\\color{blue}{12}}\\\\\n{\\color{red}{5}} & {\\color{green}{10}} & {\\color{blue}{25}}\n\\end{bmatrix}\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "05_matrices.html#ols-and-matrices",
    "href": "05_matrices.html#ols-and-matrices",
    "title": "5  Matrices",
    "section": "5.9 OLS and matrices",
    "text": "5.9 OLS and matrices\n\nWe can use the logic above to calculate estimates for our ordinary least squares (OLS) models.\nOLS is a linear regression technique used to find the best-fitting line for a set of data points (observations) by minimizing the residuals (the differences between the observed and predicted values).\nWe minimize the sum of the squared errors.\n\n\n5.9.1 Dependent variable\n\nSuppose, for example, we have a sample consisting of \\(n\\) observations.\nThe dependent variable is denoted as an \\(n \\times1\\) column vector.\n\n\\[Y = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\ny_3 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\\]\n\n\n5.9.2 Independent variables\n\nSuppose there are \\(k\\) independent variables and a constant term, meaning \\(k+1\\) columns and \\(n\\) rows.\nWe can represent these variables as an \\(n \\times (k+1)\\) matrix, expressed as follows:\n\n\\[X= \\begin{bmatrix}\n1 & x_{11} & \\dots & x_{1k} \\\\\n1 & x_{21} & \\dots & x_{2k} \\\\\n\\vdots & \\vdots & \\dots & \\vdots \\\\\n1 & x_{n1} & \\dots & x_{nk}\n\\end{bmatrix}\\]\n\n\\(x_{ij}\\) is the \\(i\\)-th observation of the \\(j\\)-th independent variable.\n\n\n\n5.9.3 Linear regression model\n\nLet’s say we have 173 observations (n = 173) and 2 IVs (k = 3).\nThis can be expressed as the following linear equation: \\[y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\epsilon\\]\nIn matrix form, we have: \\[\\begin{aligned} \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} = \\begin{bmatrix}\n1 & x_{11} & x_{21} \\\\\n1 & x_{21} & x_{22} \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & x_{1173} & x_{2173}\n\\end{bmatrix} \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2\n\\end{bmatrix} + \\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\vdots \\\\\n\\epsilon_{173}\n\\end{bmatrix}\\end{aligned} \\]\nAll 173 equations can be represented by: \\[y=X\\beta+\\epsilon\\]\n\n\n\n5.9.4 Estimates\n\nWithout getting too much into the mechanics, we can calculate our coefficient estimates with matrix algebra using the following equation:\n\n\\[\\hat{\\beta} = (X'X)^{-1}X'Y\\]\n\nRead aloud, we say “X prime X inverse, X prime Y”.\nThe little hat on our beta (\\(\\hat{\\beta}\\)) signifies that these are estimates.\nRemember, the OLS method is to choose \\(\\hat{\\beta}\\) such that the sum of squared residuals (“SSR”) is minimized.\n\n\n5.9.4.1 Example in R\n\nWe will load the mtcars data set (our favorite) for this example, which contains data about many different car models.\n\n\ncars_df &lt;- mtcars\n\n\nNow, we want to estimate the association between hp (horsepower) and wt (weight), our independent variables, and mpg (miles per gallon), our dependent variable.\nFirst, we transform our dependent variable into a matrix, using the as.matrix function and specifying the column of the mtcars data set to create a column vector of our observed values for the DV.\n\n\nY &lt;- as.matrix(cars_df$mpg)\nY\n\n      [,1]\n [1,] 21.0\n [2,] 21.0\n [3,] 22.8\n [4,] 21.4\n [5,] 18.7\n [6,] 18.1\n [7,] 14.3\n [8,] 24.4\n [9,] 22.8\n[10,] 19.2\n[11,] 17.8\n[12,] 16.4\n[13,] 17.3\n[14,] 15.2\n[15,] 10.4\n[16,] 10.4\n[17,] 14.7\n[18,] 32.4\n[19,] 30.4\n[20,] 33.9\n[21,] 21.5\n[22,] 15.5\n[23,] 15.2\n[24,] 13.3\n[25,] 19.2\n[26,] 27.3\n[27,] 26.0\n[28,] 30.4\n[29,] 15.8\n[30,] 19.7\n[31,] 15.0\n[32,] 21.4\n\n\n\nNext, we do the same thing for our independent variables of interest, and our constant.\n\n\n# create two separate matrices for IVs\nX1 &lt;- as.matrix(cars_df$hp)\nX2 &lt;- as.matrix(cars_df$wt)\n\n# create constant column\n\n# bind them altogether into one matrix\nconstant &lt;-  rep(1, nrow(cars_df))\nX &lt;- cbind(constant, X1, X2)\nX\n\n      constant          \n [1,]        1 110 2.620\n [2,]        1 110 2.875\n [3,]        1  93 2.320\n [4,]        1 110 3.215\n [5,]        1 175 3.440\n [6,]        1 105 3.460\n [7,]        1 245 3.570\n [8,]        1  62 3.190\n [9,]        1  95 3.150\n[10,]        1 123 3.440\n[11,]        1 123 3.440\n[12,]        1 180 4.070\n[13,]        1 180 3.730\n[14,]        1 180 3.780\n[15,]        1 205 5.250\n[16,]        1 215 5.424\n[17,]        1 230 5.345\n[18,]        1  66 2.200\n[19,]        1  52 1.615\n[20,]        1  65 1.835\n[21,]        1  97 2.465\n[22,]        1 150 3.520\n[23,]        1 150 3.435\n[24,]        1 245 3.840\n[25,]        1 175 3.845\n[26,]        1  66 1.935\n[27,]        1  91 2.140\n[28,]        1 113 1.513\n[29,]        1 264 3.170\n[30,]        1 175 2.770\n[31,]        1 335 3.570\n[32,]        1 109 2.780\n\n\n\nNext, we calculate \\(X'X\\), \\(X'Y\\), and \\((X'X)^{-1}\\).\n\n\nDon’t forget to use %*% for matrix multiplication!\n\n\n# X prime X\nXpX &lt;- t(X) %*% X\n\n# X prime X inverse\nXpXinv &lt;- solve(XpX)\n\n# X prime Y\nXpY &lt;- t(X) %*% Y\n\n# beta coefficient estimates\nbhat &lt;- XpXinv %*% XpY\nbhat\n\n                [,1]\nconstant 37.22727012\n         -0.03177295\n         -3.87783074\n\n\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWhittinghill, Dexter C, and Robert V Hogg. 2001. “A Little Uniform Density with Big Instructional Potential.” Journal of Statistics Education 9 (2).\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "06_tidy_data2.html",
    "href": "06_tidy_data2.html",
    "title": "6  Tidy data analysis II",
    "section": "",
    "text": "6.1 Loading data in different formats.\nIn this session, we’ll cover a few more advanced topics related to data wrangling. Again we’ll use the tidyverse:\nIn this module we will use cross-national data from the Quality of Government (QoG) project (Dahlberg et al., 2023).\nNotice how in the data/ folder we have multiple versions of the same dataset (a subset of the QOG basic dataset): .csv (comma-separated values), .rds (R), .xlsx (Excel), .dta (Stata), and .sav (SPSS).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tidy data analysis II</span>"
    ]
  },
  {
    "objectID": "06_tidy_data2.html#loading-data-in-different-formats.",
    "href": "06_tidy_data2.html#loading-data-in-different-formats.",
    "title": "6  Tidy data analysis II",
    "section": "",
    "text": "6.1.1 CSV and R data files\nWe can use the read_csv() and read_rds() functions from the tidyverse1 to read the .csv and .rds (R) data files:\n\nqog_csv &lt;- read_csv(\"data/sample_qog_bas_ts_jan23.csv\")\n\nRows: 1085 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): cname, ccodealp, region, ht_colonial\ndbl (4): year, wdi_pop, vdem_polyarchy, vdem_corr\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nqog_rds &lt;- read_rds(\"data/sample_qog_bas_ts_jan23.rds\")\n\nFor reading files from other software (Excel, Stata, or SPSS), we need to load additional packages. Luckily, they are automatically installed when one installs the tidyverse.\n\n\n6.1.2 Excel data files\nFor Excel files (.xls or .xlsx files), the readxl package has a handy read_excel() function.\n\nlibrary(readxl)\nqog_excel &lt;- read_excel(\"data/sample_qog_bas_ts_jan23.xlsx\")\n\n\n\n\n\n\n\nTip\n\n\n\nUseful arguments of the read_excel() function include sheet =, which reads particular sheets (specified via their positions or sheet names), and range =, which extracts a particular cell range (e.g., `A5:E25`).\n\n\n\n\n6.1.3 Stata and SPSS data files\nTo load files from Stata (.dta) or SPSS (.spss), one needs the haven package and its properly-named read_stata() and read_spss() functions:\n\nlibrary(haven)\nqog_stata &lt;- read_stata(\"data/sample_qog_bas_ts_jan23.dta\")\nqog_spss &lt;- read_spss(\"data/sample_qog_bas_ts_jan23.sav\")\n\n\n\n\n\n\n\nTip\n\n\n\nDatasets from Stata and SPSS can have additional properties, like variable labels and special types of missing values. To learn more about this, check out the “Labelled data” chapter from Danny Smith’s Survey Research Datasets and R (2020).\n\n\n\n\n6.1.4 Our data for this session\nWe will rename one of our objects to qog:\n\nqog &lt;- qog_csv\nqog\n\n# A tibble: 1,085 × 8\n   cname      ccodealp  year region wdi_pop vdem_polyarchy vdem_corr ht_colonial\n   &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      \n 1 Antigua a… ATG       1990 Carib…   63328             NA        NA British    \n 2 Antigua a… ATG       1991 Carib…   63634             NA        NA British    \n 3 Antigua a… ATG       1992 Carib…   64659             NA        NA British    \n 4 Antigua a… ATG       1993 Carib…   65834             NA        NA British    \n 5 Antigua a… ATG       1994 Carib…   67072             NA        NA British    \n 6 Antigua a… ATG       1995 Carib…   68398             NA        NA British    \n 7 Antigua a… ATG       1996 Carib…   69798             NA        NA British    \n 8 Antigua a… ATG       1997 Carib…   71218             NA        NA British    \n 9 Antigua a… ATG       1998 Carib…   72572             NA        NA British    \n10 Antigua a… ATG       1999 Carib…   73821             NA        NA British    \n# ℹ 1,075 more rows\n\n\nThis dataset is a small sample of QOG, which contains data for countries in the Americas from 1990 to 2020. The observational unit is thus country-year. You can access the full codebook online. The variables are as follows:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ncname\nCountry name\n\n\nccodealp\nCountry code (ISO-3 character convention)\n\n\nyear\nYear\n\n\nregion\nRegion (following legacy WDI convention). Added to QOG by us.\n\n\nwdi_pop\nTotal population, from the World Development Indicators\n\n\nvdem_polyarchy\nV-Dem’s polyarchy index (electoral democracy)\n\n\nvdem_corr\nV-Dem’s corruption index\n\n\nht_colonial\nFormer colonial ruler",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tidy data analysis II</span>"
    ]
  },
  {
    "objectID": "06_tidy_data2.html#recoding-variables",
    "href": "06_tidy_data2.html#recoding-variables",
    "title": "6  Tidy data analysis II",
    "section": "6.2 Recoding variables",
    "text": "6.2 Recoding variables\nTake a look at the ht_colonial variable. We can do a simple tabulation with count():\n\nqog |&gt; \n  count(ht_colonial)\n\n# A tibble: 6 × 2\n  ht_colonial         n\n  &lt;chr&gt;           &lt;int&gt;\n1 British           372\n2 Dutch              31\n3 French             31\n4 Never colonized    62\n5 Portuguese         31\n6 Spanish           558\n\n\n\n\n\n\n\n\nTip\n\n\n\nAnother common way to compute quick tabulations in R is with the table() function. Be aware that this takes a vector as the input:\n\ntable(qog$ht_colonial)\n\n\n        British           Dutch          French Never colonized      Portuguese \n            372              31              31              62              31 \n        Spanish \n            558 \n\n\n\n\nWe might want to recode this variable. For instance, we could create a dummy/binary variable for whether the country was a British colony. We can do this with if_else(), which works with logical conditions:\n\nqog |&gt; \n  # the arguments are condition, true (what to do if true), false\n  mutate(d_britishcol = if_else(ht_colonial == \"British\", 1, 0)) |&gt; \n  count(d_britishcol)\n\n# A tibble: 2 × 2\n  d_britishcol     n\n         &lt;dbl&gt; &lt;int&gt;\n1            0   713\n2            1   372\n\n\nInstead of a numeric classification (0 and 1), we could use characters:\n\nqog |&gt; \n  mutate(cat_britishcol = if_else(ht_colonial == \"British\", \"British\", \"Other\")) |&gt; \n  count(cat_britishcol)\n\n# A tibble: 2 × 2\n  cat_britishcol     n\n  &lt;chr&gt;          &lt;int&gt;\n1 British          372\n2 Other            713\n\n\nif_else() is great for binary recoding. But sometimes we want to create more than two categories. We can use case_when():\n\nqog |&gt; \n  # syntax is condition ~ value\n  mutate(cat_col = case_when(\n    ht_colonial == \"British\" ~ \"British\",\n    ht_colonial == \"Spanish\" ~ \"Spanish\", \n    .default = \"Other\" # what to do in all other cases\n  )) |&gt; \n  count(cat_col)\n\n# A tibble: 3 × 2\n  cat_col     n\n  &lt;chr&gt;   &lt;int&gt;\n1 British   372\n2 Other     155\n3 Spanish   558\n\n\nThe .default = argument in case_when() can also be used to leave the variable as-is for non-specified cases. For example, let’s combine Portuguese and Spanish colonies:\n\nqog |&gt; \n  # syntax is condition ~ value\n  mutate(cat_col = case_when(\n    ht_colonial %in% c(\"Spanish\", \"Portuguese\") ~ \"Spanish/Portuguese\",\n    .default = ht_colonial # what to do in all other cases\n  )) |&gt; \n  count(cat_col)\n\n# A tibble: 5 × 2\n  cat_col                n\n  &lt;chr&gt;              &lt;int&gt;\n1 British              372\n2 Dutch                 31\n3 French                31\n4 Never colonized       62\n5 Spanish/Portuguese   589\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nCreate a dummy variable, d_large_pop, for whether the country-year has a population of more than 1 million. Then compute its mean. Your code:\nWhich countries are recorded as “Never colonized”? Change their values to other reasonable codings and compute a tabulation with count(). Your code:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tidy data analysis II</span>"
    ]
  },
  {
    "objectID": "06_tidy_data2.html#missing-values",
    "href": "06_tidy_data2.html#missing-values",
    "title": "6  Tidy data analysis II",
    "section": "6.3 Missing values",
    "text": "6.3 Missing values\nMissing values are commonplace in real datasets. In R, missing values are a special type of value in vectors, denoted as NA.\n\n\n\n\n\n\nWarning\n\n\n\nThe special value NA is different from the character value “NA”. For example, notice that a numeric vector can have NAs, while it obviously cannot hold the character value “NA”:\n\nc(5, 4.6, NA, 8)\n\n[1] 5.0 4.6  NA 8.0\n\n\n\n\nA quick way to check for missing values in small datasets is with the summary() function:\n\nsummary(qog)\n\n    cname             ccodealp              year         region         \n Length:1085        Length:1085        Min.   :1990   Length:1085       \n Class :character   Class :character   1st Qu.:1997   Class :character  \n Mode  :character   Mode  :character   Median :2005   Mode  :character  \n                                       Mean   :2005                     \n                                       3rd Qu.:2013                     \n                                       Max.   :2020                     \n                                                                        \n    wdi_pop          vdem_polyarchy     vdem_corr      ht_colonial       \n Min.   :    40542   Min.   :0.0710   Min.   :0.0260   Length:1085       \n 1st Qu.:   389131   1st Qu.:0.5570   1st Qu.:0.1890   Class :character  \n Median :  5687744   Median :0.7030   Median :0.5550   Mode  :character  \n Mean   : 25004057   Mean   :0.6569   Mean   :0.4922                     \n 3rd Qu.: 16195902   3rd Qu.:0.8030   3rd Qu.:0.7540                     \n Max.   :331501080   Max.   :0.9160   Max.   :0.9630                     \n                     NA's   :248      NA's   :248                        \n\n\nNotice that we have missingness in the vdem_polyarchy and vdem_corr variables. We might want to filter the dataset to see which observations are in this situation:\n\nqog |&gt; \n  filter(vdem_polyarchy == NA | vdem_corr == NA)\n\n# A tibble: 0 × 8\n# ℹ 8 variables: cname &lt;chr&gt;, ccodealp &lt;chr&gt;, year &lt;dbl&gt;, region &lt;chr&gt;,\n#   wdi_pop &lt;dbl&gt;, vdem_polyarchy &lt;dbl&gt;, vdem_corr &lt;dbl&gt;, ht_colonial &lt;chr&gt;\n\n\nBut the code above doesn’t work! To refer to missing values in logical conditions, we cannot use == NA. Instead, we need to use the is.na() function:\n\nqog |&gt; \n  filter(is.na(vdem_polyarchy) | is.na(vdem_corr))\n\n# A tibble: 248 × 8\n   cname      ccodealp  year region wdi_pop vdem_polyarchy vdem_corr ht_colonial\n   &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      \n 1 Antigua a… ATG       1990 Carib…   63328             NA        NA British    \n 2 Antigua a… ATG       1991 Carib…   63634             NA        NA British    \n 3 Antigua a… ATG       1992 Carib…   64659             NA        NA British    \n 4 Antigua a… ATG       1993 Carib…   65834             NA        NA British    \n 5 Antigua a… ATG       1994 Carib…   67072             NA        NA British    \n 6 Antigua a… ATG       1995 Carib…   68398             NA        NA British    \n 7 Antigua a… ATG       1996 Carib…   69798             NA        NA British    \n 8 Antigua a… ATG       1997 Carib…   71218             NA        NA British    \n 9 Antigua a… ATG       1998 Carib…   72572             NA        NA British    \n10 Antigua a… ATG       1999 Carib…   73821             NA        NA British    \n# ℹ 238 more rows\n\n\nNotice that, in most R functions, missing values are “contagious.” This means that any missing value will contaminate the operation and carry over to the results. For example:\n\nqog |&gt; \n  summarize(mean_vdem_polyarchy = mean(vdem_polyarchy))\n\n# A tibble: 1 × 1\n  mean_vdem_polyarchy\n                &lt;dbl&gt;\n1                  NA\n\n\nSometimes we’d like to perform our operations even in the presence of missing values, simply excluding them. Most basic R functions have an na.rm = argument to do this:\n\nqog |&gt; \n  summarize(mean_vdem_polyarchy = mean(vdem_polyarchy, na.rm = T))\n\n# A tibble: 1 × 1\n  mean_vdem_polyarchy\n                &lt;dbl&gt;\n1               0.657\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCalculate the median value of the corruption variable for each region (i.e., perform a grouped summary). Your code:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tidy data analysis II</span>"
    ]
  },
  {
    "objectID": "06_tidy_data2.html#pivoting-data",
    "href": "06_tidy_data2.html#pivoting-data",
    "title": "6  Tidy data analysis II",
    "section": "6.4 Pivoting data",
    "text": "6.4 Pivoting data\nWe will now load another time-series cross-sectional dataset, but in a slightly different format. It’s adapted from the World Bank’s World Development Indicators (WDI) (2023) and records gross domestic product at purchasing power parity (GDP PPP).\n\ngdp &lt;- read_excel(\"data/wdi_gdp_ppp.xlsx\")\n\n\ngdp\n\n# A tibble: 266 × 35\n   country_name        country_code   `1990`   `1991`   `1992`   `1993`   `1994`\n   &lt;chr&gt;               &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 Aruba               ABW           2.03e 9  2.19e 9  2.32e 9  2.48e 9  2.69e 9\n 2 Africa Eastern and… AFE           9.41e11  9.42e11  9.23e11  9.19e11  9.35e11\n 3 Afghanistan         AFG          NA       NA       NA       NA       NA      \n 4 Africa Western and… AFW           5.76e11  5.84e11  5.98e11  5.92e11  5.91e11\n 5 Angola              AGO           6.85e10  6.92e10  6.52e10  4.95e10  5.02e10\n 6 Albania             ALB           1.59e10  1.14e10  1.06e10  1.16e10  1.26e10\n 7 Andorra             AND          NA       NA       NA       NA       NA      \n 8 Arab World          ARB           2.19e12  2.25e12  2.35e12  2.41e12  2.48e12\n 9 United Arab Emirat… ARE           2.01e11  2.03e11  2.10e11  2.12e11  2.27e11\n10 Argentina           ARG           4.61e11  5.04e11  5.43e11  5.88e11  6.22e11\n# ℹ 256 more rows\n# ℹ 28 more variables: `1995` &lt;dbl&gt;, `1996` &lt;dbl&gt;, `1997` &lt;dbl&gt;, `1998` &lt;dbl&gt;,\n#   `1999` &lt;dbl&gt;, `2000` &lt;dbl&gt;, `2001` &lt;dbl&gt;, `2002` &lt;dbl&gt;, `2003` &lt;dbl&gt;,\n#   `2004` &lt;dbl&gt;, `2005` &lt;dbl&gt;, `2006` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2008` &lt;dbl&gt;,\n#   `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;,\n#   `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;, `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;,\n#   `2019` &lt;dbl&gt;, `2020` &lt;dbl&gt;, `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;\n\n\nNote how the information is recorded differently. Here columns are not variables, but years. We call datasets like this one wide, in contrast to the long datasets we have seen before. In general, R and the tidyverse work much nicer with long datasets. Luckily, the tidyr package of the tidyverse makes it easy to convert datasets between these two formats.\n\n\n\nSource: Illustration by Allison Horst, adapted by Peter Higgins.\n\n\nWe will use the pivot_longer() function:\n\ngdp_long &lt;- gdp |&gt; \n  pivot_longer(cols = -c(country_name, country_code), # cols to not pivot\n               names_to = \"year\", # how to name the column with names\n               values_to = \"wdi_gdp_ppp\",  # how to name the column with values\n               names_transform = as.integer) # make sure that years are numeric\ngdp_long\n\n# A tibble: 8,778 × 4\n   country_name country_code  year wdi_gdp_ppp\n   &lt;chr&gt;        &lt;chr&gt;        &lt;int&gt;       &lt;dbl&gt;\n 1 Aruba        ABW           1990 2025472682.\n 2 Aruba        ABW           1991 2186758474.\n 3 Aruba        ABW           1992 2315391348.\n 4 Aruba        ABW           1993 2484593045.\n 5 Aruba        ABW           1994 2688426606.\n 6 Aruba        ABW           1995 2756904694.\n 7 Aruba        ABW           1996 2789595753.\n 8 Aruba        ABW           1997 2986175079.\n 9 Aruba        ABW           1998 3045659222.\n10 Aruba        ABW           1999 3083365758.\n# ℹ 8,768 more rows\n\n\nDone! This is a much friendlier format to work with. For example, we can now do summaries:\n\ngdp_long |&gt; \n  summarize(mean_gdp_ppp = mean(wdi_gdp_ppp, na.rm = T), .by = country_name)\n\n# A tibble: 266 × 2\n   country_name                mean_gdp_ppp\n   &lt;chr&gt;                              &lt;dbl&gt;\n 1 Aruba                            3.38e 9\n 2 Africa Eastern and Southern      1.61e12\n 3 Afghanistan                      5.56e10\n 4 Africa Western and Central       1.15e12\n 5 Angola                           1.38e11\n 6 Albania                          2.56e10\n 7 Andorra                        NaN      \n 8 Arab World                       4.22e12\n 9 United Arab Emirates             4.29e11\n10 Argentina                        8.06e11\n# ℹ 256 more rows\n\n\n\n\n\n\n\n\nExercise\n\n\n\nConvert back gdp_long to a wide format using pivot_wider(). Check out the help file using ?pivot_wider. Your code:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tidy data analysis II</span>"
    ]
  },
  {
    "objectID": "06_tidy_data2.html#merging-datasets",
    "href": "06_tidy_data2.html#merging-datasets",
    "title": "6  Tidy data analysis II",
    "section": "6.5 Merging datasets",
    "text": "6.5 Merging datasets\nIt is extremely common to want to integrate data from multiple sources. Combining information from two datasets is called merging or joining.\nTo do this, we need ID variables in common between the two data sets. Using our QOG and WDI datasets, these variables will be country code (which in this case is shared between the two datasets) and year.\n\n\n\n\n\n\nTip\n\n\n\nStandardized unit codes (like country codes) are extremely useful when merging data. It’s harder than expected for a computer to realize that “Bolivia (Plurinational State of)” and “Bolivia” refer to the same unit. By default, these units will not be matched.2\n\n\nOkay, now to the merging. Imagine we want to add information about GDP to our QOG main dataset. To do so, we can use the left_join() function, from the tidyverse’s dplyr package:\n\nqog_plus &lt;- left_join(qog, # left data frame, which serves as a \"base\"\n                      gdp_long, # right data frame, from which to draw new columns\n                      by = c(\"ccodealp\" = \"country_code\", # can define name equivalencies!\n                             \"year\"))\n\n\nqog_plus |&gt; \n  # select variables for clarity\n  select(cname, ccodealp, year, wdi_pop, wdi_gdp_ppp)\n\n# A tibble: 1,085 × 5\n   cname               ccodealp  year wdi_pop wdi_gdp_ppp\n   &lt;chr&gt;               &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n 1 Antigua and Barbuda ATG       1990   63328  966660878.\n 2 Antigua and Barbuda ATG       1991   63634  987701012.\n 3 Antigua and Barbuda ATG       1992   64659  999143284.\n 4 Antigua and Barbuda ATG       1993   65834 1051896837.\n 5 Antigua and Barbuda ATG       1994   67072 1122128908.\n 6 Antigua and Barbuda ATG       1995   68398 1073208718.\n 7 Antigua and Barbuda ATG       1996   69798 1144088355.\n 8 Antigua and Barbuda ATG       1997   71218 1206688391.\n 9 Antigua and Barbuda ATG       1998   72572 1263778328.\n10 Antigua and Barbuda ATG       1999   73821 1310634399.\n# ℹ 1,075 more rows\n\n\n\n\n\n\n\n\nTip\n\n\n\nMost of the time, you’ll want to do a left_join(), which is great for adding new information to a “base” dataset, without dropping information from the latter. In limited situations, other types of joins can be helpful. To learn more about them, you can read Jenny Bryan’s excellent tutorial on dplyr joins.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nThere is a dataset on country’s CO2 emissions, again from the World Bank (2023), in “data/wdi_co2.csv”. Load the dataset into R and add a new variable with its information, wdi_co2, to our qog_plus data frame. Finally, compute the average values of CO2 emissions per capita, by country. Tip: this exercise requires you to do many steps—plan ahead before you start coding! Your code:\n\n\n\n6.5.1 Sanity checks\nSanity checks are small tests to make sure that your code is doing what you think it’s doing. They are especially important in complex operations like joins, but the idea can be extended to pretty much any command.\nThe tidylog package gives more information about tidyverse operations, and it’s an easy/automatic way to check your work:\n\nlibrary(tidylog)\n\n\nqog_plus &lt;- left_join(qog, # left data frame, which serves as a \"base\"\n                      gdp_long, # right data frame, from which to draw new columns\n                      by = c(\"ccodealp\" = \"country_code\", # can define name equivalencies!\n                             \"year\"))\n\nleft_join: added 2 columns (country_name, wdi_gdp_ppp)\n           &gt; rows only in qog           0\n           &gt; rows only in gdp_long (7,693)\n           &gt; matched rows           1,085\n           &gt;                       =======\n           &gt; rows total             1,085\n\n\nYou can also construct sanity checks manually. For instance, we know that a left join shouldn’t modify a data frame’s number of rows:\n\nnrow(qog) == nrow(qog_plus)\n\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tidy data analysis II</span>"
    ]
  },
  {
    "objectID": "06_tidy_data2.html#plotting-extensions-trend-graphs-facets-and-customization",
    "href": "06_tidy_data2.html#plotting-extensions-trend-graphs-facets-and-customization",
    "title": "6  Tidy data analysis II",
    "section": "6.6 Plotting extensions: trend graphs, facets, and customization",
    "text": "6.6 Plotting extensions: trend graphs, facets, and customization\n\n\n\n\n\n\nExercise\n\n\n\nDraw a scatterplot with time in the x-axis and democracy scores in the y-axis. Your code:\n\n\nHow can we visualize trends effectively? One alternative is to use a trend graph. Let’s start by computing the yearly averages for democracy in the whole region:\n\ndem_yearly &lt;- qog |&gt; \n  summarize(mean_dem = mean(vdem_polyarchy, na.rm = T), .by = year)\n\nsummarize: now 31 rows and 2 columns, ungrouped\n\ndem_yearly\n\n# A tibble: 31 × 2\n    year mean_dem\n   &lt;dbl&gt;    &lt;dbl&gt;\n 1  1990    0.581\n 2  1991    0.600\n 3  1992    0.605\n 4  1993    0.620\n 5  1994    0.629\n 6  1995    0.642\n 7  1996    0.651\n 8  1997    0.657\n 9  1998    0.663\n10  1999    0.661\n# ℹ 21 more rows\n\n\nNow we can plot them with a scatterplot:\n\nggplot(dem_yearly, aes(x = year, y = mean_dem)) +\n  geom_point()\n\n\n\n\n\n\n\n\nWe can add geom_line() to connect the dots:\n\nggplot(dem_yearly, aes(x = year, y = mean_dem)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\n\nWe can, of course, remove to points to only keep the line:\n\nggplot(dem_yearly, aes(x = year, y = mean_dem)) +\n  geom_line()\n\n\n\n\n\n\n\n\nWhat if we want to plot trends for different countries? We can use the group and color aesthetic mappings (no need to do a summary here! data is already at the country-year level):\n\n# filter to only get Colombia and Venezuela\ndem_yearly_countries &lt;- qog |&gt; \n  filter(ccodealp %in% c(\"COL\", \"VEN\"))\n\nfilter: removed 1,023 rows (94%), 62 rows remaining\n\nggplot(dem_yearly_countries, aes(x = year, y = vdem_polyarchy, color = cname)) +\n  geom_line()\n\n\n\n\n\n\n\n\nRemember that we can use the labs() function to add labels:\n\nggplot(dem_yearly_countries, aes(x = year, y = vdem_polyarchy, color = cname)) +\n  geom_line() +\n  labs(x = \"Year\", y = \"V-Dem Electoral Democracy Score\", color = \"Country\", \n       title = \"Evolution of democracy scores in Colombia and Venezuela\",\n       caption = \"Source: V-Dem (Coppedge et al., 2022) in QOG dataset.\")\n\n\n\n\n\n\n\n\nAnother way to display these trends is by using facets, which divide a plot into small boxes according to a categorical variable (no need to add color here):\n\nggplot(dem_yearly_countries, aes(x = year, y = vdem_polyarchy)) +\n  geom_line() +\n  facet_wrap(~cname)\n\n\n\n\n\n\n\n\nFacets are particularly useful for many categories (where the number of distinguishable colors reaches its limit):\n\nggplot(qog |&gt; filter(region == \"South America\"), \n       aes(x = year, y = vdem_polyarchy)) +\n  geom_line() +\n  facet_wrap(~cname)\n\nfilter: removed 713 rows (66%), 372 rows remaining\n\n\n\n\n\n\n\n\n\nWith facets, one can control whether each facet picks its own scales or if all facets share the same scale. For example, let’s plot the populations of Canada and the US:\n\nggplot(qog |&gt; filter(cname %in% c(\"Canada\", \"United States\")), \n       aes(x = year, y = wdi_pop)) +\n  geom_line() +\n  facet_wrap(~cname)\n\nfilter: removed 1,023 rows (94%), 62 rows remaining\n\n\n\n\n\n\n\n\n\nThe scales are so disparate that unifying them yields a plot that’s hard to interpret. But if we’re interested in within-country trends, we can let each facet have its own scale with the scales = argument (which can be “fixed”, “free_x”, “free_y”, or “free”):\n\nggplot(qog |&gt; filter(cname %in% c(\"Canada\", \"United States\")), \n       aes(x = year, y = wdi_pop)) +\n  geom_line() +\n  facet_wrap(~cname, scales = \"free_y\")\n\nfilter: removed 1,023 rows (94%), 62 rows remaining\n\n\n\n\n\n\n\n\n\nThis ability to visualize within time trends also makes facets appealing in many situations.\n\n\n\n\n\n\nTip\n\n\n\nPlots made with ggplot2 are extremely customizable. For example, we could want to change the y-axis labels in the last plot to something more readable:\n\n# create as object \"p\" to use later\np &lt;- ggplot(qog |&gt; filter(cname %in% c(\"Canada\", \"United States\")), \n       aes(x = year, y = wdi_pop)) +\n  geom_line() +\n  facet_wrap(~cname, scales = \"free_y\") +\n  scale_y_continuous(labels = scales::label_number(big.mark = \",\")) +\n  # also add labels\n  labs(x = \"Year\", y = \"Population\",\n       title = \"Population trends in Canada and the United States\",\n       caption = \"Source: World Development Indicators (World Bank, 2023) in QOG dataset.\")\n\nfilter: removed 1,023 rows (94%), 62 rows remaining\n\np\n\n\n\n\n\n\n\n\nWhile it’s impossible for us to review all the customization options you might need, a fantastic reference is the “ggplot2: Elegant Graphics for Data Analysis” book by Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nUsing your merged dataset from the previous section, plot the trajectories of C02 per capita emissions for the US and Haiti. Use adequate scales.\n\n\n\n6.6.1 Themes\nWe can change the overall aspect of a ggplot2 figure by changing its theme:\n\np +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\np +\n  theme_classic()\n\n\n\n\n\n\n\n\n\np + \n  theme_bw()\n\n\n\n\n\n\n\n\nIf you are going to make multiple plots in a script, you can set the theme at the beginning with theme_bw():\n\ntheme_set(theme_bw())\np\n\n\n\n\n\n\n\n\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWhittinghill, Dexter C, and Robert V Hogg. 2001. “A Little Uniform Density with Big Instructional Potential.” Journal of Statistics Education 9 (2).\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tidy data analysis II</span>"
    ]
  },
  {
    "objectID": "06_tidy_data2.html#footnotes",
    "href": "06_tidy_data2.html#footnotes",
    "title": "6  Tidy data analysis II",
    "section": "",
    "text": "Technically, the read_csv() and read_rds() functions come from readr, one of the tidyverse constituent packages.↩︎\nThere are R packages to deal with these complications. fuzzyjoin matches units by their approximate distance, using some clever algorithms. countrycode allows one to standardize country names and country codes across different conventions.↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tidy data analysis II</span>"
    ]
  },
  {
    "objectID": "07_probability.html",
    "href": "07_probability.html",
    "title": "7  Probability",
    "section": "",
    "text": "7.1 What is probability?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "07_probability.html#what-is-probability",
    "href": "07_probability.html#what-is-probability",
    "title": "7  Probability",
    "section": "",
    "text": "Informally, a probability is a number that describes how likely an event is.\n\nIt is, by definition, between 0 and 1.\nWhat is the probability that a fair coin flip will result in heads?\n\nWe can also think of a probability as an outcome’s relative frequency after repeating an “experiment” many times.1\n\nIn this setting, an experiment is “an action or a set of actions that produce stochastic [random] events of interest” (Imai and Williams 2022, p. 281). Not to confuse with scientific experiments!\nIf we were to flip a million fair coins, what will be the proportion of heads?\n\n\n\n\nA probability space \\((\\Omega, S, P)\\) is a formal way to talk about a random process:\n\nThe sample space (\\(\\Omega\\)) is the set of all possible outcomes.\nThe event space (\\(S\\)) is a collection of events (an event is a subset of \\(\\Omega\\)).\nThe probability measure (\\(P\\)) is a function that assigns a probability in \\(\\mathbb{R}\\) to every event in \\(S\\). So \\(P: S \\rightarrow \\mathbb{R}\\).\n\nWe can formalize our intuitions with the probability axioms (sometimes called Kolmogorov’s axioms):\n\n\\(P(A) \\ge 0, \\; \\forall A \\in S\\).\n\nProbabilities must be non-negative.\n\n\\(P(\\Omega) = 1\\).\n\nSomething has to happen!\nProbabilities sum/integrate to 1.\n\n\\(P(A \\cup B) = P(A) + P(B), \\; \\forall A, B \\in S, \\; A\\cup B = \\emptyset\\).\n\nThe probability of disjoint (mutually exclusive) events is equal to the sum of their individual probabilities.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "07_probability.html#definitions-and-properties-of-probability",
    "href": "07_probability.html#definitions-and-properties-of-probability",
    "title": "7  Probability",
    "section": "7.2 Definitions and properties of probability",
    "text": "7.2 Definitions and properties of probability\n\nJoint probability: \\(P(A \\cap B)\\). The probability that the two events will occur in one realization of the experiment.\nLaw of total probability: \\(P(A) = P(A \\cap B) + P(A \\cap B^C)\\).\nAddition rule: \\(P(A \\cup B) = P(A) + P(B) - P (A \\cap B)\\).\nConditional probability: \\(\\displaystyle P(A|B)=\\frac{P(A \\cap B)}{P(B)}\\)\n\nthe probability of event \\(A\\) occurring given that event \\(B\\) has already occurred is the probability that both \\(A\\) and \\(B\\) occur together devided by the probability that event \\(B\\) occurs\n\nBayes theorem: \\(\\displaystyle P(A|B) = \\frac{P(A) \\cdot P(B|A)}{P(B)}\\)\n\n\\(P(A|B)\\): the probability of event A occurring given that B is true\n\\(P(B|A)\\): the probability of event B occurring given that A is true.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "07_probability.html#random-variables-and-probability-distributions",
    "href": "07_probability.html#random-variables-and-probability-distributions",
    "title": "7  Probability",
    "section": "7.3 Random variables and probability distributions",
    "text": "7.3 Random variables and probability distributions\n\nA random variable is a function (\\(X: \\Omega \\to \\mathbb{R}\\)) of the outcome of a random generative process. Informally, it is a “placeholder” for whatever will be the output of a process we’re studying.\nA probability distribution describes the probabilities associated with the values of a random variable.\nRandom variables (and probability distributions) can be discrete or continuous.\n\n\n7.3.1 Discrete random variables and probability distributions\n\nA sample space in which there are a (finite or infinite) countable number of outcomes\nEach realization of random process has a discrete probability of occurring.\n\n\\(f(X=x_i)=P(X=x_i)\\) is the probability the variable takes the value \\(x_i\\).\n\n\n\nAn example\n\nWhat’s the probability that we’ll roll a 3 on one die roll: \\[Pr(y=3) = \\dfrac{1}{6}\\]\nIf one roll of the die is an “experiment,” we can think of a 3 as a “success.”\n\\(Y \\sim Bernoulli \\left(\\frac{1}{6} \\right)\\)\nFair coins are \\(\\sim Bernoulli(.5)\\), for example.\nMore generally, \\(Bernoulli(\\pi )\\). We’ll talk about other probability distributions soon.\n\n\\(\\pi\\) represents the probability of success.\n\n\nLet’s do another example on the board, using the sum of two fair dice.\n\n\n\n\n\n\nExercise:\n\n\n\nWhat’s the probability that the sum of two fair dice equals 7?\n\n\n\n\n\n7.3.2 Continuous random variables and probability distributions\n\nWhat happens when our outcome is continuous?\nThere are an infinite number of outcomes. This makes the denominator of our fraction difficult to work with.\nThe probability of the whole space must equal 1.\nThe domain may not span -\\(\\infty\\) to \\(\\infty\\).\n\nEven space between 0 and 1 is infinite!\n\nTwo common examples are the uniform and normal probability distributions, which we will discuss below.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "07_probability.html#functions-describing-probability-distributions",
    "href": "07_probability.html#functions-describing-probability-distributions",
    "title": "7  Probability",
    "section": "7.4 Functions describing probability distributions",
    "text": "7.4 Functions describing probability distributions\n\n7.4.1 Probability Mass Function (PMF) – Discrete Variables\nProbability of each occurrence encoded in probability mass function (PMF)\n\n\\(0 \\leq f(x_i) \\leq 1\\): Probability of any value occurring must be between 0 and 1.\n\\(\\displaystyle\\sum_{x}f(x_i) = 1\\): Probabilities of all values must sum to 1.\n\n\n\n\n7.4.2 Probability Density Function (PDF) – Continuous Variables\n\nSimilar to PMF from before, but for continuous variables.\nUsing integration, it gives the probability a value falls within a particular interval\n\\[P[a\\le X\\le b] = \\displaystyle\\int_a^b f(x) \\, dx\\]\nTotal area under the curve is 1.\n\\(P(a &lt; X &lt; b)\\) is the area under the curve between \\(a\\) and \\(b\\) (where \\(b &gt; a\\)).\n\n\n\n\n\nBox plot and PDF of a normal distribution \\(N(0, \\sigma^2)\\)\n\n\nSource: Wikipedia Commons\n\n\n7.4.3 Cumulative Density Function (CDF)\n\nDiscrete\n\nCumulatve density function is probability X will take a value of x or lower.\nPDF is written \\(f(x)\\), and CDF is written \\(F'(x)\\). \\[F_X(x) = Pr(X\\leq x)\\]\nFor discrete CDFs, that means summing up over all values.\n\n\n\n\n\n\n\nExercise:\n\n\n\nWhat is the probability of rolling a 6 or lower with two dice? \\(F(6)=?\\)\n\n\n\n\nContinuous\n\nWe can’t sum probabilities for continuous distributions (remember the 0 problem).\nSolution: integration \\[F_Y(y) = \\int_{-\\infty}^{y} f(y) dy\\]\nExamples of uniform distribution.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "07_probability.html#common-types-of-probability-distributions",
    "href": "07_probability.html#common-types-of-probability-distributions",
    "title": "7  Probability",
    "section": "7.5 Common types of probability distributions",
    "text": "7.5 Common types of probability distributions\nThere are many useful probability distributions. In this section we will cover three of the most common ones: the binomial, uniform, and normal distributions.\n\n7.5.1 Binomial distribution\nA Binomial distribution is defined as follow: \\(X \\sim Binomial(n, p)\\)\nPMF:\n\\[\n{n \\choose k} p^k(1-p)^{n-k}\n\\] , where \\(n\\) is the number of trials, \\(p\\) is the probability of success, and \\(k\\) is the number of successes.\nRemember that:\n\\[\n{n \\choose k} = \\frac{n!}{k!(n-k)!}\n\\]\nFor example, let’s say that voters choose some candidate with probability 0.02. What is the probability of seeing exactly 0 voters of the candidate in a sample of 100 people?\nWe can compute the PMF of a binomial distribution using R’s dbinom() function.\n\ndbinom(x = 0, size = 100, prob = 0.02)\n\n[1] 0.1326196\n\n\n\ndbinom(x = 1, size = 100, prob = 0.02)\n\n[1] 0.2706522\n\n\nSimilarly, we can compute the CDF using R’s pbinom() function:\n\npbinom(q = 0, size = 100, prob = 0.02)\n\n[1] 0.1326196\n\n\n\npbinom(q = 100, size = 100, prob = 0.02)\n\n[1] 1\n\n\n\npbinom(q = 1, size = 100, prob = 0.02)\n\n[1] 0.4032717\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCompute the probability of seeing between 1 and 10 voters of the candidate in a sample of 100 people.\n\n\n\n\n7.5.2 Uniform distribution\nA uniform distribution has two parameters: a minimum and a maximum. So \\(X \\sim U(a, b)\\).\n\nPDF:\n\n\\[\n\\displaystyle{\n\\begin{cases}\n  \\frac{1}{b-a} & \\text{, }{x \\in [a, b]}\\\\\n  0             & \\text{, otherwise}\n\\end{cases}\n}\n\\]\n\nCDF:\n\n\\[\n\\displaystyle{\n\\begin{cases}\n  0 & \\text{, }{x &lt; a}\\\\\n  \\frac{x-a}{b-a} & \\text{, }{x \\in [a, b]}\\\\\n  1             & \\text{, }{x&gt;b}\n\\end{cases}\n}\n\\]\nIn R, dunif() gives the PDF of a uniform distribution. By default, it is \\(X \\sim U(0, 1)\\).\n\nlibrary(tidyverse)\n\n\nggplot() +\n  stat_function(fun = dunif, xlim = c(-4, 4))\n\n\n\n\n\n\n\n\nMeanwhile, punif() evaluates the CDF of a uniform distribution.\n\npunif(q = .3)\n\n[1] 0.3\n\n\n\n\n\n\n\n\nExercise\n\n\n\nEvaluate the CDF of \\(Y \\sim U(-2, 2)\\) at point \\(y = 1\\). Use the formula and punif().\n\n\n\n\n7.5.3 Normal distribution\nA normal distribution has two parameters: a mean and a standard deviation. So \\(X \\sim N(\\mu, \\sigma)\\).\n\nPDF: \\(2 {\\displaystyle {\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}}\\)\n\nIn R, dnorm() gives us the PDF of a standard normal distribution (\\(Z \\sim N(0, 1)\\)):\n\nggplot() +\n  stat_function(fun = dnorm, xlim = c(-4, 4))\n\n\n\n\n\n\n\n\nLike you might expect, pnorm() computes the CDF of a normal distribution (by default, the standard normal).\n\npnorm(0)\n\n[1] 0.5\n\n\n\npnorm(1) - pnorm(-1)\n\n[1] 0.6826895\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat is the probability of obtaining a value above 1.96 or below -1.96 in a standard normal probability distribution? Hint: use the pnorm() function.\n\n\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWhittinghill, Dexter C, and Robert V Hogg. 2001. “A Little Uniform Density with Big Instructional Potential.” Journal of Statistics Education 9 (2).\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "07_probability.html#footnotes",
    "href": "07_probability.html#footnotes",
    "title": "7  Probability",
    "section": "",
    "text": "This is sometimes called the frequentist interpretation of probability. There are other possibilities, such as Bayesian interpretations of probability, which describe probabilities as degrees of belief.↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "08_stats_sims.html",
    "href": "08_stats_sims.html",
    "title": "8  Statistics and simulations",
    "section": "",
    "text": "8.1 Random sampling\nBefore we jump to statistics and simulations, we’ll cover how to do random sampling in R.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistics and simulations</span>"
    ]
  },
  {
    "objectID": "08_stats_sims.html#random-sampling",
    "href": "08_stats_sims.html#random-sampling",
    "title": "8  Statistics and simulations",
    "section": "",
    "text": "8.1.1 Random sampling from theoretical distributions\n\nUniform distribution\nFor the uniform distribution, the arguments specify how many draws we want and the boundaries\n\nrunif(n = 20, min = -3, max = 3)\n\n [1]  2.538019555  2.739158976  0.596257565 -1.044849989 -1.819599018\n [6]  2.515448641  0.241870921  0.263839343  0.848012131 -0.555053809\n[11]  0.872958605  2.993528921 -1.984639603 -0.001654142 -1.358670604\n[16] -2.511016146 -1.104047602  0.935297658 -2.081949951  2.552993698\n\n\nWhen we draw a million times from the distribution, we can then plot it and see that it does look as we would expect:\n\nset.seed(123)\nmy_runif &lt;- runif(n = 1000000, min = -3, max = 3)\n\n\nggplot(data.frame(my_runif), aes(x = my_runif)) +\n  geom_histogram(binwidth = 0.25, boundary = 0, closed = \"right\") +\n  scale_x_continuous(breaks = seq(-5, 5, 1), limits = c(-5, 5))\n\n\n\n\n\n\n\n\n\n\nBinomial distribution\nFor the binomial distribution, we can specify the number of draws, how many trials each draw will have, and the probability of success.\nFor instance, we can ask R to do the following twenty times: flip a fair coin one hundred times, and count the number of tails.\n\nrbinom(n = 20, size = 100, prob = 0.5)\n\n [1] 48 45 54 50 58 50 42 58 48 57 53 49 52 51 49 40 57 53 52 41\n\n\nWith prob = , we can implement unfair coins:\n\nrbinom(n = 20, size = 100, prob = 0.9)\n\n [1] 88 87 93 95 93 92 91 94 87 91 90 92 93 89 90 95 91 90 86 88\n\n\n\n\nNormal distribution\nFor the Normal or Gaussian distribution, we specify the number of draws, the mean, and standard deviation:\n\nrnorm(n = 20, mean = 0, sd = 1)\n\n [1]  1.10455864  0.06386693 -1.59684275  1.86298270 -0.90428935 -1.55158044\n [7]  1.27986282 -0.32420495 -0.70015076  2.17271578  0.89778913 -0.01338538\n[13] -0.74074395  0.36772316 -0.66453402 -1.11498344 -1.15067439 -0.55098894\n[19]  0.10503154 -0.27183645\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCompute and plot my_rnorm, a vector with 10,000 draws from a Normal distribution \\(X\\) with mean equal to -10 and standard deviation equal to 2 (\\(X\\sim N(-10,2)\\)). You can recycle code!\n\n\n\n\n\n8.1.2 Random sampling from data\nIn this section we will work with good ol’ mtcars, one of R’s most notable default datasets. We’ll assign it to an object so it shows in our Environment pane:\n\nmy_mtcars &lt;- mtcars\n\n\n\n\n\n\n\nTip\n\n\n\nDefault datasets such as mtcars and iris are useful because they are available to everyone, and once you become familiar with them, you can start thinking about the code instead of the intricacies of the data. These qualities also make default datasets ideal for building reproducible examples (see Wickham 2014)\n\n\nWe can use the function sample() to obtain random values from a vector. The size = argument specifies how many values we want. For example, let’s get one random value of the “mpg” column:\n\nsample(my_mtcars$mpg, size = 1)\n\n[1] 24.4\n\n\nEvery time we run this command, we can get a different result:\n\nsample(my_mtcars$mpg, size = 1)\n\n[1] 14.7\n\n\n\nsample(my_mtcars$mpg, size = 1)\n\n[1] 15.5\n\n\nIn some occasions we do want to get the same result consistently after running some random process multiple times. In this case, we set a seed, which takes advantage of R’s pseudo-random number generator capabilities. No matter how many times we run the following code block, the result will be the same:\n\nset.seed(123)\nsample(my_mtcars$mpg, size = 1)\n\n[1] 15\n\n\nSampling with replacement means that we can get the same value multiple times. For example:\n\nset.seed(12)\nsample(c(\"Banana\", \"Apple\", \"Orange\"), size = 3, replace = T)\n\n[1] \"Apple\"  \"Apple\"  \"Orange\"\n\n\n\nsample(my_mtcars$mpg, size = 100, replace = T)\n\n  [1] 26.0 15.2 18.7 18.7 30.4 21.0 24.4 26.0 32.4 15.8 32.4 19.2 18.1 16.4 19.2\n [16] 27.3 14.3 10.4 17.3 13.3 21.4 13.3 19.2 24.4 15.0 27.3 17.8 15.2 15.8 14.3\n [31] 19.7 16.4 18.7 15.8 19.2 21.0 14.3 15.2 14.3 27.3 21.4 33.9 33.9 21.4 30.4\n [46] 33.9 21.4 17.3 17.3 10.4 26.0 18.7 15.2 30.4 10.4 10.4 15.5 14.3 26.0 17.3\n [61] 33.9 26.0 24.4 18.7 30.4 32.4 21.5 30.4 15.2 27.3 13.3 17.3 21.4 24.4 13.3\n [76] 22.8 33.9 13.3 21.5 14.3 19.2 30.4 24.4 26.0 15.8 10.4 24.4 14.3 15.2 10.4\n [91] 19.2 21.0 16.4 19.2 24.4 19.7 18.7 10.4 18.7 17.8\n\n\nIn order to sample not from a vector but from a data frame’s rows, we can use the slice_sample() function from dplyr:\n\nmy_mtcars |&gt; \n  slice_sample(n = 2) # a number of rows\n\n                  mpg cyl disp  hp drat   wt  qsec vs am gear carb\nDodge Challenger 15.5   8  318 150 2.76 3.52 16.87  0  0    3    2\nDatsun 710       22.8   4  108  93 3.85 2.32 18.61  1  1    4    1\n\n\n\nmy_mtcars |&gt; \n  slice_sample(prop = 0.5) # a proportion of rows\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n\n\nAgain, we can also use seeds here to ensure that we’ll get the same result each time:\n\nset.seed(123)\nmy_mtcars |&gt; \n  slice_sample(prop = 0.5)\n\n                    mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMaserati Bora      15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nCadillac Fleetwood 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nHonda Civic        30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nMerc 450SLC        15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nDatsun 710         22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 280           19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nFiat 128           32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nDodge Challenger   15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 280C          17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nHornet Sportabout  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nToyota Corolla     33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFord Pantera L     15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nAMC Javelin        15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nFerrari Dino       19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 230           22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nLotus Europa       30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n\n\n\n\n\n\n\n\nExercise\n\n\n\nUse slice_sample() to sample 32 rows from mtcars with replacement.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistics and simulations</span>"
    ]
  },
  {
    "objectID": "08_stats_sims.html#statistics",
    "href": "08_stats_sims.html#statistics",
    "title": "8  Statistics and simulations",
    "section": "8.2 Statistics",
    "text": "8.2 Statistics\n\nThe problems considered by probability and statistics are inverse to each other. In probability theory we consider some underlying process which has some randomness or uncertainty modeled by random variables, and we figure out what happens. In statistics we observe something that has happened, and try to figure out what underlying process would explain those observations. (quote attributed to Persi Diaconis)\n\n\nIn statistics we try to learn about a data-generating process (DGP) using our observed data. Examples: surveys, GDP statistics.\nUsually we are restrained to samples, while our DGPs of interest are population-based.\n\nSo we use random sampling or refer to superpopulations as a way to justify how the data we observe can approximate the population.\n\nStatistics has two main targets:\n\nEstimation: how we find a reasonable guess of an unknown property (parameter) of a DGP\nInference: how we describe uncertainty about our estimate\n\nWe use an estimator \\(\\hat\\theta(\\cdot)\\), which is a function that summarizes data, as a guess about a parameter \\(\\theta\\). A guess generated by an estimator in a given sample is called an estimate.\n\n\n\n\n\n\n\nExercise\n\n\n\nSuppose there’s a uniform distribution \\(X \\sim U(0, \\text{unknown})\\) out there.1 We want to use a sample from this distribution (let’s say of n=30 observations) to estimate the unknown upper bound.\nDiscuss: would the sample maximum be a good estimator? Why or why not?\n\n\n\nTheoretical statistics is all about finding “good” estimators. A few properties of good estimators:\n\nUnbiasedness: Across multiple random samples, an unbiased estimator gets the right answer on average.\nLow variance: Across multiple random samples, a low-variance estimator is more concentrated around the true parameter.\nBUT it’s sometimes hard to get both unbiasedness and low variance. So we have to make sacrifices. We usually quantify this via the mean squared error: \\(MSE = bias^2 + variance\\). Comparing two estimators, the one with the lowest MSE is said to be more efficient.\nConsistency: A consistent estimator converges in probability to the true value. “If we had enough data, the probability that our estimate would be far from the truth would be close to zero” (Aronow and Miller 2019, p. 105).\n\nApplied statistics is about using these techniques reasonably in messy real-world situations…",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistics and simulations</span>"
    ]
  },
  {
    "objectID": "08_stats_sims.html#simulations",
    "href": "08_stats_sims.html#simulations",
    "title": "8  Statistics and simulations",
    "section": "8.3 Simulations",
    "text": "8.3 Simulations\n\nIn simulations, we generate fake data following standard procedures. Why?\n\nTo better understand how our estimators work in different settings (the methods reason)\nTo get insights about complex processes with many moving parts (the substantive reason) (let’s talk about gerrymandering).\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSimulate drawing an n=30 random sample from a \\(X \\sim U(0, 10)\\) distribution and take its maximum value.\n\n\n\n8.3.1 Loops\nLoops allow us to repeat operations in R. The most common construct is the for-loop:\n\nfor (i in 1:10){\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nIt’s common to perform operations at each step and save the results. We typically create an empty object and “fill it in” at each step:\n\nresults &lt;- double(10)\nfor (i in 1:10){\n  results[i] &lt;- i ^ 2\n}\n\n\nresults\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\n\n\n\n\n\n\n\nFunctional loops\n\n\n\nAnother way to do loops is with the *apply() family of functions:\n\nsapply(1:10, function(x){x ^ 2})\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\n\n\n\n\nWe talked about loops and various extensions in one of our methods workshops last year: Speedy R.\n\n\n\n8.3.2 An example simulation\nWe will simulate our exercise from above 10,000 times:\n\nset.seed(1)\nk &lt;- 10000 # number of simulations\nn &lt;- 30    # number of observations in each simulation\n\n# define an empty numeric object\nsimulated_estimates &lt;- double(k)\n\n# loop: at each step draw a random n=30 sample and get its maximum \nfor (i in 1:k){\n  random_sample &lt;- runif(n, 0, 10)\n  simulated_estimates[i] &lt;- max(random_sample)\n}\n\nNow we can analyze our simulated estimates:\n\nmean(simulated_estimates)\n\n[1] 9.677225\n\n\n\nggplot(data.frame(x = simulated_estimates), aes(x = x)) +\n  geom_histogram(binwidth = 0.05, boundary = 0, closed = \"right\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWe just simulated to evaluate the sample maximum as an estimator. Modify the code above to evaluate the following two estimators:\n\n\\(\\displaystyle\\text{(sample maximum)} \\cdot \\frac{(n+1)}{n}\\)\n\\(\\displaystyle 2 \\cdot \\text{(sample mean)}\\)\n\n\n\n\n\n8.3.3 Another example simulation: bootstrapping\nBootstrap (and its relatives) is one way in which we can do inference, i.e., assess uncertainty. (We’ll go through the intuition on the board.)\n\n# set seed an number of simulations\nset.seed(1)\nk &lt;- 10000\nbootstrapped_means &lt;- double(k)\nfor (i in 1:k){\n  m &lt;- my_mtcars |&gt; slice_sample(prop = 1, replace = T)\n  bootstrapped_means[i] &lt;- mean(m$mpg)\n}\n\n\nggplot(data.frame(bootstrapped_means), aes(x = bootstrapped_means)) +\n  geom_histogram(binwidth = 0.25, boundary = 0, closed = \"right\")\n\n\n\n\n\n\n\n\n\n\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. “Automatic Differentiation in Machine Learning: A Survey.” The Journal of Machine Learning Research 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022. “V-Dem Codebook V12.” Varieties of Democracy (V-Dem) Project. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia Alvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The Quality of Government Basic Dataset, Version Jan23.” University of Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government doi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress In The Age Of Trump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton; Oxford: Princeton University Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.” MIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact Matching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a Toolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSmith, Danny. 2020. Survey Research Datasets and R. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service. 2019. “Department of Agriculture Agricultural Research Service.” https://fdc.nal.usda.gov/.\n\n\nWhittinghill, Dexter C, and Robert V Hogg. 2001. “A Little Uniform Density with Big Instructional Potential.” Journal of Statistics Education 9 (2).\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistics and simulations</span>"
    ]
  },
  {
    "objectID": "08_stats_sims.html#footnotes",
    "href": "08_stats_sims.html#footnotes",
    "title": "8  Statistics and simulations",
    "section": "",
    "text": "This is a fascinating distribution with a rich history, and it is used in many statistical textbooks (Whittinghill and Hogg, 2001). It is thoroughly covered in the UT SDS Mathematical Statistics sequence.↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistics and simulations</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Arel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018.\n“Countrycode: An r Package to Convert Country Names and Country\nCodes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of\nAgnostic Statistics. Cambridge University Press.\n\n\nBank, World. 2023. “World Bank Open Data.” https://data.worldbank.org/.\n\n\nBaydin, Atılım Günes, Barak A. Pearlmutter, Alexey Andreyevich Radul,\nand Jeffrey Mark Siskind. 2017. “Automatic Differentiation in\nMachine Learning: A Survey.” The Journal of Machine Learning\nResearch 18 (1): 5595–5637.\n\n\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I.\nLindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2022.\n“V-Dem Codebook V12.” Varieties of Democracy (V-Dem)\nProject. https://www.v-dem.net/dsarchive.html.\n\n\nDahlberg, Stefan, Aksen Sundström, Sören Holmberg, Bo Rothstein, Natalia\nAlvarado Pachon, Cem Mert Dalli, and Yente Meijers. 2023. “The\nQuality of Government Basic Dataset, Version Jan23.” University\nof Gothenburg: The Quality of Government Institute. https://www.gu.se/en/quality-government\ndoi:10.18157/qogbasjan23.\n\n\nFiveThirtyEight. 2021. “Tracking Congress\nIn The Age Of\nTrump [Dataset].” https://projects.fivethirtyeight.com/congress-trump-score/.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social\nScience: An Introduction in Tidyverse. Princeton; Oxford: Princeton\nUniversity Press.\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for\nPolitical and Social Research. Princeton, NJ: Princeton University\nPres.\n\n\nPontin, Jason. 2007. “Oppenheimer’s Ghost.”\nMIT Technology Review, October 15, 2007. https://www.technologyreview.com/2007/10/15/223531/oppenheimers-ghost-3/.\n\n\nRobinson, David. 2020. Fuzzyjoin: Join Tables Together on Inexact\nMatching. https://github.com/dgrtwo/fuzzyjoin.\n\n\nRossi, Hugo. 1996. “Mathematics Is an Edifice, Not a\nToolbox.” Notices of the AMS 43 (10): 1108.\n\n\nSmith, Danny. 2020. Survey Research Datasets and\nR. https://socialresearchcentre.github.io/r_survey_datasets/.\n\n\nU. S. Department of Agriculture [USDA], Agricultural Research Service.\n2019. “Department of Agriculture Agricultural Research\nService.” https://fdc.nal.usda.gov/.\n\n\nWhittinghill, Dexter C, and Robert V Hogg. 2001. “A Little Uniform\nDensity with Big Instructional Potential.” Journal of\nStatistics Education 9 (2).\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of\nStatistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023.\nGgplot2: Elegant Graphics for\nData Analysis. 3rd ed. https://ggplot2-book.org/.",
    "crumbs": [
      "References"
    ]
  }
]