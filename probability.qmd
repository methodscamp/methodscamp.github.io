# Probability

## What is probability?

-   Frequency with which an event occurs.
    -   Typically: $$Pr(A) = P(A) = \pi(A) = \dfrac{\text{Number of ways an event can occur}}{\text{Total number of possible outcomes}}$$
-   Probability predicts real-world events using theoretical quantities.
    -   Formally, it assigns a likelihood of occurrence to each event in sample space
    -   We use the probability space triplet ($\Omega, S, P$), which are the sample space, event space, and probability mapping, respectively.
-   We can consider probability as a function that maps $\Omega \to \mathbb{R}$.
-   We can conceive it in terms of relative frequency or subjective belief.

## Kolmogorov's axioms

-   $Pr(S_i)\in\mathbb{R},\hspace{2mm} 1 \geq Pr(S_i)\geq 0 \qquad \forall S_i\in S$
    -   Where $S$ is the event space, $S_i$ are events.
    -   Probabilities must be non-negative.
-   $Pr(\Omega) = 1$
    -   Where $\Omega$ is the sample space.
    -   Something has to happen.
    -   Probabilities sum/integrate to 1.
-   $Pr\left(\bigcup_{i = 1}^\infty S_i\right) = \sum_{i=1}^\infty Pr(S_i) \iff Pr(S_i \cap S_j) = 0\hspace{2mm} \forall i\neq j$
    -   The probability of disjoint (mutually exclusive) sets is equal to the sum of their individual probabilities.

## Some definitions

-   **Random variable**: a variable whose value is determined by the outcome of a random process.
    -   Sometimes also called a *stochastic variable*.
    -   May be discrete or continuous.
-   **Distribution** (of a random variable): the set of values the variable might take.
    -   Probability mass function / probability density function defines the probability with which each value occurs.
    -   Always sums / integrates to 1.
-   **Realization** (of a random variable): a particular value taken by the variable.

------------------------------------------------------------------------

-   **Population**: the entire set of objects (people, cases, etc.) in which we are interested.
    -   Often denoted $N$.
-   **Sample**: a subset of the population we can observe, from which we try to make generalizations about the population.
    -   Often denoted $n$.
-   **Frequency distribution**: a count of how often a variable takes each of its possible values.
    -   The number of members of a sample that take each value of a variable.
-   **Independent random variables**: two variables are statistically independent if the value of one does not affect the value of the other.
    -   Formally, $Pr(A \cap B)=Pr(A)Pr(B)$

## Discrete probability

-   A sample space in which there are a (finite or infinite) countable number of outcomes
-   Each realization of random process has a discrete probability of occurring.
    -   $f(X=x_i)=P(X=x_i)$ is the probability the variable takes the value $x_i$.

### Probability Mass Function (PMF)

Probability of each occurrence encoded in probability mass function (PMF)

-   $0 \leq f(x_i) \leq 1$
    -   Probability of any value occurring must be between 0 and 1.
-   $\displaystyle\sum_{x}f(x_i) = 1$
    -   Probabilities of all values must sum to 1.

### Discrete distribution

-   What's the probability that we'll roll a 3 on one die roll: $$Pr(y=3) = \dfrac{1}{6}$$

-   If one roll of the die is an "experiment."

-   We can think of a 3 as a "success."

-   $Y \sim Bernoulli \left(\frac{1}{6} \right)$

-   Fair coins are $\sim Bernoulli(.5)$, for example.

-   More generally, $Bernoulli(\pi )$.

    -   $\pi$ represents the probability of success.

------------------------------------------------------------------------

-   Drawing a specific card from a deck: $$Pr(y=\text{ace of spades}) = \dfrac{1}{52}$$

-   Drawing any card with a specific value from a deck: $$Pr(y=ace) = \dfrac{4}{52}$$

-   Getting a specific value on two dice rolls: $$Pr(y=8) = \dfrac{5}{36}$$

-   We can express the probability mass function in tabular format or in a graph.

## Continuous probability

-   What happens when our outcome is continuous?
-   There are an infinite number of outcomes.
-   This makes the denominator of our fraction difficult to work with.
-   The probability of the whole space must equal 1.
-   Even if all events are equally likely, $\dfrac{1}{\infty} =0$

### Basics

-   The domain may not span -$\infty$ to $\infty$.
    -   Even space between 0 and 1 is infinite.
-   The domain is defined as the area under the probability density function.
-   Two common examples are the uniform and bell curves.

### Probability Density Function (PDF)

-   Similar to PMF from before, but for continuous variables.
-   Gives the probability a value falls within a particular interval
    -   $P[a\le X\le b] = \displaystyle\int_a^b f(x) \, dx$
    -   Total area under the curve is 1.
    -   $P(a < X < b)$ is the area under the curve between $a$ and $b$ (where $b > a$).

## Cumulative Density Function (CDF)

### Discrete

-   Cumulatve density function is probability X will take a value of x or lower.
-   PDF is written $f(x)$, and CDF is written $F'(x)$. $$F_X(x) = Pr(X\leq x)$$
-   For discrete CDFs, that means summing up over all values.
-   What is the probability of rolling a 6 or lower with two dice? $F(6)=?$

### Continuous

-   We can't sum probabilities for continuous distributions (remember the 0 problem).
-   Solution: integration $$F_Y(y) = \int_{-\infty}^{y} f(y) dy$$
-   Examples of uniform distribution.

## Statistics

### Introduction

-   While probability allows us to make predictions about events using distributions, statistics uses events to make estimates about distributions and variables.
-   It is the process of learning from data.
-   A statistic is a summary of data, capturing some theoretically-relevant quantity.
-   Broad categories of numerical and categorical.

### Univariate statistics

-   These measure a single variable.
-   Readily expressed in graphical form.
-   Common examples:
    -   Central tendency (mean, median, and mode)
    -   Variance

### Examples of univariate statistics

-   The mean ($\bar{x}$) is calculated by summing the data, then dividing by the number of observations: $$\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i$$

-   The median is found by ordering the observations from highest to lowest and finding the one in the middle.

-   The mode is the most common number.

$$x= \begin{bmatrix} 1 & 2 & 3 & 4 & 5 & 6 & 6 & 7 & 8 & 9 \end{bmatrix}$$

-   What are the mean, median, and mode of x?

### Measures of central tendency

-   Mean balances values on either side.
-   Median balances observations on either side.
-   Mode finds the most typical observation.
-   Which is the best? Like most of what you'll learn in statistics, it depends.

### Deviations from central tendency

-   Consider two data sets: $$\begin{aligned}
    x= \begin{bmatrix} 1 & 1.5 & 2 & 2.5 & 5.5 & 8.5 & 9 & 9.5 & 10 \end{bmatrix}
    \end{aligned}$$ $$\begin{aligned}
    y= \begin{bmatrix} 4.5 & 4.8 & 5 & 5.3 & 5.5 & 5.7 & 6 & 6.2 & 6.5 \end{bmatrix}
    \end{aligned}$$

-   What is the mean of each?

-   What is the median of each?

-   Are they similar distributions?

### Variance

-   We use variance to measure the spread of a single variable.
-   Formally defined as the squared deviation from the mean ($\mu$).
-   For discrete random variables, it is written $Var(x)=\sigma^2=\displaystyle\frac{1}{n}\displaystyle\sum_{i=1}^n(x_i-\mu)^2$
-   For continuous random variables, it is written $Var(x)=\sigma^2=\displaystyle\int (x-\mu)^2 f(x) \text{ }dx$

### Standard deviation

-   Sometimes variance doesn't make sense, either mathematically or conceptually.
    -   Not always clear how to interpret "squared deviation from the mean."
-   Instead, will frequently see standard deviation, which is square root of variance.
-   It is written $\sigma$.

## Bivariate statistics

### Covariance

-   While measures of central tendency and variance/standard deviation provide useful summaries of a single variable, they don't provide insights into relationships between variables.
-   For that, we need bivariate statistics.
-   Most common and straightforward is covariance.

------------------------------------------------------------------------

-   Colloquially, can think of covariance as measure of linear deviation from mean.
-   When values from one variable are above their mean, are values from the other above or below their mean?
-   Put another way, if I told you the value of x was high, would you expect values of y to be high or low?
-   Formally, it is written as: $$cov(X,Y)=E(X-E(X))(Y-E(Y))=E(XY)-E(X)E(Y)$$
-   It is important to note that the magnitude is meaningless; only the direction is interpretable.

### Correlation

-   Correlation is a normalized measure of covariance
-   It is calculated as: $$\rho_{X,Y}=\frac{cov(X,Y)}{\sigma_X \sigma_Y}$$
-   It varies between -1 and 1.
-   What is correlation of two independent variables?

## Regression

### Ordinary least squares

-   Ordinary least squares regression (OLS) is probably the most widely-used model in political science.
-   It is all about drawing a line through data.
-   This allows us to evaluate the relationship (the association) between $x$ on $y$.
-   The dependent variable, $y$, must be continuous, generally speaking.
-   The main question is which line to draw.

------------------------------------------------------------------------

Line and equation ($\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_{i}$) on board

### Residuals

-   In basically any set of data, no line can pass through every point (observation).
-   We will always have make some error in predicting values.
-   The error between the line and some point is referred to as the residual.
-   If we refer to our predicted value as $\hat{y}$, then we can calculate the residual for each observation with the following equation: $$e_i = y_i - \hat{y}_i$$

### Finding the right line

-   OLS determines the "best" line by minimizing the sum of squared residuals.
-   Plug in all the values for the slope amd intercept and calculate the sum of squared residuals for these infinity combinations.
-   That is a lot of work.
-   The best solution turns out to be calculus.
-   We want to minimize the sum of squared residuals with respect to our $\beta$'s.
